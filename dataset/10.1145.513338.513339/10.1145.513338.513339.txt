Peer-to-Peer Hypertext 
 
Moderator 
 
Uffe K. Wiil 
Dept. of Computer Science and Engineering 
Aalborg University Esbjerg, Denmark 
ukwiil@cs.aue.auc.dk 
 
Panelists 
Niels Olof Bouvin 
Dept. of Computer Science 
Aarhus University, Denmark 
bouvin@daimi.au.dk 
 
David C. De Roure 
Electronics and Computer Science 
University of Southampton, UK 
dder@ecs.soton.ac.uk 
 Deena Larsen 
Hypertext author 
Lakewoord, Colorado, USA 
textra@chisp.net 
 
Mark K. Thompson 
Electronics and Computer Science 
University of Southampton, UK 
mkt@ecs.soton.ac.uk 
 
1. INTRODUCTION 
Over the past several decades, hypertext system architectures 
have evolved from the early monolithic systems to the middleware-oriented, component-based, and open systems of today [7]. 
The current trend is to provide hypertext structuring services 
wrapped in components with well-defined interf aces [6]. The 
dominant form of relationships between services in these massively distributed environments continues to be the traditional client-server based one. 
Recently, a peer-to-peer based approach to hypertext systems 
has been discussed as an alternative to the client-server based approach [1,2,3,4]. 
The peer-to-peer approach can be traced back to the visionary 
work at Xerox PARC on ubiquitous computing starting in 1988 [5]. Since then several related and overlapping terms have been introduced to characterize this new approach such as pervasive computing, mobile computing, wireless networks, ad hoc networks, peer-to-peer networks, etc. 
This panel will try to identify, clarify, and discuss some of the 
issues and potential benefits involved in peer-to-peer hypertext. The panel will examine the issues from different perspectives: from the perspective of hypertext system developers, hypertext authors (writers), and hypertext readers. 
Copyright is held by the author/owner(s). 
HT’02,  June 11-15, 2002, College Park, Maryland, USA. 
ACM 1-58113-477-0/02/0006.  The panelists will be asked to respond to questions such as: 
• What advantages and drawbacks does the peer-to-peer 
approach introduce compared to the traditional client-server based approach? 
• Is it time to abandon the traditional client-server based 
model of hypertext and introduce a peer-to-peer model? 
• How do we handle the situation where the availability of 
documents and structure cannot be guaranteed and services become disrupted? 
• What does peer-to-peer hypertext even mean to hypertext 
system developers, authors, and readers? 
• Will hypertext developers be charged with an additional set 
of issues or will it be simpler to develop hypertext systems in a peer-to-peer world? 
• Does it even matter to hypertext authors and readers how 
the underlying system architecture is laid out? 
• Can hypertext authors use the disrupted nature of document 
and structure services to write new types of hypertexts where the flow may depend on the availability of services? 
• Questions from the audience are also very welcome. 
Each panelist was given the opportunity to examine these 
questions in advance and respond with their position on the issues. 
692. POSITIONS 
Niels Olof Bouvin 
An important characteristic of most P2P systems is the 
transience of data: Data is not necessarily available, nor is its location necessarily known. If transience can be successfully addressed (through the use of good replication and query algorithms), the potential of P2P hypermedia is tantalizing. The greatest promise of P2P hypermedia is to facilitate sharing. A P2P hypermedia system would for a lone user be a tool for structuring, just like a conventional hypermedia tool. However, the hypermedia tool becomes increasingly valuable with the number of participant users. The Web has numerous examples of very active communities (blogs, discussion boards, Wiki Wiki Webs, etc.), and I see no reason why the same could not be extended to a hypermedia system, as long as the tools are easily accessible and features a strong network effect.  
A P2P system must rely on replication for scalability. How then 
can an author be correctly identified? Normally, authors have control over their content as they control the location of the data. In a P2P system, there is no such control. To ensure authentication, data must be digitally signed – a technique, which can also be used ensure the integrity of distributed material (see the Freenet project for much more work in this area – http://www.freenetproject.org). If the challenges of replication, searching, and authentication can be met, P2P hypermedia can be a hugely scalable system, providing many users with a forum to share their work, without relying on central parties to maintain hypermedia servers. 
Deena Larsen 
We need to consciously develop a mechanism to ensure that “important” documents are available when local shutdowns occur. For example, the US Department of the Interior shut down all Internet access on December 5th 2001, due to litigation 
over Indian Trust Assets. This graphically demonstrated the need for non-proprietary (peer-to-peer) housing of documents and systems as the US was without earthquake warning systems, park reservations, dam operations, and other vital functions for several days to several months. How do we determine what is important and how do we ensure that it is on a peer-to-peer network? Who makes these decisions in governments, corporations, and other environments? What are the criteria for determining what should be on a peer-to-peer network and what on a traditional client-server? Other issues include information security, updates, authorship, and proprietary data. 
For hypertext and new media artists and writers, the issues 
center around system availability more than access to the net 
itself. Whatever document and structure services come into being, hypertext authors will use, subvert, and disrupt. Hypertext authors have already incorporated the ephemeral nature of links and used 404 errors and other non-intentional web features as vital parts of the work. How will works be preserved? I can read a novel from the 18
th century, but cannot 
read a novel from 1986 as InterMedia is no longer available. Further issues include copyrights, distribution control, and software capabilities. David C. De Roure 
Agent-based computing systems, and many systems that can be described in those terms, are essentially peer-to-peer. Through various projects using software agents for distributed information management we already have a good understanding of peer-to-peer hypertext. For example, agents can be used to manage the lifecycle of links in open hypermedia systems, including link discovery, resolution, delivery, and maintenance; they also provide automation in adaptive, context-aware information systems. More recently, this approach has been facilitated by semantic web technologies, notably XML (which supports process-to-process information exchange) and RDF (which supports a shared understanding of metadata). One of the challenges of the peer-to-peer and pervasive context is the dynamic availability of resources: we can build the machinery to overcome this, attempting the perfect link integrity to which the open hypermedia model aspires, but we should also consider the requirements of contemporary applications in order to provide realistic, scalable and pervasive solutions. 
Mark K. Thompson 
One of the defining characteristics of peer-to-peer technologies as deployed on the Internet today is that they – sometimes incorrectly – boast a lack of pre-defined infrastructure between peer services. This in itself introduces systems issues such as naming, discovery, and interaction, but also promotes development of componentised, heterogeneous systems whose configuration is transient. Developing hypermedia systems in this setting engenders dynamic hyperstructures that are probabilistic, rather than deterministic.  The X-factor of chance provided by peer-to-peer when combined with pervasive computing technology, such as the integration of physical artifacts, offers new sets of challenges for the hypermedia community. 
“Peer-to-peer with Pervasive” hypermedia should, amongst 
other things, challenge systems builders to cater for the novel set of issues when dealing with transient and possibly mobile structure; offer users both serendipitous and contextually-directed access to what would otherwise be un-navigable resources in ad hoc scenarios; and provide new fora for storytelling and participative hypermedia. 
3. BIOGRAPHIES 
Niels Olof Bouvin  started working with hypermedia in 1996, 
when he developed DHM/WWW, which showcased an open hypermedia client integrating the World Wide Web. This work has continued, currently concluding with the Arakne Environment. Niels Olof received his Ph.D. in 2001 on the topic 
of “Web Augmentation with Open Hypermedia”. His current research interests are P2P networking, Web augmentation, fluid annotations, and mobile hypermedia. Together with Kaj Grønbæk and Lennert Sloth, he r eceived the Engelbart Award in 
1997. Niels Olof is currently an assistant professor at the Department of Computer Science, University of Aarhus, Denmark. 
Deena Larsen  has been working to push the limits of what 
hypertext can do for about a d ecade. She has written several 
hypertexts on disk and online, organized writers' workshops and 
70online communication networks to help develop a hypertext/new 
media/electronic literature community. 
David C. De Roure  is a Professor of Computer Science in the 
Intelligence Agents Multimedia Research Group in the Department of Electronics and Computer Science, University of Southampton, UK, where he researches very large scale adaptive information systems. With a background in distributed link services and agent based computing, his research now focuses on the interface between pervasive computing and advanced knowledge technologies. He was Programme Co-Chair of the WWW2002 Conference and appeared on the Semantic Web panel at HT01. 
Mark K. Thompson  is a Research Assistant in the Intelligence 
Agents Multimedia Research Group in the Department of Electronics and Computer Science, University of Southampton, UK. He is a member of the EPSRC Interdisciplinary Research Collaboration EQUATOR, which concerns technical, social, and design issues in the development of new inter-relationships between the physical and digital worlds. His research background and interests are pervasive computing technology as applied to distributed information management, particularly the application of hypermedia techniques to ad hoc information environments. 
Uffe Kock Wiil  received his Ph.D. in computer science from 
Aalborg University, Denmark (1993). He is the founder of the Department of Computer Science and Engineering at Aalborg University's Esbjerg campus, where he is currently employed as an associate professor and serves as the department head. He has more than a decade of experience in building distributed hypermedia systems. He has been appointed several chair positions at the ACM Hypertext Conference including Program Chair in 1999. His research interests include hypermedia, structural computing, Web and Internet technologies, CSCW, digital libraries, and programming technology. REFERENCES 
[1] N. O. Bouvin. 2002. Pervasive hypermedia: Open hypermedia in a peer-to-peer context. In Proceedings of the 
2002 ACM Hypertext Conference , (Baltimore, MD, Jun.), 
ACM Press. 
[2] T. J. Lukka and B. Fallenstein. 2002. Freenet-like GUIDs for implementing Xanalogical hypertext. In Proceedings of 
the 2002 ACM Hypertext Conference , (Baltimore, MD, 
Jun.), ACM Press. 
[3] M. K. Thompson, D. C. De Roure, and D. Michaelides. 2000. Weaving the pervasive information fabric. In Proceedings of the 6
th International Workshop on Open 
Hypermedia Systems . Lecture Notes in Computer Science 
(LNCS 1903), Springer Verlag, pp. 87-95. 
[4] M. K. Thompson and D. C. De Roure. 2001. Hypermedia by coincidence. In Proceedings of the 2001 ACM Hypertext 
Conference , (Århus, Denmark, Aug.), ACM Press, pp. 129-
130. 
[5] M. Weiser. 1993. Some computer science issues in ubiquitous computing. Communications of the ACM , 
36(7):75-84. 
[6] U. K. Wiil, D. L. Hicks, and P. J. Nürnberg. 2001. Multiple open services: A new approach to service provision in open hypermedia systems, In Proceedings of the 2001 ACM 
Hypertext Conference , (Århus, Denmark, Aug.), ACM 
Press, pp. 83-92. 
[7] U. K. Wiil and P. J. Nürnberg. 1999. Evolving hypermedia middleware services: Lessons and observations. In Proceedings of the 1999 ACM Symposium on Applied 
Computing , (San Antonio, TX, Feb.), ACM Press, pp. 427-
436. 
 
71