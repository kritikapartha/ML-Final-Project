{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_leafs(term_tree):\n",
    "    terms = set()\n",
    "    queue = [term_tree]\n",
    "\n",
    "    while(queue):\n",
    "        curr_dict = queue.pop()\n",
    "        for key in curr_dict:\n",
    "            if(curr_dict[key]):\n",
    "                queue.append(curr_dict[key])\n",
    "            else:\n",
    "                terms.add(key.lower())\n",
    "    return terms\n",
    "\n",
    "dataset_dir = \"./dataset\"\n",
    "\n",
    "labeled_documents = []\n",
    "\n",
    "for doi in os.listdir(dataset_dir):\n",
    "    text_path = os.path.join(dataset_dir, doi, f\"{doi}.txt\")\n",
    "    if(not os.path.exists(text_path)):\n",
    "        continue\n",
    "    text = open(text_path, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "    \n",
    "    terms_path = os.path.join(dataset_dir, doi, f\"{doi}.json\")\n",
    "    if(not os.path.exists(terms_path)):\n",
    "        continue\n",
    "    \n",
    "    gt_term_tree = json.load(open(terms_path))\n",
    "    gt_terms = get_leafs(gt_term_tree)\n",
    "    \n",
    "    labeled_documents.append((text, list(gt_terms)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Towards a Critical Race Methodology in Algorithmic Fairness\\nAlex Hanna∗\\nEmily Denton∗\\nAndrew Smart\\nJamila Smith-Loud\\n{alexhanna,dentone,andrewsmart,jsmithloud}@google.com\\nABSTRACT\\nWe examine the way race and racial categories are adopted in\\nalgorithmic fairness frameworks. Current methodologies fail to ad-\\nequately account for the socially constructed nature of race, instead\\nadopting a conceptualization of race as a fixed attribute. Treating\\nrace as an attribute, rather than a structural, institutional, and rela-\\ntional phenomenon, can serve to minimize the structural aspects\\nof algorithmic unfairness. In this work, we focus on the history of\\nracial categories and turn to critical race theory and sociological\\nwork on race and ethnicity to ground conceptualizations of race for\\nfairness research, drawing on lessons from public health, biomedi-\\ncal research, and social survey research. We argue that algorithmic\\nfairness researchers need to take into account the multidimension-\\nality of race, take seriously the processes of conceptualizing and\\noperationalizing race, focus on social processes which produce\\nracial inequality, and consider perspectives of those most affected\\nby sociotechnical systems.\\nCCS CONCEPTS\\n•Applied computing →Sociology ;•Social and professional\\ntopics →Race and ethnicity .\\nKEYWORDS\\nalgorithmic fairness, critical race theory, race and ethnicity\\nACM Reference Format:\\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020.\\nTowards a Critical Race Methodology in Algorithmic Fairness. In Conference\\non Fairness, Accountability, and Transparency (FAT* ’20), January 27–30, 2020,\\nBarcelona, Spain. ACM, New York, NY, USA, 12 pages. https://doi.org/10.\\n1145/3351095.3372826\\n∗Both authors contributed equally to this research.\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nFAT* ’20, January 27–30, 2020, Barcelona, Spain\\n©2020 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-6936-7/20/02.\\nhttps://doi.org/10.1145/3351095.3372826The problem does not end with the collection of racial\\ndata; it only begins. The problem accelerates when\\nwe attempt to analyze these data statistically... The\\nracialization of data is an artifact of both the strug-\\ngles to preserve and to destroy racial stratification.\\nBefore the data can be deracialized, we must deracial-\\nize the social circumstances that have created racial\\nstratification.\\n– Tufuku Zuberi [125, pp. 102]\\n1 INTRODUCTION\\nIn recent years, there has been increasing recognition of the poten-\\ntial for algorithmic systems to reproduce or amplify existing social\\ninequities. In response, the research field of algorithmic fairness has\\nemerged. This rapidly evolving research area is focused on devel-\\noping tools and techniques with aspirations to make development,\\nuse, and resulting impact of algorithmic systems conform to various\\nsocial and legal notions of fairness. The concept of fairness, in ad-\\ndition to being situational, evolving, and contested from a number\\nof philosophical and legal traditions, can only be understood in\\nreference to the different social groups that constitute the organi-\\nzation of society. Consequently, the vast majority of algorithmic\\nfairness frameworks are specified with reference to these social\\ngroups, often requiring a formal encoding of the groups into the\\ndataset and/or algorithm.\\nHowever, most social groups relevant to fairness analysis re-\\nflect highly contextual and unstable social constructs. These social\\ngroups are often defined with recourse to legal anti-discrimination\\nconcepts such as \"protected classes,\" which, in the US, refers to race,\\ncolor, national origin, religion, sex, age, or disability. However, the\\nprocess of drawing boundaries around distinct social groups for\\nfairness research is fraught; the construction of categories has a\\nlong history of political struggle and legal argumentation.\\nNumerous recent works have highlighted the limitations of cur-\\nrent algorithmic fairness frameworks [ 53,108]. Several of these\\ncritiques point to the tendency to abstract away critical social and\\nhistorical contexts and minimize the structural conditions that un-\\nderpin problems of algorithmic unfairness. We build on this critical\\nresearch, focusing specifically on the use of race and racial cate-\\ngories within this field. IN this literature, the topic of the instability\\nof racial categories has gone relative unexplored, with the notable\\nexception of Benthall and Haynes [ 12], which we discuss in detail\\nbelow.\\nRace is a major axis around which algorithmic allocation of\\nresources and representation is bound. It may indeed be the most\\nsignificant axis, given attention by investigative journalists (e.g. [ 7])\\nand critical race and technology scholars (e.g. [ 11,20,21,24,85]).\\n501FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\nBecause of this, it is imperative that the race-based methodologies\\nand racial categories themselves are interrogated and critically\\nevaluated.\\nIn this paper we develop several lines of critique directed at the\\ntreatment of race within algorithmic fairness methodologies. In\\nshort, we observe that current methodologies fail to adequately\\naccount for the socially constructed nature of race, instead adopting\\na conceptualization of race as a fixed attribute. This manifests in the\\nwidespread use of racial categories as if they represent natural and\\nobjective differences between groups. Treating race as an attribute,\\nrather than a structural, institutional, and relational phenomenon,\\nin turn serves to minimize the structural aspects of algorithmic\\nunfairness.\\nThe process of operationalizing race is fundamentally a project\\nof racial classification and thus must be understood as a political\\nproject, or, more specifically, what Omi and Winant [ 86] refer to\\nas a racial project. The tools for measuring individual characteris-\\ntics, such as national censuses and other population-level survey\\ninstruments, were and are still often based in the politics of racial\\noppression and domination. While we acknowledge the importance\\nof measuring race for the purposes of understanding patterns of\\ndifferential performance or differentially adverse impact of algo-\\nrithmic systems, in this work, we emphasize that data collection\\nand annotation efforts must be grounded in the social and historical\\ncontexts of racial classification and racial category formation. To\\noversimplify is to do violence, or even more, to re-inscribe violence\\non communities that already experience structural violence.\\nIt is useful to distinguish between two ways in which race comes\\ninto play in algorithmic fairness research: (i) the operationalization\\nof race, i.e. the process of converting the abstract concept of race\\ninto something that is concrete and measurable and (ii) the use of\\nracial variables within algorithmic frameworks. These aspects are\\ntightly interconnected since race must be operationalized before\\nracial variables can be utilized.\\nOur contributions are as follows: we review the use of race in\\nprior fairness work and discuss an intervention in the debate on\\nracial categories. We then survey the history of racial classifica-\\ntion, scientific racism, and racial classification in national censuses.\\nWe introduce the notion of multidimensionality of race, and sub-\\nsequently review how different disciplines have dealt with the\\ntricky problem of operationalizing race, most notably in biomedical,\\nsurvey, and public health research. In light of these discussions,\\nwe address how race has been treated in the group fairness and\\ndisaggregated analysis paradigms. Lastly, we argue that fairness re-\\nsearchers need to take into account the multidimensionality of race,\\ntake seriously the processes of conceptualizing and operationaliz-\\ning race, focus on processes of racism, and consider perspectives of\\nthose most affected by sociotechnical systems.\\n2 THE PROBLEM WITH RACIAL\\nCATEGORIES\\nIn perhaps the best-known debate surrounding algorithmic fair-\\nness, Julia Angwin and her colleagues illustrated the differing rates\\nat which Black defendants had been issued high pre-trial risk as-\\nsessment scores by the COMPAS recidivism prediction algorithm\\ncompared to white defendants [ 7]. Equivant – then, Northpointe,the company which developed COMPAS – defended the system\\nby reanalyzing the ProPublica data [ 28] and ProPublica later re-\\nsponded [ 6]. The debate has become a cornerstone of algorithmic\\nfairness research; to date, the original story has some 700 citations\\non Google Scholar.\\nThe Propublica analysis of COMPAS, as well as the responses\\nfrom Northpointe and other secondary analyses performed by third-\\nparty researchers, relied on data from the Broward County Sheriff’s\\nOffice which was obtained through a public records request. Race\\nclassifications in this data identified defendants as Black, White,\\nHispanic, Asian, Native American or Other. These categories look\\nfamiliar – they are nearly identical to the way that the US Census\\ndefines race. However, there are the notable absences of the cate-\\ngories \"Native Hawaiian or Other Pacific Islander\", and there is a\\nredefinition of \"Hispanic\" as a race rather than an ethnicity.\\nIn the methodological appendix to the original article [ 69], Jeff\\nLarson and other ProPublica authors do not delve into how the\\nrace of the defendant is measured. Larson admits that he did not\\nknow how Broward County classified individuals1. It’s not clear\\nwhy Broward County used the modified Census racial schema,\\nbut, as detailed below, it may have to do with a 1977 directive\\nfrom the federal US Office of Management and Budget. In general,\\napproaches to measuring race for police records are inconsistent\\nacross municipalities. They can rely on self-identification, state\\nrecords, or observation by criminal justice workers. In her work on\\nracial disparities in incarceration in Wisconsin, sociologist Pamela\\nOliver notes that the race of a single individual can change across\\nrecords even within a single jurisdiction2. Therefore, even in the\\nmost famous debate of the field, we don’t know why the data take\\non a particular racial schema, nor do we have information about\\nhow defendants are racially categorized.\\n2.1 Using race-like categories?\\nWhile an area of major concern for critical race and technology\\nscholarship, the use of racial categories in algorithmic fairness re-\\nsearch (i.e. the research community which has emerged around\\nvenues like FAT* and AIES) has largely gone unquestioned. There\\nare two important exceptions. First, in the original Gender Shades\\npaper which helped establish intersectional testing, Buolamwini\\nand Gebru note the instability of race and ethnicity labels, which\\njustifies their use of skin tone [ 22]. Second, the critique levied by\\ncomputational social scientist Sebastian Benthall and critical race\\nscholar Bruce D. Haynes [ 12] describes how little attention has\\nbeen given to the meaning of protected class labels, the political\\norientation of class labels, and the normative assumptions of ma-\\nchine learning design. They focus on the typical referent group\\n– African-Americans in the US – and discuss how \"racial classi-\\nfication is embedded in state institutions, and reinforced in civil\\nsociety in ways that are relevant to the design of machine learning\\nsystems.\" Race is invoked because \"racial classification signifies\\nsocial, economic, and political inequities anchored in state and civic\\ninstitutional practices.\"\\nWe wholly agree with their overall argument, which is that\\nalgorithmic fairness research has been largely silent on the issues\\n1Personal correspondence, Jeff Larson, August 19, 2019.\\n2Personal correspondence, Pamela Oliver, August 19, 2019.\\n502Towards a Critical Race Methodology in Algorithmic Fairness FAT* ’20, January 27–30, 2020, Barcelona, Spain\\nof racial statistics and categories, and that in that silence, has thus\\nreified them. In their criticism of the COMPAS debate, we echo\\ntheir argument:\\nrather than taking racial statistics... at face value, the\\nprocess that generates them and the process through\\nwhich they are interpreted should be analyzed with\\nthe same rigor and skepticism as the recidivism pre-\\ndiction algorithm. Thematically, we argue that racial\\nbias is far more likely to come from human judgments\\nin data generation and interpretation than from an al-\\ngorithmic model, and that this has broad implications\\nfor fairness in machine learning [12, pp. 291].\\nWe diverge from their critique, not necessarily in their problem\\nformation, but in their solution. They offer up a narrow path to\\nfollow which plots a course between the Scylla of \"[m]achines\\nthat ... [reify] racial categories that are inherent unfair\" and the\\nCharybdis of \"systems that allocate resources in ways that are\\nblind to race [which] will reproduce racial inequality in society\"\\n[12, pp. 294]. In their solution, they propose using an unsupervised\\nmachine learning method to create race-like categories which aim to\\naddress \"historical racial segregation with reproducing the political\\nconstruction of racial categories.\" One virtue of this method is that\\nit is attentive to creating a metric which makes sense with respect\\nto a particular social inequality, that is, spatial segregation. This is\\nsomething we agree with in our adoption of a multidimensional\\nperspective below.\\nWe’d like to raise a few complexities in their formulation which\\nmake us hesitant to adopt their solution. First, it would be a grave\\nerror to supplant the existing categories of race with race-like\\ncategories inferred by unsupervised learning methods. Despite the\\nrisk of reifying the socially constructed idea called race, race does\\nexist in the world, as a way of mental sorting, as a discourse which is\\nadopted, as a social thing which has both structural and ideological\\ncomponents. In other words, although race is social constructed,\\nrace still has power. To supplant race with race-like categories for\\nthe purposes of measurement sidesteps the problem.\\nSecond, supplanting race with race-like categories depends highly\\non context, namely how race operates within particular systems\\nof inequality and domination. Benthall and Haynes restrict their\\nanalysis to that of spatial segregation, which is to be sure, an im-\\nportant and active research area and subject of significant policy\\ndiscussion (e.g. [ 76,99]). However, that metric may appear illegi-\\nble to analyses pertaining to other racialized institutions, such as\\nthe criminal justice system, education, or employment (although\\none can readily see their connections and interdependencies). The\\nway that race matters or pertains to particular types of structural\\ninequality depends on that context and requires its own modes of\\noperationalization.\\nThird, and relatedly, as Wendy Chun and Ruha Benjamin have\\ndiscussed [ 11,24], race operates both with andastechnology. At the\\nsame time we focus on the ontological aspects of race (what is race,\\nhow is it constituted and imagined in the world), it is necessary to\\npay attention to what we do with race and measures which may be\\ninterpreted as race. The creation of metrics and indicators which\\nare race-like will still be interpreted as race. As we discuss more\\nbelow, the example of genomics research is indicative. Even asgenomics researchers warn not to interpret genetic ancestry as race\\nor ethnicity [ 15,122], this has not prevented people (e.g. customers\\nof direct-to-consumer genetic testing companies) from interpreting\\nthem as such [98].\\nFinally, we’d like to underline the infrastructural criticism embed-\\nded in the act of categorization itself. By infrastructure, we refer to\\nthe way that classifications and standards form the infrastructure of\\nour existing information society [ 17]. Inverting infrastructures and\\nseeking their fissures allows us to understand how they are used\\nin practice and how politics are embedded in their creation. This\\npoint dovetails with the critiques levied by abolitionist activists,\\nscholars, and technologists [ 1,9,11]. J. Khadijah Abdurahman [ 1]\\nargues that to study algorithmic fairness is to sidestep the problem\\nbeyond the algorithmic frame, that we, with communities who are\\ndisproportionally affected by redlining, predictive policing, and\\nsurveillance, should not just contest how \"protected classes within\\nalgorithms are generated-but [should] viscerally reject the notion\\nhuman beings should be placed in cages in the first place. \" The task\\nof using racial classifications at all requires us to expand the frame\\nto consider what the larger implications of classifying are. Who is\\ndoing the classifying? For what purpose are they classifying and to\\nwhat end?\\n3 HISTORIES OF RACIAL CATEGORIZATION\\nWe recount histories of racial category construction by administra-\\ntive and scientific bodies as a means of highlighting the decidedly\\nsocial constructivist notion of race. As Hacking [ 48] has said, social\\nconstructivism is the act of \"making up people.\" Social construction\\ndoes not mean that things in the world are not \"real\" [ 31], but that\\nthat thing – be it a racial, gender, or class category – was brought\\ninto existence or shaped by historical events, social forces, political\\npower, and/or colonial conquest, all of which could have been very\\ndifferent. When we demonstrate that something is socially con-\\nstructed, it becomes clear that it could be constructed differently,\\nand then we can start to demand changes in it [48, pp. 6-7].\\nRace as a concept is widely acknowledged by the social sciences\\nto be a social construction. In the racial formation framework, Omi\\nand Winant [ 86] discuss how race is both real and socially con-\\nstructed. The social constructedness of race decenters the concept\\nas a property of individuals determined by phenotypical properties.\\nInstead, social constructivism places race within specific history\\nand context. The constructedness of race in any given point in time\\nis tied to the specific racial project. A racial project is \"simultane-\\nously an interpretation, representation, or explanation of racial\\nidentities and meanings, and an effort to organize and distribute re-\\nsources (economic, political, cultural) along particular racial lines\"\\n[86, pp. 56]. Those racial projects can range in scale and kind, in\\nthe microinteractional to the structural.\\nRace is not something that arrived whole cloth, but needed to be\\nmade up. Race, although socially constructed, of course has very\\nsalient material effects. Accordingly, the most modern understand-\\ning of race argues that it is inauthentic to conceptualize race as a\\nnatural property of individuals. The \"naturalization\" of race and\\nracial categories do not come from nowhere. It has been constructed\\nand been continually reproduced as part of a project of upholding\\n503FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\na particular type of racial project – one that is historically and cul-\\nturally bound. Historian Ibram X. Kendi, for instance, thoroughly\\noutlines the history of race and racial projects from 15-century\\nEurope to the present [ 64]. Projects that \"misrecognize\" race as\\nnatural [ 27] are driven by the hegemonic nature of racial projects\\n[86]. It would be more accurate to describe race as relational and as\\na property of institutions, organizations, and larger structures [ 93].\\nRace may be more accurately discussed as having relational quali-\\nties, with dimensions that are symbolic or based on phenotype, but\\nthat are also contingent on specific social and historical contexts.\\n3.1 Classification and the Racial History of\\nSocial Statistics\\nClassification is a process imbued with social, economic, and orga-\\nnizational imperatives [ 17,38]. Those who do the categorizing have\\ntheir own institutional and occupational priorities. Categories them-\\nselves become a type of infrastructure: they are the ground upon\\nwhich other structural and ideational elements are built. When they\\nwork, they are invisible, but when they break down, the boundaries\\nand assumptions begin to show. With the development of markets\\nand the rise of technological innovation and actuarial science, classi-\\nfication of individuals has moved from the more general assessment\\nbased on subgroup distinction to individuation based on market\\nimperatives.\\nAlthough in the modern neoliberal era, markets have been an\\nemerging force in categorization and segmentation [ 39], the en-\\ntity tasked with categorization above all others has been the state.\\nAlmost as soon as humans began living in agriculturally-based\\ncommunities, it became necessary to count the number of people\\nliving under the control of a certain party [ 107]. Categorization was\\nalso used to streamline counting items of like type – such as trees\\nin a forest, grain, or human beings. Further, since early grain-based\\nagriculture necessitated vast numbers of essentially slave-laborers,\\nrulers needed to know how many slaves there were and moreover,\\nwho counted as a slave. As Scott argues, during the agricultural\\nrevolution there was initially strong resistance to transitioning\\nfrom life as a hunter-gatherer to a more hierarchical, regimented,\\nand predictable existence as a slave harvesting grain and to \"being\\ncounted\" by newly-created states [107].\\nThe modern discipline of statistics grew out of a bureaucratic\\nneed to manage large populations and natural resources [ 52]. More\\nspecifically and perniciously, the field of social statistics emerged\\nfrom the need to make differentiations between white Europeans\\nand their descendants, and other peoples. Francis Galton was a\\nmajor figure in the development of social statistics; he was also a\\neugenicist and a major proponent of using statistics as a means to\\njustify racial superiority of European-origin peoples [ 16,124,125].\\nAs Galton writes in his Hereditary Genius :\\nThe natural ability of which this book mainly treats,\\nis such as a modern European possesses in a much\\ngreater average share than men of the lower races.\\nThere is nothing either in the history of domestic\\nanimals or in that evolution to make us doubt that\\na race of sane men may be formed, who shall be as\\nmuch superior mentally and morally to the modernEuropean as the modern European is to the lowest of\\nthe Negro races (quoted in [124]).\\nThe practice of racial classification that emerged in social sta-\\ntistics is closely tied to the project of nation-building, in multiple\\nsenses. In the first case, racial classification was a prerequisite to\\nthe project of European colonization of the Americas. The racial-\\nization of chattel slavery is a modern phenomenon. European colo-\\nnial projects developed alongside Enlightenment ideas of liberal\\ndemocracy. Creating structures of racial stratification necessitated\\nideational components that would legitimate the enslavement of\\nAfricans [ 125]. Until the early 20th century in the West and the\\nrise of cultural anthropologists such as Franz Boas, there was wide-\\nspread support for eugenics and scientific racism, and the belief\\nthat the state must intervene to protect the physical and mental\\ncharacteristics of the white race. In 1926, 23 of 48 states had laws\\npermitting sterilization [ 106]. At the height of miscegenation leg-\\nislation (that is, laws against marriages which were \"interracial\"\\nor \"cross-cultural\" in nature), 41 American colonies and states had\\nlaws against the practice [ 88]. As the project of nation-building\\n(more specifically, colonization of non-European nations) waned in\\nthe 1960s, so did the popularity of eugenics as a scientific practice.\\nHowever, as Syed Mustafa Ali notes, the scientific racism under-\\ngirding colonialism has persisted as \"‘sedimented’ ways of knowing\\nand being – based on systems of categorisation, classification, and\\ntaxonomisation and the ways that these are manifested in practices,\\nartefacts and technologies\" [ 2]. Technologies of racial classification,\\nsuch as blood quantum for Native Americans, persist and upon lega-\\ncies of settler colonialism. These technologies become enshrined in\\nfolk understandings of racial percentages, such as genetic ancestry\\n[113], or in law, as blood quantum remains a legal requisite to tribal\\nmembership, tribal constitution, and land claims.\\nSecond, since the founding of the Americas, delineating racial\\nboundaries has been part and parcel of the state-building project.\\nAssessing and evaluating racial and ethnic boundaries is a state\\npractice that coincides not only with a cultural understanding of\\nwho belongs to the polity, but has and continually is a prerequisite\\nfor formal citizenship in many countries. In the pre-war United\\nStates, appeals to citizenship for non-native born residents were\\nfundamentally appeals to whiteness. Lopez documents how appeals\\nto whiteness and the boundaries of the category of whiteness flexed\\nand contracted in legal argumentation [ 73]. The 1790 Naturalization\\nAct restricted citizenship to \"any alien, being a free white person\"\\nwho had been in the United States for at least two years. It wasn’t\\nuntil the passage of the Immigration and Nationality Act of 1952\\nthat the explicit racialized nature of naturalization was restricted,\\nalthough the Act laid the groundwork for the current restrictive\\nrace-based ratio system which exists today. Maghbouleh demon-\\nstrates how, prior to 1952, Iranians could be used as a \"racial hinge\"\\nto argue both for and against claims to whiteness, and how citi-\\nzenship claims critically depended on who could make this claim\\n[75].\\nThird, national censuses are a critical component of state ma-\\nchinery. State enumeration of populations works to facilitate the\\nadministrative and bureaucratic functions of the modern nation-\\nstate. As Scott points out:\\n504Towards a Critical Race Methodology in Algorithmic Fairness FAT* ’20, January 27–30, 2020, Barcelona, Spain\\nState simplifications such as maps, censuses, cadastral\\nlists, and standard units of measurement represent\\ntechniques for grasping a large and complex reality;\\nin order for officials to be able to comprehend aspects\\nof the ensemble, that complex reality must be reduced\\nto schematic categories. The only way to accomplish\\nthis is to reduce an infinite array of detail to a set of\\ncategories that will facilitate summary descriptions,\\ncomparisons, and aggregation. The invention, elabora-\\ntion, and deployment of these abstractions represent,\\nas Charles Tilly [ 116] has shown, an enormous leap\\nin state capacity – a move from tribute and indirect\\nrule to taxation and direct rule [106, pp. 77].\\nFurthermore, censuses not only quantify members of racial so-\\ncial groups, but also create and constitute the boundaries of these\\nsocial groups themselves. Melissa Nobles outlines how racial classifi-\\ncations are taken for granted by policymakers, but how national\\ncensuses not only enumerate polity members into racial categories,\\nbut also help to reinscribe racial categories themselves, that is,\\n\"[c]ensus-taking is one of the institutional mechanisms by which\\nracial boundaries are set\" [ 112, pp. xi]. In the US, the national census\\nis the sole basis for which representation is guaranteed in the House\\nof Representatives and the Electoral College. In the antebellum US,\\nthe Three-Fifths Compromise, in which Black slaves were counted\\nas three-fifths of a human for the purposes of appropriation and\\ntaxation, is the most dramatic example of racial classification in\\ncounting populations. Whereas the passage of the 14th Amendment\\nguaranteed all Americans being counted as a whole person regard-\\nless of race, racial categories defined in the Census in the latter part\\nof the 19th Century reflected fractional referents to Black racial lin-\\neage based in the ascendant movement of eugenics and race science\\nmentioned above. Categories of \"octoroon\" and \"quadroon\", along\\nwith \"mulatto\" denoted fractional heredities of the Black population\\n[112].\\nBecause of the allocative nature of census-taking and the bureau-\\ncracies associated with the practice, what goes into the census is\\npolitically contested by mobilizing racial and ethnic groups. The\\npolitical contestation of the census has been laid bare recently with\\nthe attempted introduction of a citizenship question into the US\\nCensus and its potential for its dampening effects on participation\\nby Latinx groups [ 72]. For instance, after the 1950 Census, in light\\nof the discoveries of dramatic undercounts of non-white popula-\\ntions and with rising pressures from the Civil Rights Movement,\\nthe 1960 Census shifting from interview identification to asking\\nindividuals to self-identify their race. As Snipp rightly points out,\\nthis procedural change was a \"fundamental redefinition of race.\"\\nPopulation statistics formerly based on phenotypical appearances\\nwere now based on \"cultural affiliation and other deeply held per-\\nsonal considerations beyond the pale of conventional demographic\\ninquiry\" [ 112, pp. 570]. This shift was not only bureaucratic but\\nalso (perhaps unknowingly by the Census Bureau) ontological: race\\nas measured by the Census aligned more with the social construc-\\ntionism and less with essentialism. The Office of Management and\\nBudget (OMB) Directive No. 15 of 1977 instituted a standard in\\nracial classifications used by government agencies and developed a\\nstandard for five distinct groups: (a) American Indians and AlaskaNatives, (b) Asian and Pacific Islanders, (c) Non-Hispanic Blacks, (d)\\nNon-Hispanic Whites, and (e) Hispanics. The impact of this direc-\\ntive had far-reaching implications, as this categorization \"permeated\\nevery level of government, many if not most large corporations,\\nand many other institutions such as schools and nonprofit orga-\\nnizations\" ([ 87], cited in [ 112]). The 1997 revision of Directive No.\\n15 separated out Native Hawaiians and other Pacific Islanders, set\\nHispanics and Latinos apart as an ethnic group (rather than a racial\\ngroup), and allowed respondents to provide multiple responses to\\nracial heritage [112].\\nDramatic changes in governance have large downstream effects\\nfor the accuracy of census counts and their categories. Khalfani et\\nal. document the differences between the South African numbers\\nbefore and after the reign of Mandela’s African National Congress,\\nand find that the raw counts and the estimations from aerial pho-\\ntography both dramatically undercount the majority African popu-\\nlations, as well as women [ 65]. The US Census Bureau spends a non-\\ntrivial amount of effort getting enumeration correct for marginal-\\nized groups; however, those efforts can be stymied with the winds of\\npartisanship and political administration. Even without the issues\\nraised by a potential citizenship question, decreased or flat-lined\\nfunding has left the Census Bureau understaffed and unable to\\nthoroughly test new methodologies [32].\\nIn sum, the act of classification cannot be divorced from the\\ngroup who is doing the classification and the organizational and\\ninstitutional pressures of performing classification. Racial classifi-\\ncation has a long history within quantitative social science, and,\\nin some ways, is the reason that social statistics have developed\\nthe way they did. That history is grounded in eugenicist thought\\nand practice, and much of that work provided the infrastructure\\nfor census-taking in the US, Canada, Latin America, and South\\nAfrica. More expansive categorization resulted from social and po-\\nlitical movements in those countries. In short, categorization itself\\nis a technological infrastructure within which institutional racism\\ncontinues to reside. In this way, we can realize the ways in which\\nrace intersects with technology but also as a technology in and\\nof itself. The framing of race as technology shifts the terrain of\\nthe discussion – rather, from what race is and how we can classify\\nindividuals, to what we can do with race and how to make it do\\ndifferent (and explicitly anti-racist) things.\\n3.2 The multidimensionality of race\\nAs highlighted by the 1960 Census shift from ascribed to self-\\nidentification, classification is largely contingent on the particular\\nappraisal of race used. When we say \"race\", we may be discussing\\nself-identification, but we also may be referring to phenotypical\\nfeatures or observed assessments from third parties. Racial classifi-\\ncations are uneasily balanced not only on the particular unstable\\nequilibrium of racial projects, but also on the micro-level processes\\nof race appraisals themselves. When it comes to measurement and\\noperationalization, \"race\" is not a single variable, but many differ-\\ning and sometimes competing variables. Roth [ 95] refers to this\\nconstellation of variables as the \"multiple dimensions\" of race. Race\\nmanifests in many ways in the real world. Each of these different\\ndimensions can be measured in a different manner for empirical\\n505FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\nresearch, and each of these different dimensions have distinct down-\\nstream outcomes. It may therefore be more appropriate to study a\\nparticular outcome using measurement which reflects a respective\\ndimension of race. Table 1 reproduces, with minor amendments,\\nthe table from [95] which outlines each of these dimensions.\\nRacial identity andracial self-classification refer to subjective\\nself-identification of race. In theory, self-identification is one di-\\nmension but in practice these two are measured in different ways\\nand have differing downstream outcomes. Racial identity can be\\nthought of as the sense of self, the identity and broad group with\\nwhich someone belongs. Accordingly, its measurement instrument\\nis that of an open-ended interviewer or survey question. Racial\\nself-classification refers to the discrete categories that one marks\\non intake forms, census surveys, and self-identification documents\\nfor employment. This dimension, as noted earlier, is highly con-\\nstrained by the categories determined apriori by institutional and\\norganizational directives, such as OMB Directive No. 15.\\nObserved race refers to the race which others believe ascribed\\nto an individual. This may be based upon first impression by an\\ninterviewer or annotator. The third-party can make an assessment\\nbased solely on appearance orinteraction. Appearance-based ob-\\nserved race depends on readily observable characteristics, whereas\\ninteraction-based observed race depends on language, accent, and\\nother physical, aural, or social cues. Reflected race is the race which\\nan individual believes that others ascribe to them. Although this\\nis a first-person evaluation, it is the first-person evaluation of a\\nthird-party appraisal.\\nPhenotype refers to the set of objective characteristics which\\ncharacterize appearance. A significant literature revolves around\\nskin color appraisals as a means for determining particular social\\noutcomes (e.g. [ 82,114]) and the use of skin color as a means of eval-\\nuating facial recognition systems (e.g. [ 22,92]). Other race-related\\nphenotypical features include hair texture and color, eye shape and\\ncolor, and lip shape. The related outcomes for phenotypical features\\nare the same as those for observed race, but are not constrained in\\nmeasurement by discrete racial categorization3.\\nThe dimensions of race noted here are, of course, unstable across\\ntime, place, and context. Just as racial classifications themselves\\nare tied to particular racial projects, modes of self-identification,\\nobserved race, and reflected race will be tied to the dynamics of\\nthat project. Self-identification has been shown to be responsive to\\none’s arrest records [ 89] and to one’s class position [ 115]. Observed\\nrace depends on the interviewer or annotator who is doing the\\nobserving. Reflected race depends not only on phenotype but also\\non the social status of the group in which the individual believes\\nthey are being recognized as. These positions shift across time,\\nnot only across the life course but also across the longue durée of\\ndifferent racial projects.\\nThree major implications follow from the of multiple dimensions\\nparadigm. The first is that there may be inconsistencies across dif-\\nferent dimensions of race. A single individual can have differing\\nracial appraisals. In the health field, for instance, Roth cites several\\nstudies in which individual self-identification differs from medical\\nrecords, interviewer classifications, or death certificates. Second,\\n3In a sense, the \"observed race/appearance-based\" dimension and the phenotype\\ndimension are collapsed for computer vision systems, because a computer vision\\nsystem can only \"see\" the objective and phenotypical.the dimensions can cross the boundaries between each other. Phe-\\nnotype and reflected race can impact self-identification but do not\\nfully determine it. However, the third and most important implica-\\ntion, also highlighted in Table 1, is that different dimensions will\\nbe associated with differing outcomes. Therefore, using a measure-\\nment of race which does not reflect social processes associated\\nwith it may misrepresent the scope of racial disparities at best and\\nunderreport them at worst.\\n4 LESSONS FROM OTHER DISCIPLINES\\nThe variance in what the concept of race can mean has been noted\\nas \"antithetical to the tenets of scientific research, which, in its\\nideal form, demands that analytical variables be consistent and\\ntheir categories mutually exclusive\" [ 70]. Yet, the social centrality\\nof race positions it as a critical concept in many fields of study. In\\nthis section, we review how other disciplines, such as public health\\nresearch, biomedical research, and studies of social inequality, have\\ngrappled with the use of racial categories within their disciplines.\\nWe surface these discussions to understand how algorithmic fair-\\nness research can learn from these methodological approaches.\\n4.1 Limitations in operationalizing race\\nScholars from a range of disciplines have observed a widespread\\nlack of clarity around the use of racial variables within their respec-\\ntive fields. Race is often inconsistently conceptualized and measured\\nacross studies; definitions and processes of operationalization tend\\nto be insufficiently documented. These inconsistencies pose serious\\nchallenges for the validity and utility of research results. For exam-\\nple, inconsistent categories and classification schemes can result\\nin mismatches between different measures of race for the same\\nindividual across different time points or across studies. This in\\nturn can significantly impact health statistics [ 62,101], population\\nstatistics [ 8,74], and measures of social inequality [ 102]. Country-\\nspecific understandings of what \"race\" references and local racial\\ncategorization schemes pose significant challenges for international\\ncomparisons [96].\\nStandardizing racial taxonomies and measurement practices does\\nnot sufficiently mitigate the many concerns scholars have raised\\nregarding the use of racial categories in scientific studies. In fact, the\\nwidespread uncritical adoption of racial categories, standardized\\nor not, can erode awareness of the social and political histories of\\nracial taxonomies and reify racial categories as natural kinds [ 19,30,\\n42,43,78,100,111]. The reification of race as a natural category can\\nin turn re-entrench systems of racial stratification which give rise to\\nreal health and social inequalities between different groups [ 62,110].\\nThe unquestioning use of racial categories in scientific research\\ncan also lead to misplaced conclusions. For example, the use of race\\nas a natural category can obfuscate the environmental, social, and\\nstructural factors that contribute to health disparities [ 29,110] and\\nlead to the racialization of certain diseases [ 19,30,37]. The genomic\\nera in particular has given rise to new concerns regarding the use of\\nrace in technologies and research. Race-based pharmaceuticals have\\nbeen critiqued for their part in reifying racial categories as markers\\nof biological difference [59, 63]. Genetic ancestry testing has been\\ncriticized for its potential to promote biological essentialilsm and\\nreinforce race privilege amongst those already experiencing it [ 15,\\n506Towards a Critical Race Methodology in Algorithmic Fairness FAT* ’20, January 27–30, 2020, Barcelona, Spain\\nDimension Description Typical Measurement Outcomes it may be appropriate for\\nstudying\\nRacial identity Subjective self-identification, not\\nlimited by pre-set options.Open-ended self-identification\\nquestionPolitical mobilization; assimilation;\\nsocial networks; voting; residential\\ndecision-making; attitudes\\nRacial Self-Classification The race you check on an official\\nform or survey with constrained\\noptions (e.g. the Census)Closed-ended survey question Demographic change; vital statis-\\ntics; disease and illness rates\\nObserved Race The race others believe you to be Interviewer classification Discrimination; socioeconomic dis-\\npartiies; residential segregation;\\ncriminal justice indicators; health\\ncare/service provision\\n- Appearance-Based Observed race based on readily ob-\\nservable characteristicsInterviewer classification with in-\\nstructions to classify on first obser-\\nvationRacial profiling; discrimination in\\npublic settings\\n- Interaction-Based Observed race based on character-\\nistics revealed through interaction\\n(e.g. language, accent, surname)Interview classification with in-\\nstructions to classify after interac-\\ntion or surveyWorkplace discrimination; housing\\ndiscrimination; language/accent-\\nbased discrimination\\nReflected Race The race you believe others as-\\nsume you to beA question such as \"What race do\\nmost people think you are?\"Self-identification processes; per-\\nceived discrimination\\nPhenotype Racial appearance Usually interviewer classification,\\nbut also \"objective\" characteristics\\nsuch as: skin color; hair texture and\\ncolor; nose shape; lip shape; eye\\ncolorDiscrimination; socioeconomic dis-\\npartiies; residential segregation;\\ncriminal justice indicators; health\\ncare/service provision\\nTable 1: Multiple dimensions of race. Reproduced and amended from [95]. Note that we have excluded \"racial ancestry\" from\\nthis table. Genetics, biomedical researchers, and sociologists of science have criticized the use of \"race\" to describe genetic\\nancestry within biomedical research [40, 49, 84, 122], while others have criticized the use of direct-to-consumer genetic testing\\nand its implications for racial and ethnic identification [15, 91, 113].\\n98]. Finally, bioinformatics tools and research practices themselves\\ncontribute to the essentialization of race [10, 42, 43, 51].\\nThere has also been significant debate about the appropriate\\nuse of racial variables in studies aimed at identifying causal ef-\\nfects [ 46,54,55,58,67]. Objections to the use of race as a causal\\nvariable are often couched in terms of manipulability, embodied\\nby the slogan \"no causation without manipulation\" [ 54]. These\\narguments often point to the physical impossibility of manipu-\\nlating race, thus precluding its use as a causal variable. Several\\nadditional, more sociologically-grounded objections to the use of\\nrace as a causal variable have emerged within social statistics [ 55],\\nanti-discrimination legal scholarship [ 67], and health research [ 46].\\nThese objections point to the fundamental role race plays in struc-\\nturing life experiences, making it nonsensical to talk about two\\nindividuals being identical, save for race. Stated another way, at-\\ntempts to isolate the treatment effect of race are often based on a\\n\"sociologically incoherent conception of what race references\" [ 67].\\nSeveral scholars have pointed to the significant social conse-\\nquences that arise when race is misconstrued as a variable that\\ncan produce causal effects [ 3,46,58,124]. First, without proper\\ncontextualization of results, there is a tendency to misattribute the\\ncausal mechanisms of difference the racial categories themselves[3,46,124], further reifying race as a natural category. Second,\\nmisidentifying race, rather than racial stratification, as the root\\ncause of social and health disparities can lead to misplaced conclu-\\nsions and ineffective public policy interventions [46].\\n4.2 Critical race methodologies\\nThese critiques have given rise to a host of critical race methodolo-\\ngies informing the use of racial categories within these respective\\ndisciplines. To begin, the choice to use racial categories at all should\\nbe carefully examined, particularly in biological contexts. Despite\\nwidespread agreement that racial categories do not describe geneti-\\ncally distinct populations [ 84], genomics researchers have grappled\\nfor over a decade with the utility and harm of using racial categories\\nto frame research studies and communicate scientific results. The\\ncontext of use is key to assessing the appropriate use of racial cate-\\ngories. For example, when adopted as a social or political category,\\nrace has utility in genomics, e.g. to document biological differences\\nthat result from processes of racial stratification. However, when\\ndisconnected from their social and political histories, racial cate-\\ngories have no place in biological research [ 30,90,122]. Genetics,\\nbiomedical researchers, and sociologists of science have emphasized\\nthe importance of distinguishing ancestry from racial taxonomies\\n507FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\nwithin biomedical research [ 40,49,84] and have warned against the\\ndangers of of using racial categories to describe genetic variation\\n[14, 30, 122].\\nAs noted above in our discussion of Benthall and Haynes, race\\ncannot and should not be done away with entirely. The key chal-\\nlenge here is to \"denaturalize without dematerializing it\" [ 78] by rec-\\nognizing race as a multidimensional, relational, and socially situated\\nconstruct. This begins with centering the process of conceptualizing\\nand operationalizing race, critically assessing the choice of cate-\\ngories and measurement schemes, and fully articulating and justify-\\ning these decisions in academic communications [ 46,60,71,77,83].\\nIn the context of causal studies, several methodologies have\\nbeen proposed to mitigate the risks of reifying race as an entity\\nthat produces causal effects. For example, some researchers have\\nconsidered studies that manipulate properties associated with race,\\nsuch as names [ 13]4. Sen and Wasow’s ’bundle of sticks’ framework\\ntheorizes race as a composite variable that can be disaggregated\\ninto constitutive elements, some of which can be manipulated [ 109].\\nEpidemiological and public health researchers have increasingly\\nshifted their practices towards the study of social determinants of\\nhealth, and in particular, the multiple ways racism affects health\\noutcomes [ 41,61,71,117,119,120]. This shift from studying effects\\nofrace to the effects of racism is a central component of critical\\nrace methodologies emerging within other disciplines. For instance,\\nFord and Hawara propose a methodology for the operationalization\\nof race within health equity studies as a multidimensional, context-\\nspecific variable with a relational component to explicitly capture\\nthe effects of racial stratification on health outcomes [34].\\nMoreover, appropriately contextualized descriptive studies can\\nbe an important tool for identifying and understanding patterns\\nof inequality. When race is utilized as a descriptive category, the\\nchoices about what categories to use and how to assign individuals\\nto categories significantly impacts the results of analysis [ 18,102,\\n114,115]. For example, Howell and Emerson compare the effec-\\ntiveness of five different operationalizations of race in predicting\\ndifferent measures of social inequality. The study found each oper-\\nationalization told a different story, pointing to the importance of\\ncritically evaluating racial categories and measurement schemes,\\nas well as the importance of articulating these decisions in the\\ncommunication of results [56].\\nIn recent years, anti-racist efforts have emerged within pub-\\nlic health that more thoroughly integrate critical race theory into\\nmethodologies and discourse [ 35,36]. While earlier studies of the\\nsocial determinants of health tend to center individual and interper-\\nsonal racialized experiences, this new paradigm shifts focus to the\\ninstitutional and structural conditions that shape racial disparities\\nin health [ 44,45,110,118]. Anti-racist practice also emphasizes\\nthe importance of researchers recognizing their privilege and posi-\\ntionality and the way this might shape their practice. For example,\\nthe Public Health Critical Race Praxis offers a self-reflexive and\\nrace-conscious research methodology that centers discourse and\\npractice around the perspectives of socially marginalized groups,\\nbuilding on community-based participatory approaches [35].\\n4However, Kohler-Hausmann has been critical of such \"audit\" studies for not being\\nable to isolate the treatment effect for race, although they do, in some circumstances,\\n\"provide evidence of a constitutive claim that grounds a thick ethical evaluation\" [ 67,\\npp. 33-34]5 IMPLICATIONS FOR USING RACE IN\\nALGORITHMIC FAIRNESS RESEARCH\\nWe observe several widespread tendencies when using race within\\nalgorithmic fairness research. First, frameworks for describing and\\nmitigating unfairness adopt a simplistic conceptualization of race\\nas a single dimensional variable that can take on a handful of val-\\nues. This simplification erases the social, economic, and political\\ncomplexity of the racial categories. Further, methodologies built\\nupon this conceptualization of race tend to treat groups as inter-\\nchangeable, obscuring the unique oppressions encountered by each\\ngroup. We argue that this framing limits the effectiveness of fair-\\nness analysis and interventions and risks reifying racial categories\\nin the process.\\nSecond, the process of conceptualizing and operationalizing race\\nfor the purposes of studying or mitigating different aspects of algo-\\nrithmic fairness has – with the exceptions noted above – received\\nlittle attention. Moreover, racial categories are frequently adopted\\nwith little attention given to the histories of the categories or suit-\\nability of the categories for the fairness assessment.\\nWe discuss the importance of focusing on categories and mea-\\nsurement processes, and fully articulating these decisions in com-\\nmunication of results. Lastly, we discuss the use of disaggregated\\nanalysis and the implications for incorporating a more nuanced\\nunderstanding of measurement for these analyses.\\n5.1 Race and group fairness\\nGroup fairness criteria represent a class of algorithmic fairness\\ndefinitions predicated on clearly defined subgroups in the dataset.\\n\"Fairness\" is obtained by equalizing a particular statistic, or set\\nof statistics, of the classifier across groups. Many different group\\nfairness criteria have been proposed. For example, demographic\\nparity requires equal rates of positive prediction [ 33] across groups.\\nEquality of odds [ 50], also referred to as equalized mistreatment\\n[123], requires equal false positive and false negative rates across\\ngroups. Equal opportunity [ 50] restricts this requirement to only\\na single value of the true outcome. Calibration (or test fairness)\\n[23,66] requires the actual outcome to be independent of protected\\nattributes, conditioned on estimated outcome.\\nWhile limitations of group fairness criteria have been previously\\nexplored [ 26,47], here we focus our examination on the conceptu-\\nalization of race embedded within this framework. By abstracting\\nracial categories into a mathematically comparable form, group-\\nbased fairness criteria deny the hierarchical nature and the social,\\neconomic, and political complexity of the social groups under con-\\nsideration. Most notably, critical race theorists and Black feminist\\nthinkers have criticized group fairness approaches for their under-\\nlying ideal, liberal approaches to ameliorating past harms.\\nBlack feminist cultural geographer Katherine McKittrick, fol-\\nlowing Patricia Hill Collins, discusses how such approaches have\\nresulted in a ’flattened geography’ which obscures the unique op-\\npressions encountered by Black women, instead treating oppressed\\nsocial groups as interchangeable [ 25,79]. Ladson-Billings and Tate,\\nin their critical race critique of liberal education paradigms, explain\\nthat the tension between and even among different racial groups are\\nnot adequately examined or understood and assume \"that all differ-\\nence is both analogous and equivalent\" [ 68]. Political philosopher\\n508Towards a Critical Race Methodology in Algorithmic Fairness FAT* ’20, January 27–30, 2020, Barcelona, Spain\\nCharles W. Mills notes that the liberal underpinnings of both theo-\\nretical and methodological approaches towards equalization have\\nhistorically tended to have significant detrimental consequences\\nfor racial minorities and only serves to advance a white \"racial\\ncontract\" [80].\\nIn short, group fairness approaches try to achieve sameness\\nacross groups without regard for the difference between the groups.\\nGroup fairness offers an incomplete version of what a race-conscious\\npolicy would be. This treats everyone the same from an algo-\\nrithmic perspective without acknowledging that people are not\\ntreated the same. As political philosopher Elizabeth Anderson notes,\\n\"[s]tandard conceptions of distributive equality, embodied in ideas\\nof equality of opportunity... fail to consider how such opportunities\\nbuild in group hierarchy\" [4].\\n5.2 Conceptualizing and operationalizing race\\nThe question of how best to operationalize race for the purposes\\nof studying or mitigating different aspects of algorithmic unfair-\\nness has received little attention. By discounting the considerations\\nthat go into operationalizing race, the scope and effectiveness of\\nsubsequent analysis and interventions fail to interrogate how the\\nparticular operationalization and measurement affect a given out-\\ncome. For instance, referring to Table 1, while observed race may\\nbe more important for studying discrimination, self-identification\\nmay be more suitable for studying identity formation and voting\\nbehavior.\\nWe suggest centering the process of conceptualizing and opera-\\ntionalizing race when working with racial variables. In particular,\\nwe urge algorithmic fairness researchers to critically evaluate exist-\\ning racial schemas. Because of data limitations, we are most often\\nbound to the categories provided by census categories or other\\ntaxonomies which stem from bureaucratic processes. We know\\nfrom the histories outlined above that these categories are unstable,\\ncontingent, and rooted in racial inequality.\\nFollowing recent work around measurement and fairness [ 5,57],\\nwe suggest taking seriously the problems of measurement modeling\\nwhen considering racial variables. Race, like fairness, is itself a\\ncontested concept, and it follows that adopting a multidimensional\\nview is one strategy forward for approaching this. Recently, Roth\\nhas called more broadly for a \"sociology of racial appraisals\" in\\nwhich we better theorize how observed race has changed over time,\\nand how these changes influence norms of classification [ 97]. Given\\nthat most, if not all, of algorithmic fairness research is ostensibly\\nconcerned with anti-discrimination, this charge should be taken\\nseriously.\\nAs a helpful analogue, a growing body of survey research has\\nhighlighted the impact that racial category and measurement choices\\ncan have on outcomes being measured [ 102–104]. Similarly, we\\nemphasize that the various choices that go into the operational-\\nization of race for the purposes of fairness-informed analysis or\\ninterventions significantly impact the result. Therefore, we need\\nto be transparent about definitions, categories, measurements, and\\nmotivations for racial schema used in research and publication. If\\npossible, multiple measures of race should be collected. In addition,\\nmeasurement of race should be considered as an empirical problem\\nin its own right.In domains where the categorization is based solely on observ-\\nable characteristics, e.g. image datasets are annotated by third-\\nparties, care needs to be taken to ensure race is not reduced to\\nphenotype. This consideration can inform how the subgroups them-\\nselves are defined (e.g. shifting conceptually from racial categories\\nto categories defined along phenotypic lines) and the way in which\\nthe dataset and analysis results are communicated with the larger\\nresearch community (e.g. detailing procedures for determining cate-\\ngories and category membership, ensuring a distinction is made be-\\ntween phenotypically-determined categories and high-level racial\\ncategories). In the context of disaggregated evaluations in computer\\nvision domains, the FAT* community has already seen a marked\\nshift towards defining groups based on phenotypic properties rather\\nthan racial categories, thanks to the pioneering work of Buolamwini\\nand colleagues.\\nHowever, we also note that shifting to phenotypically-determined\\ncategories does not provide a full-stop solution. Given the dark his-\\ntory of physiognomy and phrenology [ 121], in particular their role\\nin promoting scientific racism and eugenics, researchers should\\nbe careful when imposing categories based on, for example, facial\\nlandmarks (e.g. IBM’s diversity in faces dataset has labelled facial\\ndimensions, proportions, etc.). Doing so risks devolving into using\\nphenotypical features as inputs for a predictive models for individ-\\nual characteristics and internal psychological states. Furthermore,\\nwhile phenotype can be used as a means to understand processes of\\ndiscrimination, practitioners should not equate race to phenotype,\\nnot to mention other biological markers such as genomes.\\n5.3 Disaggregated analysis\\nFrameworks for algorithmic audit studies [ 22,92] and standardized\\nmodel reporting guidelines [ 81] have highlighted the importance\\nof reporting model statistics disaggregated by groups defined along\\ncultural, demographic, or phenotypic lines. In contrast to group\\nfairness definitions, which are often employed as ideals to be met,\\ndisaggregated analysis operates at a descriptive level. These analy-\\nses should begin from a pragmatist understanding of differences\\nand interrogate the most salient aspects of race to be considered for\\nparticular technologies. For instance, in their audits of facial anal-\\nysis technologies, Buolamwini and her colleagues concentrate on\\nthe phenotypical dimensions of race to understand the disparities\\nof these vision-based systems5. Results of disaggregated analysis\\ndepend heavily on the choice of categories and racial measurements\\nused.\\nWithin survey research, sociologists of race have investigated\\nthe utility of different racial categories for the purposes of under-\\nstanding markers of social inequality. Howell and Emerson find that\\na five-fold measure of self-identified race explains more variation\\nin measures of inequality in income, housing, and health in the US\\n[56]. Saperstein and Penner find that racial categorization operates\\nboth as an input and an output to racial inequality. Individuals who\\nexperience an increase in social position (e.g. increased income)\\nmay be more likely to \"lighten\" themselves in survey responses,\\nwhile those who become more disadvantaged (e.g. experiencing\\nunemployment or incarceration) may \"darken\" themselves [ 89,105].\\nThe implication here is that it is necessary to understand the way\\n5It should be noted that they do not take a similar approach towards gender.\\n509FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\nthat a particular racial dimension manifests in measurement and\\nhow it relates to the sociotechnical system under consideration. We\\nencourage algorithmic fairness researchers to explore how different\\nracial dimensions and their attendant measurements might reveal\\ndifferent patterns of unfairness in sociotechnical systems.\\n5.4 Limits of the algorithmic frame\\nBefore undertaking the project of systematizing social groups based\\non race, we should begin by interrogating why race is a relevant\\nfactor in the analysis of the system to begin with. Here, it is criti-\\ncal to expand the scope of analysis beyond the algorithmic frame\\n[108] and interrogate how patterns of racial oppression might be\\nembedded in the data and model and might interact with the re-\\nsulting system. We urge researchers to question the following: is\\ndelineating data along racial lines critical to understanding patterns\\nof fairness produced or reproduced by the system?\\nIn many cases, severe fairness concerns are evident prior to\\nany quantitative race-based analysis of the system’s outputs. For\\nexample, Rashida Richardson and colleagues recently published\\nan extensive survey of the \"dirty data\" used to develop predictive\\npolicing systems in the US [ 94]. The report reveals that in nine out\\nof the thirteen jurisdictions reviewed, predictive policing systems\\ningested data that was generated while the jurisdiction was un-\\nder investigation for corrupt, racially-biased, or otherwise illegal\\npolicing practices. This work represents an analysis centered not\\naround race but around racially disparate policing patterns. This\\nechoes much of the work from within health and biomedical sci-\\nences which emphasizes a racism effect rather than a race effect.\\n5.5 Centering perspectives of marginalized\\ngroups\\nLastly, interventions in the vein of algorithmic fairness need to con-\\nsider the problems of the racially oppressed based on their view of\\ninjustices. Political philosopher Elizabeth Anderson proposes that\\nwe think about injustices from what she calls a non-ideal standpoint\\nmethodology [4]. The methodology is non-ideal insofar as that it\\nviews injustices from the point of view of the aggrieved groups and\\nthe particular harms these groups are facing. The methodology is\\n\"standpoint\"-based insofar as we should begin from the perspec-\\ntives of oppressed groups. Here, we can draw lessons from public\\nhealth scholars who prioritize community-based participatory ap-\\nproaches. These methodologies, which are grounded in critical race\\ntheory and feminist standpoint epistemology, take lived experi-\\nences of marginalized groups as a valuable and essential source of\\nknowledge.\\n6 CONCLUSION\\nIn this article, we have pulled back the frame from the algorithms\\ninvolved in algorithmic fairness themselves and focused on the cat-\\negories which constitute the \"protected classes\" used within these\\nframeworks. We focus on race, given the critical nexus of race,\\ntechnology, and inequality, and the prevalence of race in algorith-\\nmic fairness scholarship. We traced the histories of classification,\\neugenics, and state-sponsored category creation, and warned of the\\ntendency to reify and naturalize racial categories which were theproduct of long histories of inequality. We then noted the multidi-\\nmensionality of race and considered lessons from disciplines which\\nhave been dealing with this issue for decades. Lastly, we review\\nthe algorithmic fairness literature in light of these considerations.\\nWe highlight limitations of algorithmic fairness methodologies that\\nadopt a simplistic and de-contextualized understanding of race and\\noutline several suggestions for algorithmic fairness researchers\\nmoving forward.\\nMost in the FAT* community are what Ruha Benjamin calls\\nthe \"overserved\" [ 11]. In this light, racial classifications need to\\nbe interrogated from the perspective of who they serve. Who is\\ndoing the categorizing and for what purpose? As in the beginning\\nof this article, we echo J. Khadijah Abdurahman, \"it is not just\\nthat classification systems are inaccurate or biased, it is who has\\nthe power to classify, to determine the repercussions / policies\\nassociated thereof and their relation to historical and accumulated\\ninjustice.\" Racial classifications can be an important tool in post-\\nhoc disaggregation analysis. However, we need to know how to\\ndisaggregate, how to contextualize categories of disaggregation,\\nand who that categorization serves.\\nACKNOWLEDGEMENTS\\nThanks to Anna Lauren Hoffmann, Ben Hutchinson, Emma Kaywin,\\nIssa Kohler-Haussman, Joan Fujimura, Lauren Mangels, Simone Wu,\\nand the three anonymous FAT* reviewers for helpful comments.\\nREFERENCES\\n[1] J. Khadijah Abdurahman. Fat* be wilin’. 2019.\\n[2]Syed Mustafa Ali. A brief introduction to decolonial computing. XRDS:\\nCrossroads, TheACM Magazine forStudents, 22(4):16–21, 2016.\\n[3]Walter R Allen, Susan A Suh, Gloria González, and Joshua Yang. Qui bono?\\nexplaining–or defending–winners and losers in the competition for educational\\nachievement. In Tukufu Zuberi and Eduardo Bonilla-Silva, editors, White logic,\\nwhite methods :racism andmethodology , chapter 13, pages 217–237. Rowman\\n& Littlefield Publishers, 2008.\\n[4]Elizabeth Anderson. Toward a non-ideal, relational methodology for political\\nphilosophy: Comments on schwartzman’s \"challenging liberalism\". Hypatia ,\\n24(4):130–145, 2009.\\n[5]McKane Andrus and Thomas K Gilbert. Towards a just theory of measurement:\\nA principled social measurement assurance program for machine learning.\\nInProceedings ofthe2019 AAAI/ACM Conference onAI,Ethics, andSociety ,\\npages 445–451. ACM, 2019.\\n[6]Julia Angwin and Jeff Larson. Propublica responds to company’s critique of\\nmachine bias story. ProPublica, 29, 2016.\\n[7]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias.\\nProPublica, 2016.\\n[8]Stanley R. Bailey, Mara Loveman, and Jeronimo O. Muniz. Measures of \"race\"\\nand the analysis of racial inequality in brazil. Social Science Research , 42(1):106\\n– 119, 2013.\\n[9]Chelsea Barabas, Karthik Dinakar, Joichi Ito, Madars Virza, and Jonathan Zittrain.\\nInterventions over predictions: Reframing the ethical debate for actuarial risk\\nassessment. arXiv preprint arXiv:1712.08238, 2017.\\n[10] Jan Baren-Nawrocka. The bioinformatics of genetic origins: how identities\\nbecome embedded in the tools and practices of bioinformatics. LifeSciences,\\nSociety andPolicy, 9:7, 09 2013.\\n[11] Ruha Benjamin. Race After Technology: Abolitionist Tools fortheNew Jim\\nCode. John Wiley & Sons, 2019.\\n[12] Sebastian Benthall and Bruce D. Haynes. Racial categories in machine\\nlearning. In Proceedings oftheConference onFairness, Accountability, and\\nTransparency, FAT* ’19, 2019.\\n[13] Marianne Bertrand and Sendhil Mullainathan. Are Emily and Greg More Em-\\nployable Than Lakisha and Jamal? A Field Experiment on Labor Market Dis-\\ncrimination. American Economic Review, 94(4):991–1013, September 2004.\\n[14] Catherine Bliss. Racial taxonomy in genomics. Social Science &Medicine ,\\n73(7):1019 – 1027, 2011.\\n510Towards a Critical Race Methodology in Algorithmic Fairness FAT* ’20, January 27–30, 2020, Barcelona, Spain\\n[15] Deborah A Bolnick, Duana Fullwiley, Troy Duster, Richard S Cooper, Joan H\\nFujimura, Jonathan Kahn, Jay S Kaufman, Jonathan Marks, Ann Morning, Alon-\\ndra Nelson, et al. The science and business of genetic ancestry testing. Science ,\\n318(5849):399–400, 2007.\\n[16] Eduardo Bonilla-Silva and Tukufu Zuberi. Toward a definition of white logic\\nand white methods. In Tukufu Zuberi and Eduardo Bonilla-Silva, editors, White\\nlogic, white methods: racism andmethodology , chapter 1, pages 3–27. Rowman\\n& Littlefield Publishers, 2008.\\n[17] Geoffrey C. Bowker and Susan Leigh Star. Sorting Things Out. MIT Press, 2000.\\n[18] Jenifer Bratter and Bridget K Gorman. Does multiracial matter? a study of racial\\ndisparities in self-rated health. Demography, 48:127–52, 02 2011.\\n[19] Lundy Braun, Anne Fausto-Sterling, Duana Fullwiley, Evelynn Hammonds,\\nAlondra Nelson, William Quivers, Susan Reverby, and Alexandra E Shields.\\nRacial categories in medical practice: How useful are they? PLoS medicine ,\\n4:e271, 10 2007.\\n[20] Andre Brock. Life on the wire: Deconstructing race on the internet. Information,\\nCommunication &Society, 12(3):344–363, 2009.\\n[21] Simone Browne. Dark matters: Onthesurveillance ofblackness . Duke Univer-\\nsity Press, 2015.\\n[22] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy\\ndisparities in commercial gender classification. In FAT*, pages 77–91, 2018.\\n[23] Alexandra Chouldechova. Fair prediction with disparate impact: A study of\\nbias in recidivism prediction instruments. BigData, 5 2:153–163, 2017.\\n[24] Wendy Hui Kyong Chun. Introduction: Race and/as Technology; or, How to Do\\nThings to Race. Camera Obscura: Feminism, Culture, andMedia Studies , 24(1\\n(70)):7–35, 05 2009.\\n[25] Patricia Hill Collins. Fighting words: Black women andthesearch forjustice ,\\nvolume 7. U of Minnesota Press, 1998.\\n[26] Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness:\\nA critical review of fair machine learning. In arXiv:1808.00023, 2018.\\n[27] Matthew Desmond and Mustafa Emirbayer. What is racial domination? Du\\nBois Review: Social Science Research onRace, 6(2):335–355, 2009.\\n[28] William Dieterich, Christina Mendoza, and Tim Brennan. COMPAS risk scales:\\nDemonstrating accuracy equity and predictive parity. Northpoint Inc, 2016.\\n[29] Denise J. Drevdahl, Debby A. Philips, and Janette Y. Taylor. Uncontested cat-\\negories: the use of race and ethnicity variables in nursing research. Nursing\\nInquiry, 13(1):52–63, 2006.\\n[30] Troy Duster. Race and reification in science. Science , 307(5712):1050–1051,\\n2005.\\n[31] Dave Elder-Vass. Towards a realist social constructionism. Sociologia,\\nproblemas epráticas, (70):9–24, 2012.\\n[32] Diana Elliott, Rob Santos, Steven Martin, and Charmaine Runes. Assessing\\nmiscounts in the 2020 census. 2019.\\n[33] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and\\nSuresh Venkatasubramanian. Certifying and removing disparate impact. In\\nProceedings ofthe21th ACM SIGKDD International Conference onKnowledge\\nDiscovery andData Mining , KDD ’15, pages 259–268, New York, NY, USA, 2015.\\nACM.\\n[34] Chandra Ford and Nina Harawa. A new conceptualization of ethnicity for social\\nepidemiologic and health equity research. Social science &Medicine , 71:251–8,\\n07 2010.\\n[35] Chandra L. Ford. Public health critical race praxis: An introduction, an inter-\\nvention, and three points for consideration. Wisconsin Law Review , 3:477–491,\\n01 2016.\\n[36] Chandra L. Ford and Collins Airhihenbuwa. Critical race theory, race equity,\\nand public health: Toward antiracism praxis. American Journal ofPublic Health ,\\n100 Suppl 1:S30–5, 02 2010.\\n[37] Morris W. Foster and Richard R Sharp. Race, ethnicity, and genomics: social\\nclassifications as proxies of biological heterogeneity. Genome research , 12\\n6:844–50, 2002.\\n[38] Michel Foucault. Theorder ofthings. Routledge, 2005.\\n[39] Marion Fourcade and Kieran Healy. Seeing like a market. Socio-Economic\\nReview, 15(1):9–29, 2016.\\n[40] Joan H Fujimura, Deborah A Bolnick, Ramya Rajagopalan, Jay S Kaufman,\\nRichard C Lewontin, Troy Duster, Pilar Ossorio, and Jonathan Marks. Clines\\nwithout classes: How to make sense of human variation. Sociological Theory ,\\n32(3):208–227, 2014.\\n[41] Mindy Fullilove. Comment: Abandoning \"race\" as a variable in public health\\nresearch - an idea whose time has come. American Journal ofPublic Health ,\\n88:1297–8, 10 1998.\\n[42] Duana Fullwiley. The molecularization of race: Institutionalizing human differ-\\nence in pharmacogenetics practice. Science asCulture, 16(1):1–30, 2007.\\n[43] Duana Fullwiley. The biologistical construction of race. Social studies ofscience ,\\n38:695–735, 11 2008.\\n[44] Jennifer Garcìa and Mienah Sharif. Black lives matter: A commentary on racism\\nand public health. American Journal ofPublic Health, 105:e1–e4, 06 2015.\\n[45] Gilbert Gee and Devon Payne-Sturges. Environmental health dispari-\\nties: A framework integrating psychosocial and environmental concepts.Environmental health perspectives, 112:1645–53, 01 2005.\\n[46] Laura E. Gomez and Nancy Lopez, editors. Mapping Race: Critical Approaches\\ntoHealth Disparities Research . New Brunswick, NJ, Rutgers University Press,\\n2013.\\n[47] Ben Green and Lily Hu. The myth in the methodology: Towards a recontextual-\\nization of fairness in machine learning. In ICML 2018, 2018.\\n[48] Ian Hacking. Thesocial construction ofwhat? Harvard university press, 1999.\\n[49] Nina T Harawa and C. Lawrence Ford. The foundation of modern racial\\ncategories and implications for research on black/white disparities in health.\\nEthnicity &Disease, 19 2:209–17, 2009.\\n[50] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in super-\\nvised learning. In Proceedings ofthe30th International Conference onNeural\\nInformation Processing Systems , NIPS’16, pages 3323–3331, USA, 2016. Curran\\nAssociates Inc.\\n[51] Alana Helberg-Proctor, Anja Krumeich, Agnes Meershoek, and Klasien\\nHorstman. The multiplicity and situationality of enacting ’ethnicity’ in dutch\\nhealth research articles. BioSocieties, 12 2017.\\n[52] Michael Herzfeld. Thesocial production ofindifference . University of Chicago\\nPress, 1993.\\n[53] Anna Lauren Hoffmann. Where fairness fails: data, algorithms, and the lim-\\nits of antidiscrimination discourse. Information, Communication &Society ,\\n22(7):900–915, 2019.\\n[54] Paul W. Holland. Statistics and causal inference. Journal oftheAmerican\\nStatistical Association, 81(396):945–960, 1986.\\n[55] Paul W. Holland. Causation and race. In Tukufu Zuberi and Eduardo Bonilla-\\nSilva, editors, White logic, white methods :racism andmethodology , chapter 5,\\npages 93–109. Rowman & Littlefield Publishers, 2008.\\n[56] Junia Howell and Michael Emerson. So what \"should\" we use? evaluating the\\nimpact of five racial measures on markers of social inequality. Sociology of\\nRace andEthnicity, 3:14–30, 5 2017.\\n[57] Abigail Z. Jacobs and Hanna Wallach. Measurement and fairness. 2019.\\n[58] Angela James. Making sense of race and racial classification. In Tukufu Zuberi\\nand Eduardo Bonilla-Silva, editors, White logic, white methods :racism and\\nmethodology, chapter 2, pages 31–45. Rowman & Littlefield Publishers, 2008.\\n[59] Jonathan Kahn. Misreading race and genomics after bidil. Nature genetics ,\\n37:655–6, 08 2005.\\n[60] Judith B. Kaplan and Trude Bennett. Use of Race and Ethnicity in Biomedical\\nPublication. JAMA, 289(20):2709–2716, 2003.\\n[61] Jay Kaufman and Richard Cooper. Commentary: Considerations for use\\nof racial/ethnic classification in etiologic research. American journal of\\nepidemiology, 154:291–8, 09 2001.\\n[62] Jay S Kaufman. How inconsistencies in racial classification demystify the race\\nconstruct in public health statistics. Epidemiology, 10 2:101–3, 1999.\\n[63] Shannon Kelly and Yashwant Pathak. Race and Ethnicity: Understanding\\nDifference intheGenome Era, pages 71–87. 07 2018.\\n[64] Ibram X Kendi. Stamped from thebeginning: Thedefinitive history ofracist\\nideas inAmerica. Random House, 2017.\\n[65] Akil Kokayi Khalfani, Tukufu Zuberi, Sulaiman Bah, and Pali J Lehohla. Race\\nand population statistics in south africa. In Tukufu Zuberi and Eduardo Bonilla-\\nSilva, editors, White logic, white methods: racism andmethodology , chapter 4,\\npages 63–92. Rowman & Littlefield Publishers, 2008.\\n[66] Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-\\noffs in the fair determination of risk scores. In ITCS, 2017.\\n[67] Issa Kohler-Hausmann. Eddie Murphy and the Dangers of Counterfactual Causal\\nThinking About Detecting Racial Discrimination. Northwestern University Law\\nReview, 113(5), 2019.\\n[68] Gloria Ladson-Billings and William F Tate IV. Toward a critical race theory of\\neducation. Teachers College Record, 97:47–68, 1995.\\n[69] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyzed\\nthe compas recidivism algorithm. ProPublica, 9, 2016.\\n[70] Catherine Lee. \"race\" and \"ethnicity\" in biomedical research: How do scien-\\ntists construct and explain differences in health? Social Science &Medicine ,\\n68(6):1183 – 1190, 2009.\\n[71] S Lin and J Kelsey. Use of race and ethnicity in epidemiologic research: Concepts,\\nmethodological issues, and suggestions for research. Epidemiologic reviews ,\\n22:187–202, 02 2000.\\n[72] Adam Liptak. Supreme court leaves census question on citizenship in doubt.\\nTheNew York Times, 2019.\\n[73] Ian Haney López. White bylaw: The legal construction ofrace. NYU Press,\\n2006.\\n[74] Mara Loveman, Jeronimo Muniz, and Stanley R. Bailey. Brazil in black and\\nwhite? race categories, the census, and the study of inequality. Ethnic andRacial\\nStudies, 35, 08 2012.\\n[75] Neda Maghbouleh. The Limits ofWhiteness: Iranian Americans and the\\nEveryday Politics ofRace. Stanford University Press, 2017.\\n[76] Douglas S Massey and Nancy A Denton. American apartheid: Segregation and\\nthemaking oftheunderclass. Harvard University Press, 1993.\\n511FAT* ’20, January 27–30, 2020, Barcelona, Spain Hanna et al.\\n[77] Vickie Mays, Ninez Ponce, Donna Washington, and Susan Cochran. Classifi-\\ncation of race and ethnicity: Implications for public health. Annual Review of\\nPublic Health, 24:83–110, 02 2003.\\n[78] Amade M’charek. Beyond fact or fiction: On the materiality of race in practice.\\nCultural Anthropology, 28:420–442, 07 2013.\\n[79] Katherine McKittrick. Demonic Grounds: Black Women andtheCartographies\\nofStruggle. University of Minnesota Press, ned - new edition edition, 2006.\\n[80] Charles W Mills. Theracial contract. Cornell University Press, 1997.\\n[81] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasser-\\nman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\\nModel cards for model reporting. In FAT, 2019.\\n[82] Ellis P Monk Jr. The cost of color: Skin color, discrimination, and health among\\nafrican-americans. American Journal ofSociology, 121(2):396–444, 2015.\\n[83] Edward Morris. Researching race: Identifying a social construction through\\nqualitative methods and an interactionist perspective. Symbolic Interaction ,\\n30:409–425, 08 2007.\\n[84] Karama C. Neal. Use and Misuse of ’Race’ in Biomedical Research. Online\\nJournal ofHealth Ethics, 5(1), 2008.\\n[85] Safiya Umoja Noble. Algorithms ofoppression: How search engines reinforce\\nracism. nyu Press, 2018.\\n[86] Michael Omi and Howard Winant. Racial formation intheUnited States . Rout-\\nledge, 2014.\\n[87] Michael A Omi. The changing meaning of race. America becoming: Racial\\ntrends andtheir consequences, 1:243–263, 2001.\\n[88] Peggy Pascoe. Miscegenation law, court cases, and ideologies of \"race\" in\\ntwentieth-century america. TheJournal ofAmerican History , 83(1):44–69, 1996.\\n[89] Andrew M Penner and Aliya Saperstein. Disentangling the effects of racial\\nself-identification and classification by others: the case of arrest. Demography ,\\n52(3):1017–1024, 2015.\\n[90] Elizabeth Phillips, Adebola Odunlami, and Vence Bonham. Mixed race: Un-\\nderstanding difference in the genome era. Social forces; ascientific medium of\\nsocial study andinterpretation, 86:795–820, 01 2008.\\n[91] Ramya Rajagopalan and Joan H Fujimura. Making history via dna, making dna\\nfrom history. Genetics andtheunsettled past: Thecollision ofDNA, race, and\\nhistory, page 143, 2012.\\n[92] Inioluwa Deborah Raji and Joy Buolamwini. Actionable auditing: Investigating\\nthe impact of publicly naming biased performance results of commercial ai\\nproducts. In AIES, 2019.\\n[93] Victor Ray. A theory of racialized organizations. American Sociological Review ,\\n84(1):26–53, 2019.\\n[94] Rashida Richardson, Jason Schultz, and Kate Crawford. Dirty data, bad predic-\\ntions: How civil rights violations impact police data, predictive policing systems,\\nand justice. New York University Law Review, 2019.\\n[95] Wendy D Roth. The multiple dimensions of race. Ethnic andRacial Studies ,\\n39(8):1310–1338, 2016.\\n[96] Wendy D. Roth. Methodological pitfalls of measuring race: international com-\\nparisons and repurposing of statistical categories. Ethnic andRacial Studies ,\\n40(13):2347–2353, 2017.\\n[97] Wendy D Roth. Unsettled identities amid settled classifications? toward a\\nsociology of racial appraisals. Ethnic andRacial Studies , 41(6):1093–1112, 2018.\\n[98] Wendy D. Roth and Biorn Ivemark. Genetic options: The impact of genetic\\nancestry testing on consumers’ racial and ethnic identities. American Journal\\nofSociology, 124(1):150–184, 2018.\\n[99] Jacob S Rugh and Douglas S Massey. Racial segregation and the american\\nforeclosure crisis. American sociological review, 75(5):629–651, 2010.\\n[100] Phia Salter and Glenn Adams. Toward a critical race psychology. Social and\\nPersonality Psychology Compass, 7, 11 2013.\\n[101] Gary D. Sandefur, Mary E. Campbell, and Jennifer Eggerling-Boeck. Racial and\\nethnic disparities in health and mortality among the u.s. elderly population. In\\nAnderson NB, Bulatao RA, and Cohen B, editors, Critical Perspectives onRacial\\nandEthnic Differences inHealth inLate Life, pages 53–94. Washington, DC:\\nThe National Academies Press, 2004.\\n[102] Aliya Saperstein. Double-checking the race box: Examining inconsistency\\nbetween survey measures of observed and self-reported race. Social Forces ,\\n85(1):57–74, 2006.\\n[103] Aliya Saperstein. Capturing complexity in the united states: which aspects of\\nrace matter and when? Ethnic andRacial Studies, 35(8):1484–1502, 2012.\\n[104] Aliya Saperstein, Jessica M Kizer, and Andrew M Penner. Making the most of\\nmultiple measures: Disentangling the effects of different dimensions of race in\\nsurvey research. American Behavioral Scientist, 60(4):519–537, 2016.\\n[105] Aliya Saperstein and Andrew M Penner. Racial fluidity and inequality in the\\nunited states. American Journal ofSociology, 118(3):676–727, 2012.\\n[106] James C Scott. Seeing likeastate: How certain schemes toimprove thehuman\\ncondition have failed. Yale University Press, 1998.\\n[107] James C Scott. Against thegrain: adeep history oftheearliest states . Yale\\nUniversity Press, 2017.\\n[108] Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubrama-\\nnian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. InProceedings oftheConference onFairness, Accountability, andTransparency ,\\nFAT* ’19, pages 59–68, New York, NY, USA, 2019. ACM.\\n[109] Maya Sen and Omar Wasow. Race as a bundle of sticks: Designs that estimate\\neffects of seemingly immutable characteristics. Annual Review ofPolitical\\nScience, 19:499–522, 05 2016.\\n[110] Abigail A. Sewell. The racism-race reification process: A mesolevel political\\neconomic framework for understanding racial health disparities. Sociology of\\nRace andEthnicity, 2(4):402–432, 2016.\\n[111] Andrew Smart, Richard Tutton, Paul Martin, George T.H. Ellison, and Richard\\nAshcroft. The standardization of race and ethnicity in biomedical science\\neditorials and uk biobanks. Social Studies ofScience, 38(3):407–423, 2008.\\n[112] C Matthew Snipp. Racial measurement in the american census: Past practices\\nand implications for the future. Annual Review ofSociology , 29(1):563–588,\\n2003.\\n[113] Kim TallBear. Native American DNA: Tribal belonging andthefalse promise\\nofgenetic science. U of Minnesota Press, 2013.\\n[114] Edward Telles, René D. Flores, and Fernando Urrea-Giraldo. Pigmentocracies:\\nEducational inequality, skin color and census ethnoracial identification in eight\\nlatin american countries. Research inSocial Stratification andMobility , 40:39 –\\n58, 2015.\\n[115] Edward E. Telles and Nelson Lim. Does it matter who answers the race question?\\nracial classification and income inequality in brazil. Demography , 35(4):465–474,\\n1998.\\n[116] Charles Tilly. Coercion, Capital, andEuropean States, AD990-1992 . B. Black-\\nwell, 1990.\\n[117] David Williams and Pamela Braboy Jackson. Social sources of racial disparities\\nin health. Health affairs (Project Hope), 24:325–34, 03 2005.\\n[118] David Williams and Chiquita Collins. Racial residential segregation: A funda-\\nmental cause of racial disparities in health. Public Health Reports , 116:404–416,\\n09 2001.\\n[119] David R. Williams. The concept of race in health services research: 1966 to 1990.\\nHealth Services Research, 29(3):261–274, 1994.\\n[120] David R. Williams and Michelle Sternthal. Understanding racial-ethnic dispari-\\nties in health: sociological contributions. Journal ofHealth andSocial Behavior ,\\n51 Suppl(Suppl):S15–S27, 2010.\\n[121] Blaise Agüera y Arcas, Margaret Mitchell, and Alexander Todorov. Physiog-\\nnomy’s new clothes. Medium, 2017.\\n[122] Michael Yudell, Dorothy Roberts, Rob DeSalle, and Sarah Tishkoff. Taking race\\nout of human genetics. Science, 351(6273):564–565, 2016.\\n[123] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P.\\nGummadi. Fairness beyond disparate treatment & disparate impact: Learn-\\ning classification without disparate mistreatment. In Proceedings ofthe26th\\nInternational Conference onWorld Wide Web, WWW ’17, pages 1171–1180,\\nRepublic and Canton of Geneva, Switzerland, 2017. International World Wide\\nWeb Conferences Steering Committee.\\n[124] Tukufu Zuberi. Deracializing social statistics: Problems in the quantification\\nof race. TheAnnals oftheAmerican Academy ofPolitical andSocial Science ,\\n568:172–185, 2000.\\n[125] Tukufu Zuberi. Thicker Than Blood: How Racial Statistics Lie. U of Minnesota\\nPress, 2001.\\n512',\n",
       "  ['sociology', 'race and ethnicity']),\n",
       " ('52    COMMUNICATIONS OF THE ACM   |  MARCH 2022  |  VOL. 65  |  NO. 3contributed articles\\nTECH COMPANIES ARE often criticized for a lack of \\ndiversity in their engineering workforce. In recent \\nyears, such companies have improved engineering \\nworkforce diversity through hiring and retention \\nefforts, according to publicly available diversity \\nreports.a However, we know little about the day-\\nto-day, on-the-job experiences of traditionally \\nunderrepresented engineers once they join an \\norganization.b\\nA core activity of software engineers at many \\ncompanies is code review , where one or more \\nengineers provide feedback on another engineer’s \\na See https://www.aboutamazon.com/working-at-amazon/diversity-and-inclusion/  \\nourworkforce-data, https://www.apple.com/diversity/, https://diversity.fb.com/readreport/,  \\nhttps://diversity.google/annual-report/, and https://www.microsoft.com/enus/diversity/.\\nb In this article, for convenience we refer to a person involved in code review as an “engineer”  \\neven though, as we shall see, non-engineers are also involved in the code review process.code to ensure software quality and \\nspread technical knowledge.1 Beyond \\nsoftware companies, code review has \\nlong been practiced in open source \\nsoftware engineering and is emerging \\nas an important practice for scientists.2 \\nCode review is fundamentally a deci -\\nsion-making process, where reviewers \\nmust decide if and when a code change \\nis acceptable; thus, code review is sus -\\nceptible to human biases. Indeed, \\nprior research on open source projects \\nsuggests that some code reviews au -\\nthored by women are more likely to be \\nrejected than those authored by men.16\\nThis article provides confirmation -\\nal evidence that some demographic \\ngroups face more code review push -\\nback than others. There is no pub -\\nlished research that has studied such \\ndifferences in a corporate setting.\\nMethod\\nThis section describes the setting of \\nthis study, the theory that we ground \\nit in, the dependent and indepen -\\ndent variables we use, our modeling \\napproach, and the dataset. While we \\nbriefly describe the variables we use \\nhere, a full description can be found in \\nthe supplementary material at https://\\ndl.acm.org/doi/10.1145/3474097.\\nSetting. Code review at Google \\nis a process that is used in the com -\\npany’s monolithic codebase.14 When \\na software engineer makes a code \\nchange—to add a new feature or fix a The Pushback \\nEffects of Race, \\nEthnicity, Gender,  \\nand Age in  \\nCode Review DOI:10.1145/3474097\\nResearch shows that White, male, and  \\nyounger engineers receive less pushback  \\nthan those in other demographics.\\nBY EMERSON MURPHY-HILL, CIERA JASPAN,  \\nCAROLYN EGELMAN, AND LAN CHENG \\n key insights\\n ˽Code review, a common practice in \\nsoftware organizations, is susceptible  \\nto human biases, where reviewer \\nfeedback may be influenced by how \\nreviewers perceive the author’s \\ndemographic identity.\\n ˽Through the lens of role congruity theory, \\nwe show the amount of pushback  \\ncode authors receive varies based on \\ntheir gender, race/ethnicity, and age.\\n ˽We estimate such pushback costs  \\nGoogle more than 1,000 extra engineer \\nhours every day, or approximately  \\n4% of the estimated time engineers  \\nspend responding to reviewer  \\ncomments, a cost borne by non-White  \\nand non-male engineers.MARCH 2022  |  VOL. 65  |  NO. 3  |  COMMUNICATIONS OF THE ACM     53IMAGE BY MANGOSTARdefect—that code must be reviewed \\nby at least one other engineer. Review -\\ners evaluate the fitness for purpose \\nof the change, as well as its quality. If \\nthey have concerns or questions, they \\nexpress those comments in the code \\nreview tool. Most reviewers are engi -\\nneers who are on the same team as the \\nauthor, but reviews can also be per -\\nformed across different teams, such \\nas when a software engineer fixes a \\nproblem in code that they use but nor -\\nmally do not work on. Authors choose \\ntheir reviewers, but the code review \\nsystem can also suggest appropriate \\nreviewers. The code review tool pro -\\nvides authors and reviewers with op -portunities to learn about each other, \\nincluding their full names and photos \\n(more in the supplementary material).\\nTheory and hypotheses.  Our study \\nis grounded in role congruity theory, \\nwhich states that a member of a group \\nwill receive negative evaluations when \\nstereotypes about the group misalign \\nwith the perceived qualities necessary \\nto succeed in a role.3 Applying this to \\nour context, the theory predicts that \\ncode reviews will be evaluated nega -\\ntively when the author of a code change \\nbelongs to a group whose stereotypes \\ndo not align with the perceived quali -\\nties of a successful programmer or \\nsoftware engineer. We evaluate three different demographic dimensions \\nacross which we predict code review \\nevaluations will vary: gender, race/eth -\\nnicity, and age.\\nWith respect to gender, we hypothe -\\nsize that reviews of women coders will \\nbe more negative than reviews of men. \\nThe rationale is the role mismatch be -\\ntween “the pervasive cultural associa -\\ntions linking men but not women with \\nraw intellectual talent” and that com -\\nputer science is perceived by some to \\nrequire high “innate intellectual tal -\\nent.”7 Likewise, we hypothesize that \\npeople who identify as Black, Hispan -\\nic, or Latinx have greater odds of facing \\nnegative evaluations than those who54    COMMUNICATIONS OF THE ACM   |  MARCH 2022  |  VOL. 65  |  NO. 3contributed articles\\nreviews with high pushback would be \\nin the 90th percentile of each metric: \\nmore than nine rounds of review, 48 \\nminutes reviewing, and 112 minutes \\nspent by the author. In this study, we \\nadopt that composite measure as our \\nindependent variable by modeling \\nwhether a review is likely to be identi -\\nfied as high pushback, or just “push -\\nback” for short.\\nIndependent variables. The indepen -\\ndent variables of primary interest are \\ngender, race/ethnicity, and age. Here, \\nwe largely use pre-existing demographic \\ncategories that Google maintains as part \\nof reporting requirements under U.S. \\nlaw. For gender, the reported categories \\nare female or male. Race/ethnicity in -\\ncludes Asian+, Black+, Latinx+, Native \\nAmerican+, and White+, where the “+” \\ndenotes the fact that engineers can \\nchoose multiple race/ethnic identities. \\nFor age, we discretize ages into ranges.\\nIndependent control variables. \\nDrawing on prior research about code \\nreview,5,13,14,17 the independent variables \\nused as controls are based on properties \\nof the change, properties of the author, \\nand other variables:\\n •For properties of the change, we \\nmodel the log value of the number of \\nlines changed, the number of review -\\ners, whether the change contains at \\nleast one modification to a file written \\nin a coding language, and several spe -\\ncial properties of a review:\\n ˴Did the review require a “read -\\nability” reviewer4—that is, a reviewer \\ncertified as an expert in programming \\nlanguage coding standards?\\n ˴Was the review part of the read -\\nability certification process, in which \\nthe author’s expertise in program -\\nming language coding standards was \\nbeing evaluated?\\n ˴Was the review a large-scale \\nchange (or LSC), approved either by a \\nlocal code owner or a globally empow -\\nered one?\\n •For properties of the author of the \\nchange, we included the level (senior -\\nity) of the reviewer, how long they have \\nbeen at Google, and their job family—\\nfor instance, software engineer, site re -\\nliability engineer, etc).\\n •Other variables we captured were \\nthe job family of the main reviewer \\nand the relationship between main re -\\nviewer and author. By “main reviewer,” \\nwe mean the reviewer who has made the most comments, or in the case of a \\ntie, the first reviewer to comment. We \\nmodel relationships as “insider” when \\nthe author and main reviewer work on \\nthe same team; otherwise, we define \\nthem as “outsider” reviews. While in -\\nsider reviews are more common, out -\\nsider reviews are necessary when, for \\nexample, an author needs to change \\nanother team’s code, such as fixing \\ndownstream dependencies on an API. \\nDescriptive statistics for all variables \\nare available in the online supplemen -\\ntary material.\\nIndependent interactions variables. \\nPrior work16 suggests that the rela -\\ntionship between author and reviewer \\nmoderates gender bias effects. To ac -\\ncount for such a moderating effect, we \\nmodel the interaction between rela -\\ntionship (insider or outsider) and each \\nindependent variable (gender, race/\\nethnicity, and age).\\nModeling approach.  Since the de -\\npendent variable was binary—either \\nthe change was flagged as receiving \\npushback or not—we used a mixed-ef -\\nfect binomial logistic regression mod -\\nel. In this model, to attempt to control \\nfor the same engineer appearing re -\\npeatedly as an author or reviewer across \\ncode reviews, we use author and main \\nreviewer identities as random effects. \\nAs in our prior work on pushback,5 we \\ndescribe the effect size in terms of odds \\nratios of the primary independent vari -\\nables, as well as their statistical signifi -\\ncance. We address potential multicol -\\nlinearity issues, performing variance \\ninflation factor (VIF) and generalized \\nvariance inflation factor (GVIF)6 checks \\nfor independent variables; since all \\ncontinuous variables’ VIF scores were \\nbelow 1.3 and GVIF scores for categori -\\ncal variables were below 1.5, we assume \\nthat multicollinearity was not a sub -\\nstantial threat to the interpretation of \\nour model. We also ensured the robust -\\nness of our analysis by replicating the \\nstudy on a different dataset; we found \\nthat gender and race/ethnicity effects \\nwere very consistent, and age effects \\nwere largely consistent (see supple -\\nmentary material).\\nDataset.  We analyzed code reviews \\nperformed in one of the main code re -\\nview tools at Google over a six-month \\nperiod from the beginning of January \\n2019 through the end of June 2019, \\nsubject to the following constraints. identify as White, because, as the Gen -\\neral Social Survey suggests, Americans \\nare less likely to view those groups as \\npossessing innate intelligence.15 On \\nthe other hand, we hypothesize that \\nthose who identify as Asian will face \\nmore positive evaluations than those \\nwho identify as White, because Asians \\nare stereotypically viewed as having \\nhigher role congruity in engineering \\nfields.8 We make no hypothesis about \\nrole congruity for Native Americans, \\ndue to a lack of prior research litera -\\nture. Recent research shows that con -\\ntributions from White developers in \\nopen source are more likely to be ac -\\ncepted than those from non-White \\ndevelopers.9 With respect to age, we \\nhypothesize that older engineers are \\nmore likely to experience negative re -\\nviews than younger engineers, because \\nof two major role mismatches:\\n •While there “is a stereotype that \\nolder workers have lower ability… \\nand are less productive than younger \\nworkers,”11 a great software engineer \\nis expected to be mentally capable of \\nhandling complexity and be highly \\nproductive.9\\n •While there is a stereotype that \\nolder workers “are harder to train, \\nless adaptable, less flexible, and more \\nresistant to change” and “have a lower \\nability to learn,”11 great software engi -\\nneers are expected to be open-mind -\\ned, continuously self-improving, and \\nto not let their understanding stag -\\nnate.9\\nDependent variable. The depen -\\ndent variable in our predictive model is \\npushback , defined as “the perception of \\nunnecessary interpersonal conflict in \\ncode review while a reviewer is block -\\ning a change request.”5 In prior work, \\nwhere we did not provide demograph -\\nic breakdowns, we compared several \\nquantitative signals that predicted \\nnegative evaluations in two ways rel -\\nevant to role congruity theory: 1) a re -\\nviewer requesting excessive changes, \\nand 2) a reviewer withholding approv -\\nal. The strongest predictors of individ -\\nual engineers’ self-reported pushback \\nwere when a code review had a high \\nnumber of rounds (that is, back and \\nforth between the author and review -\\ners), a high amount of time spent by \\nreviewers, and a high amount of time \\nspent by the author addressing the re -\\nviewers’ concerns. In that work, suchMARCH 2022  |  VOL. 65  |  NO. 3  |  COMMUNICATIONS OF THE ACM     55contributed articles\\nof the chart shows the model’s inde -\\npendent variables, along with their p \\nvalues in parentheses. The right half \\nshows the odds ratio for each indepen -\\ndent variable. Odds ratios of less than \\n1.0 mean lower odds; odds ratios larger \\nthan 1.0 mean higher odds.\\nFigure 1(a) displays our control Reviews must have had at least one \\nreviewer (which excludes some experi -\\nmental, emergency, and documenta -\\ntion changes), and both the author and \\nall reviewers must be full-time-equiva -\\nlent Google employees working in the \\nU.S. Changes from authors who had \\nincomplete demographic data were ex -cluded. In sum, this analysis includes \\nmore than two million code reviews \\nfrom over 30,000 authors.\\nResults\\nFigure 1 displays the results of our \\nmixed-effect regression predicting \\ncode-review pushback. The left half Figure 1. Odds ratios from regression analysis predicting pushback in code review for controls (a), the main demographic predictors of \\ninterest (b), and the outsider interaction (c). Odds ratios are omitted for non-significant results.\\n1.98\\n0.03\\n2.73\\n1.58\\n1.58\\n0.40\\n0.73\\n0.51\\n0.38\\n0.29\\n0.61\\n0.55\\n0.48\\n1.54\\n1.67\\n1.21\\n1.54\\n1.15\\n1.42\\n1.10\\n1.18\\n1.34\\n1.48\\n1.60\\n2.05\\n3.21\\n1.15\\n1.32\\n1.76\\n2.02\\n2.19\\n2.79\\n3.680 1 2 3 4\\nlog(linesChanged) (p<.001)\\nlscGlobal (p=.794)\\nlscLocal (p<.001)\\nreviewerCount (p<.001)\\nreadabilityReviewTRUE  (p<.001)\\nneedsReadabilityTRUE (p<.001)\\nhasCodeFALSE (p<.001)\\n4 (p<.001)\\n5 (p<.001)\\n6 (p<.001)\\n7 (p<.001)\\n1-2 years (p<.0 01)\\n3-5 years (p<.0 01)\\n6+ years (p<.001)\\nENG_SRE (p=.2 31)\\nENG_OTHER (p<.001)\\nOTHER (p<.001)\\nFemale (p<.001)\\nBlack+ (p<.001)\\nHispanicOrLatinX+ (p=.013)\\nNativeAm erican+ (p= .735)\\nAsian+ (p<.001)\\n25-29 years (p=.002)\\n30-34 years (p<.001)\\n35-39 years (p <.001)\\n40-44 years (p<.001)\\n45-49 years (p<.001)\\n50-59 years (p<.001)\\n60+ years (p<.001)\\nOutsider (p<.0 01)\\nOutsider:Fem ale (p=.039)\\nOutsider:Black+ (p=.431)\\nOutsider:HispanicOrLatinX+ (p=.2 64)\\nOutsider:NativeAmerican+ (p=.456)\\nOutsider:Asian+ (p=.149)\\nOutsider:25-29 years (p=.254)\\nOutsider:30-34 years (p=.002)\\nOutsider:35-39 years (p<.001)\\nOutsider:40-44 years (p<.001)\\nOutsider:45-4 9 years (p =.006)\\nOutsider:50-59 years (p=.025)\\nOutsider:60+ years (p=.220)Changelist Properties\\nAuthor Age \\n(baseline = 18–24 years)\\nRelationship:Race/Ethnicity \\n(baseline = Insider:White+)\\nRelationship:Age \\n(baseline = Insider:18–24 years)Author Job Level \\n(baseline = 3)\\nAuthor Tenure \\n(baseline = Less th an 1 year)\\nAuthor Job Family \\n(baseline = ENG_SOFT)\\nAuthor Race/Ethnicity \\n(baseline = White+)Odds Ratio\\n(a)\\n(b)\\n(c)56    COMMUNICATIONS OF THE ACM   |  MARCH 2022  |  VOL. 65  |  NO. 3contributed articles\\nother places in Figure 1 for non-signif -\\nicant factors.\\nFigure 1(b) displays results that \\nevaluate our hypotheses—demo -\\ngraphic predictors of pushback. Since \\nour model uses an interaction effect \\nbetween demographics and the rela -\\ntionship, the first set of demographics \\nshould be interpreted as applying to \\ninsiders—that is, when the author and \\nmain reviewer are on the same team.\\nWith respect to gender, consistent \\nwith the gender correlations observed \\non GitHub,16 women’s changes have \\na 1.21 higher likelihood of receiving \\npushback than changes by men. Like -\\nwise, compared to White+ engineers, \\nthe odds of pushback are higher on \\nauthors who identify as Black+ (1.54), \\nHispanic or Latinx+ (1.15), and Asian+ \\n(1.42). With respect to age, the results \\nshow changes from older engineers \\nhave higher odds of pushback com -\\npared to younger engineers, even after \\naccounting for seniority and tenure. \\nFor instance, a change authored by an \\nengineer who is 60 years old or older is \\nmore than three times likely to receive \\npushback than that of an author at the \\nsame level and tenure who is between \\n18 and 24 years old.\\nFigure 1(c) shows the results for \\noutsider code reviews. Overall, the re -\\nsults indicate that code reviews from \\nengineers on a different team than the \\nauthor have higher odds (1.15) of push -\\nback. For race/ethnicity and gender, \\nthere are few statistically significant \\ndifferences for insider and outsider \\ncode reviews—that is, unlike in prior \\nwork,16 relationships are not a sub -\\nstantial mediating factor. In the cases \\nwhere there is a statistically significant \\ninteraction, the effect is compounding. \\nFor instance, compared to an 18-to-24-\\nyear-old insider, the model would na -\\nively predict that reviews by outsider \\nauthors who are between 30 and 34 \\nyears old would have 1.36 (1.18 odds \\nfor 30–34 years old × 1.15 odds for out -\\nsiders) greater odds of pushback, but \\nthe interaction coefficient indicates \\nthat the actual odds of pushback for \\nthis group is even higher, at 1.77.\\nIn summary, these results indicate \\nthat regardless of team relationship \\nbetween author and main reviewer, au -\\nthors from some demographic groups \\nface higher odds of pushback during \\ncode review than others. Women au -variables. For instance, the first row \\nindicates that the log of the number \\nof lines changed in the code review is \\nsignificantly (p<.001) correlated with \\npushback. Changing more lines of code \\nincreases the odds of the review being \\nflagged with pushback. On the other \\nhand, the odds of a locally approved, \\nlarge-scale change (LSC) review—gen -\\nerally a low-risk change—being identi -\\nfied with pushback is substantially low -\\ner (0.02) compared to non-LSC reviews. \\nAs the figure indicates, each new re -\\nviewer increases the odds of pushback \\n(2.73), as does whether the review is \\npart of the readability certification \\nprocess (1.58) and whether a certified \\nreadability reviewer is required (1.58). \\nA review without code—for instance, \\ndocumentation only—is less likely to \\nbe flagged with pushback (0.4) than a \\nchange being reviewed with code.\\nAs Figure 1(a) indicates, job-rel -\\nevant author characteristics also \\nchange the odds of pushback. Reviews \\nby more senior-level authors are less \\nlikely to receive pushback than those \\nof, for instance, an entry-level engineer \\n(level 3). This confirms findings from \\nprior work5 that more senior engineers \\nare less likely to face pushback. Like -\\nwise, a review author who has been at \\nGoogle for less than a year is more like -\\nly to face pushback than one who has \\nbeen with the organization longer. In -\\ncluding such experience covariates in \\nour model helps isolate demographic \\nfactors—covariates which might oth -\\nerwise confound results. For instance, \\nGoogle’s 2020 diversity report states \\nthat women tend to have lower attri -\\ntion than men, and Native American+ \\nemployees have higher attrition than \\nWhite+ employees.\\nCompared to the most common \\nsoftware engineering role—software \\nengineer, or ENG_SOFT—changes \\nauthored by other types of engineers \\n(ENG_OTHER, such as research sci -\\nentist engineers) and non-engineers \\n(OTHER, such as technical operations \\nemployees) are more likely to receive \\npushback. We did not detect a statisti -\\ncally significant difference in the odds \\nof pushback for changes from site \\nreliability engineers (ENG_SRE) com -\\npared to the baseline, regular software \\nengineers. Since pushback for SRE \\nauthors was not significant, we omit \\nodds ratios for SREs and in several Some demographic \\ngroups face  \\nmore code review \\npushback  \\nthan others.MARCH 2022  |  VOL. 65  |  NO. 3  |  COMMUNICATIONS OF THE ACM     57contributed articles\\nReferences\\n1. Bacchelli, A. and Bird, C. Expectations, outcomes, and \\nchallenges of modern code review. International \\nConf. on Software Engineering  (2013), 712-721.\\n2. Check Hayden, E. Mozilla plan seeks to debug \\nscientific code. Nature News 501 , 7468 (2013), 472.\\n3. Eagly, A.H. and Karau, S.J. Role congruity theory \\nof prejudice toward female leaders. Psychological \\nReview 109 , 3 (2002), 573.\\n4. Eby, L.T., McManus, S.E., Simon, S.A., and Russell, \\nJ.E. The protege’s perspective regarding negative \\nmentoring experiences: The development of a \\ntaxonomy. J. of Vocational Behavior 57 , 1 (2000), \\n1-21.\\n5. Egelman, C.D., Murphy-Hill, E., Kammer, E., Hodges, \\nM.M., Green, C., Jaspan, C., and Lin, J. Pushback: \\nCharacterizing and detecting negative interpersonal \\ninteractions in code review. Intern. Conf. on Software \\nEngineering (2020), 174-185.\\n6. Fox, J. and Monette, G. Generalized collinearity \\ndiagnostics. J. of the American Statistical Association \\n87, 417 (1992), 178-183.\\n7. Leslie, S.J., Cimpian, A., Meyer, M., and Freeland, \\nE. Expectations of brilliance underlie gender \\ndistributions across academic disciplines. Science \\n347, 6219 (2015), 262-265.\\n8. Leong, F.T. and Hayes, T.J. Occupational stereotyping \\nof Asian Americans. The Career Development \\nQuarterly 39 , 2 (1990), 143-154.\\n9. Li, P.L., Ko, A.J., and Begel, A. What distinguishes \\ngreat software engineers? Empirical Software \\nEngineering 25 , 1 (2020), 322-352.\\n10. Murphy-Hill, E., Dicker, J., Hodges, M., Egelman, \\nC.D., Jaspan, C.N.C., Cheng, L., Kammer, L., Holtz, B., \\nJorde, M.A., Dolan, A.M.K., and Green, C. Engineering \\nimpacts of anonymous author code review: A field \\nexperiment. Trans. on Software Engineering.  (To \\nappear).\\n11. Nadri, R., Rodriguez-Perez, G., and Nagappan, M. On \\nthe relationship between the developer’s perceptible \\nrace and ethnicity and the evaluation of contributions \\nin OSS. Trans. on Software Engineering. ( To appear).\\n12. Posthuma, R.A. and Campion, M.A. Age stereotypes \\nin the workplace: Common stereotypes, moderators, \\nand future research directions. J. of Management 35 , \\n1 (2009), 158-188.\\n13. Potvin, R. and Levenberg, J. Why Google stores \\nbillions of lines of code in a single repository. \\nCommunications of the ACM 59 , 7 (2016), 78-87.\\n14. Sadowski, C., Söderberg, E., Church, L., Sipko, M., \\nand Bacchelli, A. Modern code review: A case study \\nat Google. Intern. Conf. on Software Engineering: \\nSoftware Engineering in Practice (2018), 181-190.\\n15. Smith, T.W., Davern, M., Freese, J., and Morgan, S.L. \\nGeneral Social Surveys (2019).\\n16. Terrell, J., Kofink, A., Middleton, J., Rainear, C., \\nMurphy-Hill, E., Parnin, C., and Stallings, J. Gender \\ndifferences and bias in open source: Pull request \\nacceptance of women versus men. PeerJ Computer \\nScience 3 , e111 (2017).\\n17. Yu, Y., Wang, H., Filkov, V., Devanbu, P., and Vasilescu, \\nB. Wait for it: Determinants of pull request evaluation \\nlatency on Github. Working Conf. on Mining Software \\nRepositories  (2015), 367-371.\\nEmerson Murphy-Hill (emersonm@google.com) is a \\nresearch scientist at Google.\\nCiera Jaspan is a software engineer at Google.\\nCarolyn Egelman is a quantitative user experience \\nresearcher at Google.\\nLan Cheng is a quantitative user experience researcher \\nat Google.thors face higher odds \\nof pushback than men; \\nAsian, Black, and His -\\npanic/Latinx authors \\nface higher odds than \\nWhite authors; and \\nolder authors face \\nhigher odds than younger authors.\\nFinally, we have presented effect \\nsizes in terms of odds ratios, but what \\ndo these differences mean in practi -\\ncal terms? We answer this question \\nby approximating the excess cost of \\npushback during the code review pro -\\ncess, particularly in terms of additional \\nrounds of review, one component of \\npushback.5 We do this by modeling \\nthe number of review rounds a change \\nundergoes, subtracting that from a \\nprediction of the number of rounds it \\nwould have taken had the author been \\na White male, and then estimating the \\ntime spent by authors addressing com -\\nments in a round of review (details are \\nin the supplementary material, includ -\\ning caveats). We estimate that the total \\namount of excess time spent during \\nthe study period was 1,050 engineer \\nhours per day, or about 4% of the esti -\\nmated time engineers spend respond -\\ning to reviewer comments, a cost borne \\nby non-White and non-male engineers. \\nWhile this number provides one view \\nof the impact of pushback, we would \\nadvise readers to interpret this esti -\\nmate with caution.\\nDiscussion and Conclusion\\nCompared with prior work, which \\nfound that some women faced less-\\nsuccessful code reviews when their \\ngender was apparent,16 the results in \\nthis paper suggest not only that wom -\\nen authors have greater odds of push -\\nback as both outsiders and insiders, \\nbut that this effect extends to other de -\\nmographic groups.\\nUnlike in an experimental setting, \\ncross-sectional retroactive studies such \\nas ours cannot conclude with certainty \\nthat there’s a causal relationship be -\\ntween demographic factors and push -\\nback. Potential third variables that we \\ncould not control for may exist. For \\ninstance, contrary to what we hypoth -\\nesized from role congruity theory, we \\nfound that Asian engineers faced greater \\nodds of pushback than White engineers. \\nThe hidden third variable here may be \\nwhether the engineer speaks English as a first language. Those who speak Eng -\\nlish as a second language may face more \\ndifficulty communicating their intent \\nand rationale during a code review dis -\\ncussion, lengthening the time it takes \\nto successfully defend a code review and \\nmanifesting as pushback. More broadly, \\nother hidden variables may exist, such \\nas code quality in the change under re -\\nview. Our analysis is limited in other \\nways as well, which we enumerate in the \\nsupplementary material.\\nWe estimated that more than 1,000 \\nhours per day is spent at Google re -\\nsponding to “excessive” pushback, a \\ncost borne by non-White, non-male, or \\nolder engineers. One way to conceptu -\\nalize this estimate is as an opportunity; \\nif we can reduce pushback for these \\ngroups of engineers, they can spend \\ntheir time being productive elsewhere. \\nBut there’s also an inverse way to con -\\nceptualize this research: White, male, \\nand younger engineers are privileged \\nto receive less pushback than those in \\nother demographics. In either case, we \\nview reducing the gaps between demo -\\ngraphic groups as a worthwhile goal, \\nand we expect our software to improve \\nas we attempt to do so.\\nAt Google, a company-wide objec -\\ntive is to make our workplace equita -\\nble, and this paper provides one way to \\nmeasure progress towards this objec -\\ntive. Our initiatives to this end are wide-\\nranging, from bias-busting trainingc to \\nanonymous author code review.10 We \\nlook forward to seeing whether such \\ninitiatives will foster more equitable \\ntreatment of different groups of engi -\\nneers in the workplace.\\nAcknowledgments\\nWe thank Alison Song, Alyson Palmer, \\nAmir Najmi, Andrea Knight, Annie Jean-\\nBaptiste, Ash Kumar, Asim Husain, Ben \\nHoltz, Caitlin Hogan, Collin Green, \\nDan Friedland, Danny Berlin, David \\nPatterson, David Sinclair, Diane Tang, \\nElvin Lee, Jill Dicker, Liz Kammer, \\nLuiz André Barroso, Maggie Hodges, \\nMark Canning, Matthew Jorde, Melody \\nMeckfessel, Melonie Parker, Nina \\nChen, Rachel Potvin, Ted Smith, and \\nanonymous reviewers for their assis -\\ntance throughout this research. \\nc See https://rework.withgoogle.com/guides/\\nunbiasing-hold-everyone-accountable/steps/\\ngive-your-own-unbiasing-workshop/\\nWatch the authors discuss  \\nthis work in the exclusive \\nCommunications  video.  \\nhttps://cacm.acm.org/videos/the-\\npushback-effectsThis work is licensed under a http://\\ncreativecommons.org/licenses/by/4.0/• more online\\nOnline-only \\nsupplementary \\nmaterial for this \\narticle can be \\nfound at https://\\ndl.acm.org/doi/  \\n10.1145/3474097.',\n",
       "  ['men',\n",
       "   'agile software development',\n",
       "   'race and ethnicity',\n",
       "   'age',\n",
       "   'computing occupations',\n",
       "   'testing, certification and licensing',\n",
       "   'women',\n",
       "   'computing industry',\n",
       "   'collaboration in software development',\n",
       "   'collaborative and social computing design and evaluation methods',\n",
       "   'software verification and validation']),\n",
       " ('Information Ecosystem Threats in Minoritized Communities:\\nChallenges, Open Problems and Research Directions\\nShiri Dori-Hacohen\\nUniversity of Connecticut & AuCoDe\\nStorrs, Connecticut, United States\\nshiridh@uconn .edu ,shiri@aucode .ioScott A. Hale\\nMeedan & University of Oxford\\nSan Francisco, California, United States\\nscott@meedan .com ,scott .hale@oii .ox .ac .uk\\nABSTRACT\\nJournalists, fact-checkers, academics, and community media are\\noverwhelmed in their attempts to support communities suffering\\nfrom gender-, race- and ethnicity-targeted information ecosystem\\nthreats, including but not limited to misinformation, hate speech,\\nweaponized controversy and online-to-offline harassment. Yet, for a\\nplethora of reasons, minoritized groups are underserved by current\\napproaches to combat such threats. In this panel, we will present and\\ndiscuss the challenges and open problems facing such communities\\nand the researchers hoping to serve them. We will also discuss\\nthe current state-of-the-art as well as the most promising future\\ndirections, both within IR specifically, across Computer Science\\nmore broadly, as well as that requiring transdisciplinary and cross-\\nsectoral collaborations. The panel will attract both IR practitioners\\nand researchers and include at least one panelist outside of IR, with\\nunique expertise in this space.\\nCCS CONCEPTS\\n•Security and privacy →Social aspects of security and privacy ;•\\nInformation systems →Collaborative and social computing\\nsystems and tools; Web searching and information discovery.\\nKEYWORDS\\ninformation ecosystem threats; misinformation; hate speech; abu-\\nsive language; community-led; minoritized communities\\nACM Reference Format:\\nShiri Dori-Hacohen and Scott A. Hale. 2022. Information Ecosystem Threats\\nin Minoritized Communities: Challenges, Open Problems and Research\\nDirections. In Proceedings of the 45th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval (SIGIR ’22), July 11–15,\\n2022, Madrid, Spain. ACM, New York, NY, USA, 2 pages. https://doi .org/\\n10 .1145/3477495 .3536327\\n1 PANEL DESCRIPTION\\nJournalists, fact-checkers, academics, and community media are\\noverwhelmed in their attempts to support communities suffering\\nfrom gender-, race- and ethnicity-targeted information ecosystem\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’22, July 11–15, 2022, Madrid, Spain.\\n©2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-8732-3/22/07. . . $15.00\\nhttps://doi .org/10 .1145/3477495 .3536327threats (IETs), such as misinformation, bias, hate speech and online-\\nto-offline harassment [e.g., 1–3]. For example, by selectively quoting\\nfrom media and reporting content out-of-context, media manipula-\\ntors deliberately seek to stoke fear, drive wedges between social and\\nracial groups, and amplify extremist ideologies. Yet, for a plethora of\\nreasons, minoritized groups are underserved by current approaches\\nto combat such threats. For example, in the wake of COVID-19,\\nAsian-American and Pacific Islander (AAPI) communities are being\\ndisproportionately targeted with such hate and misinformation [ 4–\\n6]; yet, such content remains largely off-the-radar of mainstream\\nfact-checkers and academics due to the variety of languages in-\\nvolved, the use of ‘alternative’ platforms, and the need for cultural\\ncontext to fully understand the manipulation.\\nIn this panel, we will present and discuss the challenges and\\nopen problems facing such communities, as well as the researchers\\nand practitioners partnering with communities to build new ap-\\nproaches.\\nTopics discussed will include:\\n•The interlocking nature of different IETs\\n•Unique challenges facing minoritized communities online\\n•Causes of overlooking IETs in minoritized communities\\n•Research challenges and open problems\\n•Current state-of-the-art regarding IET mitigation in general,\\nand in minoritized communities more specifically\\n•Efforts currently underway regarding IETs in minoritized\\ncommunities\\n•Promising future research directions (both within IR specifi-\\ncally, and across Computer Science more broadly)\\n•Major challenges requiring transdisciplinary and cross-sectoral\\ncollaborations\\n2 ORGANIZERS\\nThe panel organizers bring a wealth of industry, non-profit and\\nacademic expertise and connections to bear on the proposed theme\\nas well as the specific topics that will be discussed.\\n2.1 Institutions\\nAuCoDe is an AI-based startup that provides a platform technology\\nthat detects controversies, misinformation, bias and reputation and\\nturns them into actionable intelligence. Meedan is a global technol-\\nogy not-for-profit that builds software and programmatic initiatives\\nto strengthen journalism, digital literacy and accessibility of infor-\\nmation online and off. The Reducing Information Ecosystem\\nThreats (RIET) Lab at the University of Connecticut focuses on\\nthreats to the information ecosystem online and to healthy public\\ndiscourse from an information retrieval lens, informed by insights\\nSIRIP Paper\\n \\nSIGIR ’22, July 11–15, 2022, Madrid, Spain\\n3384from the social sciences. The Oxford Internet Institute is a mul-\\ntidisciplinary research and teaching department of the University\\nof Oxford, dedicated to the social science of the Internet.\\n2.2 Moderator and Panelists\\nPanel co-organizer and Moderator Prof. Shiri Dori-Hacohen is\\nthe Founder & Executive Chairwoman at AuCoDe and an Assistant\\nProfessor at the Department of Computer Science & Engineering at\\nthe University of Connecticut. At the University of Connecticut, she\\nleads the Reducing Information Ecosystem Threats (RIET) Lab and\\nis an inaugural member of a first-of-its-kind entrepreneurial cluster,\\nbridging the startup and academic worlds. She has 18 years of\\nexperience in industry, startups, and academia, including at Google\\nand Facebook, and has served as PI or Co-PI on over $2.7M worth of\\nfederal funds from the US National Science Foundation. Prof. Dori-\\nHacohen has been a quoted expert in media outlets including Forbes\\nand the Boston Globe.\\nPanel co-organizer and Panelist Prof. Scott A. Hale is the Di-\\nrector of Research at Meedan, an Associate Professor at the Oxford\\nInternet Institute of the University of Oxford, and a Fellow at the\\nAlan Turing Institute. In these roles, he works to bridge academia\\nand industry divides and increase equitable access to quality in-\\nformation online with a focus on misinformation and hate speech.\\nProf. Hale is currently the PI on an NSF Convergence Accelera-\\ntor Phase I grant titled “FACT CHAMP - Fact-checker, Academic,\\nand Community Collaboration Tools: Combating Hate, Abuse, and\\nMisinformation with Minority-led Partnerships”.\\nAdditional Panelists. The organizers will leverage their broad\\nand deep networks to attract a diverse panel from industry, non-\\nprofit, government and academia with unique expertise in this\\nspace, both within and outside of IR. Within IR, we will invite ex-\\nperts specializing in IETs, including both an industry practitioner,\\nsuch as Dr. Zeki Yalniz (Research Scientist in the misinformation\\ngroup, Facebook) or Dr. David Corney (Senior NLP Data Scientist at\\nFull Fact), and an academic researcher, such as Dr. Yelena Mejova\\n(Senior Research Scientist, ISI Foundation), Dr. Damiano Spina and\\nProf. Mark Sanderson of RMIT (who collaborate extensively with\\nthe RMIT FactLab), or Prof. Matt Lease of UT Austin. Furthermore,\\nwe will invite 1-2 additional panelists outside of traditional IRboundaries, such as Prof. Kiran Garimella, School of Communica-\\ntion and Information, Rutgers; Dr. Douglas Maughan, Office Head\\nfor the National Science Foundation (NSF) Convergence Accelera-\\ntor; Prof. Kathleen Hall Jamieson, Director, Annenberg Public Policy\\nCenter and Co-founder, FactCheck.org; Gordon Pennycook, Asso-\\nciate Professor, University of Regina; Prof. Michael Lynch, Board\\nof Trustees Distinguished Professor of Philosophy and Director of\\nthe Humanities Institute, University of Connecticut; or Kyla Fullen-\\nwider, Senior Advisor for the Surgeon General of the United States.\\nWe commit to inviting at least one panelist in each category (IR\\npractitioner, IR researcher, and non-IR), but in any case no more\\nthan 5 panelists total (including the co-organizers). We strongly\\nbelieve this structure will greatly enhance the quality of the panel\\nand its value to the SIRIP audience.\\nACKNOWLEDGEMENTS\\nThis material is based upon work supported by the National Science\\nFoundation Convergence Accelerator (contract 49100421C0035).\\nAny opinions, findings and conclusions or recommendations ex-\\npressed in this material are those of the author(s) and do not neces-\\nsarily reflect the views of the National Science Foundation.\\nREFERENCES\\n[1]Alicia L Best, Faith E Fletcher, Mika Kadono, and Rueben C Warren. 2021. Insti-\\ntutional distrust among African Americans and building trustworthiness in the\\nCOVID-19 response: implications for ethical public health practice. Journal of\\nHealth Care for the Poor and Underserved 32, 1 (2021), 90.\\n[2]J Jaiswal, C LoSchiavo, and DC Perlman. 2020. Disinformation, misinformation\\nand inequality-driven mistrust in the time of COVID-19: Lessons unlearned from\\nAIDS denialism. AIDS and Behavior 24, 10 (2020), 2776–2780.\\n[3]Jagdish Khubchandani and Yilda Macias. 2021. COVID-19 vaccination hesitancy\\nin Hispanics and African-Americans: A review and recommendations for practice.\\nBrain, Behavior, & Immunity-health 15 (2021), 100277.\\n[4]Jae Yeon Kim and Aniket Kesari. 2021. Misinformation and Hate Speech: The Case\\nof Anti-Asian Hate Speech During the COVID-19 Pandemic. Journal of Online\\nTrust and Safety 1, 1 (2021).\\n[5]Mingqi Li, Song Liao, Ebuka Okpala, Max Tong, Matthew Costello, Long Cheng,\\nHongxin Hu, and Feng Luo. 2021. COVID-HateBERT: A pre-trained language\\nmodel for COVID-19 related hate speech detection. In 2021 20th IEEE International\\nConference on Machine Learning and Applications (ICMLA). IEEE, 233–238.\\n[6]Bertie Vidgen, Scott Hale, Ella Guest, Helen Margetts, David Broniatowski, Zeerak\\nWaseem, Austin Botelho, Matthew Hall, and Rebekah Tromble. 2020. Detecting\\nEast Asian prejudice on social media. In Proceedings of the Fourth Workshop on\\nOnline Abuse and Harms . Association for Computational Linguistics, Online, 162–\\n172. https://doi .org/10 .18653/v1/2020 .alw-1 .19\\nSIRIP Paper\\n \\nSIGIR ’22, July 11–15, 2022, Madrid, Spain\\n3385',\n",
       "  ['social aspects of security and privacy',\n",
       "   'web searching and information discovery',\n",
       "   'collaborative and social computing systems and tools']),\n",
       " (\"Evaluation of the Use of Growth Mindset in the CS Classroom \\nDaehan Kwak, Patricia Morreale, Sarah T. Hug†, Yulia Kumar, Jean Chu, Ching-Yu Huang,  \\nJ. Jenny Li and Paoline Wang \\n School of Computer Science and Technology, Kean University, Union, NJ, USA \\n†Colorado Evaluation & Research Consulting, Westminster, CO, USA \\n{dkwak, pmorreal, ykumar, jchu, chuang, juli, pawang}@kean.edu, hug@colorado.edu†\\nABSTRACT \\nWithin computer science education, a growth mindset is \\nencouraged. However, faculty development on the use of growth \\nmindset in the classroom is rare and resources to support the use \\nof a growth mindset are limited. A framework for a computer \\nscience growth mindset classroom, which includes faculty \\ndevelopment, lesson plans, and vocabulary for use with students, \\nhas been developed. The objective is to determine if faculty \\ndevelopment in growth mindset and active use of the growth \\nmindset cues in the CS0 and CS1 classroom result in superior \\nacademic outcomes. Comparative study results are presented for \\ntwo semesters of virtual classroom environments: one semester \\nwithout Growth Mindset, and one semester with Growth Mindset. \\nFemale students demonstrated the most growth, as measured by \\nacademic grades, in CS0, and maintained that growth in CS1. \\nMales demonstrated growth as well, with both males and females \\nconverging at the same high point of accomplishment at the end of \\nCS1. Race and ethnicity gaps between students were reduced, \\nimproving academic equity.  \\nCCS CONCEPTS \\n• Social and professional topics → Professional topics → \\nComputing education → Computing education programs \\n→ Computer science education . \\nKEYWORDS \\nGrowth mindset; Computer Science Education; Broadening \\nParticipation; Faculty Development. \\nACM Reference format: \\nDaehan Kwak, Patricia Morreale, Sarah T. Hug, Yulia Kumar, Jean Chu, \\nChing-Yu Huang, Juan Li and Paoline Wang. 2022. Evaluation of the Use of \\nGrowth Mindset in the CS Classroom. In Proceedings of the 53rd ACM \\nTechnical Symposium on Computer Science Education (SIGCSE’22), March 2-\\n5, 2022, Providence, RI, USA . ACM, New York, NY, USA, 7 pages. \\nhttps://doi.org/10.1145/3478431.3499365    \\nPermission to make digital or hard copies of all or part of this work for personal or \\nclassroom use is granted without fee provided that copies are not made or distributed \\nfor profit or commercial advantage and that copies bear this notice and the full citation \\non the first page. Copyrights for components of this work owned by others than the \\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \\nrepublish, to post on servers or to redistribute to lists, requires prior specific \\npermission and/or a fee. Request permissions from Permissions@acm.org. \\nSIGCSE’22, March 2–5, 2022, Providence, RI, USA \\n© 2022 Association for Computing Machinery.  \\nACM ISBN 978-1-4503-9070-5/22/03...$15.00 \\nhttps://doi.org/10.1145/3478431.3499365 1  Introduction \\nA growth mindset encourages the development of intelligence, in \\ncontrast to a fixed mindset, which considers intelligence to be \\nfixed and unable to be changed [1, 2]. A person with a growth \\nmindset faces challenges in a positive way resulting in progressive \\nachievements. Earlier work on the growth mindset is rooted in the \\nbelief that basic qualities are dynamic and changing. In learning \\nenvironments, growth mindset can be encouraged through specific \\nfeedback and cues to students, emphasizing effort instead of \\naccomplishment. Research has demonstrated that a growth \\nmindset can raise the grades and engagement of underrepresented \\nstudents [3, 4]. Innovations explored here support providing \\ngrowth mindset training to computer science faculty and the use \\nof growth mindset techniques and feedback methods in CS0 and \\nCS1 computer science classrooms. Resources to support the use of \\na growth mindset in the computer science classroom are limited, \\nand faculty development in the use of growth mindset in the \\nclassroom is rare. Prior work has focused on growth mindset in \\nthe K-12 classroom [5, 6], and materials support teacher-student \\ndaily classroom interaction. This research team developed faculty \\ntools and practices that promote student development of a growth \\nmindset approach to their CS0 and CS1 courses.   \\n2  Background \\n2.1  Growth mindset in education \\nPrior work on growth mindset in educational settings has \\nidentified the use of growth mindset to move students away from \\ndeficit thinking and towards strengths-based thinking [7-9]. \\nGrowth mindset is asset-based and is a method which may help \\nstudents overcome identity threats that may impact academic \\nperformance [10, 11]. Approaches for implementing growth \\nmindset in the classroom often focus on K-12 environments [5, 6]. \\nOne of the most detailed outlines of interventions for \\nimplementing growth mindset in the classroom [8] was prepared \\nfor teachers of students with disabilities, but does not provide \\ninformation on a specific implementation, the grade levels of \\nstudents for the interventions identified, or any results. We build \\non this literature and show student achievement comparisons \\nacross time and across instructors to build a case for the utility of \\nthe growth mindset model developed for CS. \\n2.2  Growth mindset in computer science \\nFaculty who communicate to students that computer science is a \\nfield that can be learned may support student persistence in the \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n878field [12, 13]. Computer science educators have theorized how \\nfaculty may support students in shifting fixed mindsets to those of \\nmalleability [14-17], and the field suggests that metacognition and \\nself-regulation, two practices of growth mindset learners, is vital \\nto success in computer science [18]. Establishing a growth mindset \\nis said to improve student academic outcomes [19, 20]. Studies of a \\n“growth mindset pedagogy” exist in K-12 learning environments \\n[21, 22] but few have studied how faculty convey messages of \\nmalleable intelligence through their teaching and assessment \\npractices. \\n2.2  Research study context \\nThe research study presented is the beginning of a longer, \\nlongitudinal examination of student support structures to \\nencourage retention in the computer science major and \\npersistence to degree completion. The context is a 4-year public \\nurban research university. The university was designated as a \\nHispanic serving Institution (HSI) in the past 5 years, and the \\ndepartment where this research was conducted is more diverse \\nthan the university, with a student population that is 54% \\nunderrepresented in the profession, evenly split between Hispanic \\nand Black students at 27% each. Many of the CS and IT majors are \\nfirst-generation college students and arrive at the university with \\nno prior experience or exposure to computer science \\nprogramming.  \\nBeginning in Fall 2019, the department began research on the use \\nof growth mindset methods in the CS0 and CS1 classrooms. The \\nresearch was initiated with a talk in Spring 2020 by Jen Rosato \\nfrom the College of St. Scholastica in Minnesota, who had \\nsuccessfully piloted a growth mindset program. A faculty reading \\ngroup was initiated, using [5], and faculty received individual \\ncopies of the book, with the intention of meeting monthly to \\ndiscuss possible classroom interventions. Meetings continued \\nduring Spring 2020, but discussion moved from in-person \\nclassroom teaching to virtual classroom teaching, and initial plans \\nto use growth mindset methods and develop university growth \\nmindset materials were superseded by the need to establish \\nremote, virtual classroom instruction, as the university and \\nsurrounding community was very highly impacted by the \\npandemic and associated restrictions. \\nDuring the summer of 2020, growth mindset discussions resumed, \\nwith the faculty working group becoming more focused, as they \\nrevised CS0 and CS1 curriculum and course materials, to ensure \\nconsistency with increasing undergraduate enrollment. Asset-\\nbased pedagogy continued to be a priority, as students were \\nperceived as more isolated due to the pandemic, potentially \\nmaking a growth mindset even more important for long-term \\npersistence and success. Over time, the faculty have been \\ncollaborating and moving growth mindset ideas into the classroom \\nteaching philosophies, incorporating more growth mindset \\nlanguage into interactions with students, and deliberately shifting \\ncourse design, including lecture materials, homework, and \\nlaboratory assignments, towards growth mindset pedagogies. \\nEarlier work [23] outlined the six principles of growth mindset, \\nadopted for computer science. These include identifying \\nmotivations for each topic in computer science, encouraging the process of mastery, praising effort, not ‘ability’, identifying and \\nencouraging initiative, such as planning, and thinking ahead, \\nencouraging persistent and multiple attempts, providing positive, constructive feedback, and avoiding the imposter syndrome.  \\n3  Approach \\n3.1  Growth mindset categories \\nFirst, the faculty distinguished between static and dynamic growth \\nmindset. Under dynamic growth mindset, in the classroom, the \\ntimeline of the course is also necessary to keep track of. While \\nthere is a general guideline of encourage and guide students on \\ntheir path to eventual success, there are timeline specific events \\nsuch as evaluation of assignments, quizzes, and exams. Such \\nevents require time specific feedback: e.g., “you can catch up on \\nwhat you’ve missed”, “just always be coding” - at the beginning of \\nthe course; “you are on the right track”, “moving in the right \\ndirection” - close to the middle of the semester; and “almost there”, \\n“you just need to finish it” - before the finals. \\nGrowth mindset (GM) can also be categorized as group vs \\nindividual. While it is important to guide “all” in the right \\ndirection and make sure that faculty are spending time preparing \\nall students to their main exams and quizzes starting in advance \\nand then discuss the exam results and point to the weak side of \\noverall performance in the section(s), faculty are able as well to \\nembed growth mindset into feedback for every single student. \\nThere might be a need to talk in person or send additional emails \\nregarding those outlining exam results in both lower and upper \\nmargins. \\nAnother gradation which can be used while talking about GM \\ncategories is general vs specific/detailed GM. When giving \\nfeedback on a student’s assignment or grading an exam, details \\nreally matter. The students should feel that faculty care and do \\nread student explanations and understand student thoughts. This \\ncan be demonstrated by faculty addressing specific details of the \\nwork and potential mistakes or praise the good work in detail. \\nThe faculty, as educators, had as a goal to combine all these GM \\ntechniques and implement them simultaneously, making GM an \\ninseparable part of the courses. This integration of growth mindset \\nbecomes the way an instructor thinks and evaluates student work, \\nwithin the tight timeframe of an individual class.  \\n3.2  Research questions \\nThe faculty designed a comparative study, based on Fall 2020 \\n(FA20) and Spring 2021 (SP21) virtual (remote) classroom data. \\nBoth semesters were taught synchronously due to covid-19, for the \\nfull 16-week semester, using Blackboard as the learning \\nmanagement system (LMS). The course experience was consistent, \\nin that the same virtual environment, CS0 or CS1 syllabus, remote \\noffice hours, and remote learning support were available. The \\nresearch questions were:  \\nRQ1: “Do explicit quotes and statements added to labs and \\nassignments about growth mindset influence student achievement, \\nas measured by course grades in CS0 and CS1?”  \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n879RQ2: “Do these quotes and statements influence student \\nachievement differentially (e.g., impact women more than men)?” \\n FA20 instruction was conducted without growth mindset \\ninterventions. The research study was refined in late 2020 and the \\nfaculty used growth mindset interventions in SP21.  \\n4  Growth Mindset in the Classroom \\n4.1  Growth Mindset Materials \\nOf the 5 faculty teaching 7 sections of CS0 and 4 faculty teaching 5 \\nsections of CS1, 2 faculty from CS0 and 2 faculty from CS1 used \\nthe growth mindset interventions in the timeframe specified for \\nthe study. The participating faculty either had been part of the \\nfaculty working group, initially started in 2019, or were closely \\nassociated with the working group, participating in discussions, \\nand sharing and contributing teaching materials to the growth \\nmindset repository the faculty was building for CS0 and CS1.  \\n Pedagogical change on the assignments were very specific and \\nclear, as shown in Figure 1. The approach of displaying growth \\nmindset inspirational quotes on bulletin board or in the classrooms \\nare good examples of visually displaying them [8]. The faculty \\nrealized that inspirational quotes embedded in labs and \\nassignments that are visually recognizable would promote growth \\nmindset. Thus, the research team collected inspirational quotes \\nthat presented growth mindset ideas and added them to all the labs \\nand assignments. Table 1 provides a selection of the inspirational \\nquotes used as a header on all the pages for the labs and \\nassignments as shown in Figure 1. \\n \\n \\nFigure 1. Examples of inspirational quotes used in CS0/1 \\nlabs \\nThe inspirational quotes are grouped by theme. Based on the \\ntimeline and type of lab and assignments, the inspirational quotes \\nare carefully selected. For example, quotes from the “Don’t Give \\nUp” theme are used towards the end of the semester, with quotes \\nfrom the “On Failure” theme being used for complicated \\nlabs/assignments, and quotes from the “Computer Science Quotes” \\ntheme is used in the middle of the semester, and so on. \\n4.2  Growth Mindset in the Classroom \\n4.2.1 Participants. The research study was managed under an \\napproved IRB protocol. Undergraduates enrolled in CS0 and CS1 \\nfor FA20 and SP21 participated in the research. During FA20, all CS0 and CS1 sections were taught as usual. During SP21, the \\nfaculty members who were participating in the growth mindset \\ncommunity of practice used their updated labs, assignments, \\nlectures, verbal cues, and feedback to students in the virtual \\nclassroom. All students enrolled in the sections with the growth \\nmindset faculty participated. Students in classes with faculty who \\nwere not participating in the growth mindset community of \\npractice did not receive the growth mindset interventions. Their \\nclass was taught in the usual manner, as it had been during FA20. \\n \\nTable 1. Selected inspirational quotes used in \\nlabs/assignments Theme – Hard Work, Determination, Effort \\nGenius is one percent inspiration and ninety-nine percent perspiration. — Thomas \\nEdison \\nWork hard now. Don’t wait. If you work hard enough, you’ll be given what you \\ndeserve. — Shaquille O’Neal  \\nTheme - Success doesn’t happen by accident \\nSuccess is the ability to go from one failure to another with no loss of enthusiasm. \\n— Winston Churchill  \\nSuccess is not an accident; success is a choice. — Stephen Curry  \\nTheme - On Failure, Mistakes are how to learn \\nNothing is impossible. The word itself says ‘I’m possible!’ — Audrey Hepburn \\nA person who never made a mistake never tried anything new. — Albert Einstein  \\nTheme - Never stop learning, Don’t Give Up \\nIf you quit once it becomes a habit. Don’t Quit. — Michael Jordan  \\nMost of the important things in the world have been accomplished by people who \\nhave kept on trying when there seemed no hope at all. — Dale Carnegie  \\nTheme - Computer Science Quotes \\nProgramming is like a game of golf. The point is not getting the ball in the hole but \\nhow many strokes it takes. — Harlan D. Mills  \\nI'm not a great programmer; I'm just a good programmer with great habits. — Kent \\nBeck \\n \\n4.2.2 Procedure. The faculty community of practice had discussed \\nthe types of common errors for each CS concept introduced in CS0 \\nand CS1. Numerous areas were identified, including weak \\nfoundations in algebra and logic, confusion about variable scopes, \\nlogical operations, loops and stop conditions, array and index \\nvalues, nested if-else and nested loops. \\nNext, the faculty discussed how they could collectively address \\nthese common problem spots and errors. The consensus was that \\nfor the GM faculty, errors, and problems which the students might \\nencounter would be discussed at the time an assignment or lab \\nwas given, to normalize the expected error(s), and to discuss how \\nstudents could respond.  \\nFaculty discussed how praise can shape students’ mindsets, the \\ndifference between process praise and person praise, and, how to \\nshift praise to highlight the process. Examples of process praise \\nwere shared during the community of practice meetings, with \\ndiscussions of where the comments could be used. The faculty goal \\nwas to avoid praising things that are typically considered stable \\nsuch as talent or intelligence. The faculty wanted to encourage the \\ndevelopment of students with a growth mindset, embracing \\nproblems as an opportunity to learn in contrast to students who \\nassume that their intelligence is set, are more likely to seek to \\ndemonstrate their “smartness” and less likely to ask questions to \\novercome setbacks in their learning. Understanding this, the \\nfaculty realized, that whether a student has a growth or fixed \\nmindset has a direct impact on how he or she faces academic \\nchallenges. A student with a growth mindset will accept \\nchallenges and persevere in order to succeed. This student will \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n880face learning and challenges with an “I have not mastered this \\n‘yet’” attitude, implying that they will master the concept in the \\nfuture. \\n4.2.3 Interventions . The growth mindset (GM) classes were very \\nsimilar to the non-GM courses. Courses began in the usual way at \\nthe start of the semester, and the GM interventions were restricted \\nto assignments, lectures, and verbal cues and feedback to students. \\nWithin the GM courses, each programming assignment given to \\nstudents now had a GM quote as a header at the top of the page, \\noffset by color and attributed to the speaker. Laboratory \\nassignments also included motivating quotes in the header on each \\npage. During lectures, one or two slides were used to initially to \\ninclude a motivating quotation, which was briefly read and \\ndiscussed with the class, before moving on to the standard lecture \\nmaterials.  \\nThe most striking intervention was with the growth mindset \\nfaculty changing their behavior when providing verbal cues and \\ndirect feedback to students. For example, rather than stating that a \\nsolution was ‘wrong’, faculty would state that the student had ‘not \\nyet’ solved the problem. Common shared errors were discussed, \\nand classes discussed solutions together, reducing the potential for \\nindividual isolation and discouragement.  \\n5  Data Gathering and Evaluation \\n5.1  Data Collection \\nIn this study, the lab and assignment scores were collected for CS0 \\nand CS1 courses for FA20 and SP21. For FA20, CS0 had a total of 7 \\nsections with 130 students, CS1 had a total of 6 sections with 103 \\nstudents. For SP21, CS0 had a total of 7 sections with 114 students, \\nCS1 had a total of 5 sections and 98 students. In summary, a total \\nof 233 students for FA20 and a total of 212 students for SP21.  \\nIn particular, the GM faculty that taught CS0 and CS1 in FA20 \\nwith no growth mindset interventions were a total of 3 and 5 \\nsections, respectively, consisting of a total of 143 students (Table \\n2). The same GM faculty that taught CS0 and CS1 in SP21 with \\ngrowth mindset interventions were a total of 3 sections each \\nconsisting of a total of 104 students. The remaining details of the \\nnumber of male/female students, ethnicity, and race information is \\nshown in Table 2.  \\nGender is categorized as Male and Female; Ethnicity as Not \\nHispanic or Latino (NHS) and Hispanic or Latino (HIS); Race as \\nWhite or European American (WH), Black or African American \\n(BA), Asian American (AS), American Indian or Alaska Native \\n(IA), and Native Hawaiian or Other Pacific Islander (HP), which \\nare based on the Census Bureau classification. \\nTwo GM faculty taught CS0 for both semesters with the same \\nmaterial and rubrics consisting of 20 labs and 7 assignments. The \\nsame process was used with CS1 for both semesters consisting of 7 \\nassignment and 7 labs. For CS0, GM_Faculty_A taught one section \\nand GM_Faculty_B taught two sections for both semesters. For CS1, GM_Faculty_C taught one section for both semesters and \\nGM_Faculty_D taught four sections in FA20 and two sections in \\nSP21. \\n \\nTable 2. Participants   Gender Ethnicity  Race \\n  M F NHS HIS WH BA AS IA/HP \\nFA 20 \\nN = 143 CS0 = 58 \\n3 sections 50 8 36 22 23 26 9 0 \\nCS1 = 85 \\n5 sections 64 21 56 29 42 28 13 2 \\nN 114 29 92 51 65 54 22 2 \\nSP 21 \\nN = 104 CS0 = 46 \\n3 sections 35 11 33 13 17 20 9 0 \\nCS1 = 58 \\n3 sections 46 12 45 13 25 20 12 1 \\nN 81 23 78 26 42 40 21 1 \\n5.2  Data Analysis Methods \\nData was gathered after the conclusion of Fall and Spring \\nsemesters. Lab and homework assignments, and final grades were \\nall used for the analysis. The CS0 and CS1 data score from the GM \\nfaculty’s sections (Table 2) were merged and classified into two \\ngroups, FA20 (non-GM) and SP21 (GM). The data was further \\nseparated by gender, ethnicity, and race for the statistical analysis. \\n \\n \\nFigure 2. Lab and Assignment average scores for non-GM \\n(FA20) and GM (SP21) \\n6  Results \\n6.1  RQ1: GM Impact on Learning Outcomes \\n6.1.1. Labs and Assignments . Overall, students in the growth \\nmindset semester (SP21) performed better on both the labs and \\nassignments with an overall average increase of 12% in CS0 and \\n8.5% in CS1 as shown in Figure 2. Here on after, both labs and \\nassignments data scores for CS0 and CS1 were combined (i.e. \\nCS0/1) and classified as FA20 (non-GM) and SP21 (GM). \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n8816.1.2. Mean Grades . From FA20 to SP21, CS0/1 students in the \\ntarget classrooms shifted mean grades from 67.5 to 74.3 on a 100-\\npoint grade scale (Figure 3), with the pattern holding for both \\ncourses. These differences were compared using independent \\nsample t-tests to understand whether the differences could be due \\nto chance, or whether this null hypothesis should be rejected. The \\nstudents who received the growth mindset intervention ( µ = 74.4, \\nsd = 27.7) compared to the students who did not receive the \\nintervention ( µ = 67.5, sd = 39.4) demonstrated significantly better \\ngrades t(460) = -2.63, p = .004. \\n6.2  RQ2: GM Impact by Demographics \\n6.2.1. Gender . When compared as sub populations, both men and \\nwomen improved their average scores in the growth mindset \\ncondition. Specifically, men raised their mean score from 66.1 to \\n73.4 (Figure 3). The growth mindset condition led to better grades \\nfor men ( µ = 73.5, sd = 27.4) than the control condition ( µ = 66.1, \\nsd = 28.7) and this difference was statistically significant t(357) = -\\n2.55, p = .006. Women also improved in the growth mindset \\ncondition from a mean of 72.9 to 77.4, yet the lower number of \\nwomen in the dataset led to lack of power to detect the difference \\nas statistically significant for them as a subgroup t(99) = -0.75, p = \\n.229. \\n6.2.2. Ethnicity : An unanticipated outcome was found with \\nethnicity, as the GM interventions had been designed for all \\nstudents and did not target any community of students. Mean \\nscores for Hispanic students appeared to increase at a greater rate \\nthan for non-Hispanic students, yet both subgroups increased their \\nachievement markedly. Hispanic students who received the \\ngrowth mindset intervention shifted nearly 10 points in final \\ngrades, with SP21 students earning mean score of 75.6 (sd = 25.1) \\nand FA20 students earning lower scores ( µ = 65.6, sd = 29.8) t(119) \\n= -2.21, p = .014). Non-Hispanic students in the growth mindset \\ncondition ( µ = 73.9, sd = 28.6) outperformed non-Hispanic students \\nin the control condition ( µ = 68.6, sd = 29.2 ) t(330) = -1.70, p = \\n.046) \\n6.2.3. Race : Other subgroups were compared from Fall to Spring, \\nwith the growth mindset intervention occurring in SP21. Black \\nstudents improved their average grades 10 points from control condition ( µ = 61.6, sd = 29.7) to growth mindset condition ( µ = \\n71.5 , sd = 28.1), t(175) = -2.35, p = .01. Asian students remained \\nrelatively steady with a 2-point shift, and white students improved \\ntheir scores by approximately 7 points from control condition ( µ = \\n70.9, sd = 27.5) to growth mindset condition ( µ = 77.2, sd = 26.5), \\nt(182) = -1.65, p = .049. The changes for Asian students were not \\nstatistically different.  \\nA threat to validity of an academic achievement study could be the \\neffect of time on grade results—meaning the specific semester \\nitself (Spring semester, at the year mark of a global pandemic) may \\nlead to increased achievement rather than the course \\nimplementation of a new initiative. The previous data comparison \\nkept the instructor constant and compared across time. This \\nanalysis compares across instructors but holds the time period \\nconstant.  \\n6.3  GM Impact by GM and non-GM faculty \\nThe final letter grade earned in the course were converted to the \\n4.0 GPA scale. This was done because while the instructors who \\nwere a part of the study were sharing final numeric percentages, \\nfor the faculty who were not including growth mindset pedagogies \\nonly final letter grades were available. \\n \\nFigure 4. Average GPA (4.0 scale) for non-GM instructors \\nand GM instructors \\nFigure 4 shows differences in average grades for each of the two \\ncourses. For CS0, the average GPA was 2.38 (C+) for non-GM \\nFigure 3. Gender, Ethnicity, Race: non-GM (FA20) and GM (SP21) \\n \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n882classes and 3.18 (B) for GM classes. For CS1, the average GPA was \\n2.66 (C+) for non-GM classes and 2.97 (B-) for GM classes. An \\nindependent samples t-test was used to see if the groups differed \\nstatistically from one another in the Spring of 2021 only, in other \\nwords, whether students in the growth mindset courses did \\nsignificantly better than the students enrolled in the same courses \\nat the same time with no growth mindset pedagogies. Students in \\nthe GM condition ( µ = 3.03, sd = 1.27) outperformed their non-GM \\npeers in SP21 ( µ = 2.75, sd = 1.24) by nearly a letter grade, and this \\ndifference was statistically significant t(210) = -3.11, p = .001. \\n6.4  Student-reported Impact \\nThe impact of the use of the growth mindset materials on students \\nwas also assessed through a 5-question Likert scale survey, \\nadministered to the students in the growth mindset sections at the \\nend of the semester. A total of 69 students responded to the \\nsurvey, with CS0 ( n = 30) and CS1 ( n = 39), as seen in Figure 5. \\nOverall students noticed the statements and most said they \\nsupported their learning, kept attitude positive. \\n \\nFigure 5. Student responses ( n = 69) to the use of GM  \\n \\nStudents in the growth mindset condition were asked to describe \\ntheir experience of the growth mindset phrases that were added to \\nassignments and labs, and sixty-nine students completed the \\nsurvey. First, more than two thirds agreed they noticed the \\nphrases and they were well visualized (71%) while slightly fewer \\nsaid the phrases helped them work hard through difficult \\nsituations (61%).  \\n \\nTable 3. Comments from students in CS0/CS1 GM sections \\nLove the comments. They are very motivating.  I enjoyed seeing them on the homework and labs. I liked reading the different quotes \\nevery time.  \\nI love it! I hope this continues because it motivates the student to face new challenges.  \\nI like those phrases; it helps me not to give up when I have problems with the labs  \\nWhen I get nervous about an assignment, I always think that I’m just learning and \\nwill grow to know it.  \\nI think that it should be more visible and by people that are well known by students.  \\n \\nMore than half agreed that the phrases motivated/engaged their \\nlearning in class (59%) and more than half read all of the phrases \\non their labs and assignments (59%). More than half stated the \\ngrowth mindset phrases encouraged them to complete \\nassignments to a greater degree than other courses (56%). \\n \\n A full dataset, with comparison of the letter grades awarded, t-test \\nresults, and charts, showed that all students did better when the \\ngrowth mindset statements were added to the classroom. This \\nstudy was conducted with virtual classrooms, and the research \\nteam is currently working to see if this continues to hold true as \\nface-to-face classroom interaction resumes. Table 4 summarizes \\nthe aforementioned statistical results. \\n \\nTable 4. Summary of t-test Results \\nNull hypothesis H₀: μ₁ - µ₂ = 0, Alternative hypothesis H₁: μ₁ - µ₂ < 0 Criteria Mean  \\n(FA20, SP21) StDev \\n(FA20, SP21) t df p-value Hypothesis \\nAll CS0/CS1 67.5, 74.3 39.4, 27.7 -2.63 460 0.004 Reject \\nGender Male 66.1, 73.5 28.7, 27.4 -2.55 357 0.006 Reject \\nFemale 72.9, 77.4 31.7, 29.1 -0.75 99 0.229 Accept \\nEthnicity Non-Hispanic \\n(NHS) 68.6, 73.9 29.2, 28.6 -1.69 330 0.046 Reject \\nHispanic (HIS)  65.6, 75.6 29.8, 25.1 -2.20 119 0.015 Reject \\nRace White (WH)  70.9, 77.2 27.5, 26.5 -1.65 182 0.049 Reject \\nBlack or \\nAfrican \\nAmerican \\n(BA) 61.6, 71.5 29.7, 28.1 -2.34 175 0.010 Reject \\nAsian (AS)  72.3, 74.0 32.2, 30.1 -0.25 83 0.401 Accept \\nGPA Non-GM \\nclasses,  \\nGM classes 2.75, 3.03 1.24, 1.27 -1.76 220 0.004 Reject \\n7  Conclusion and Future work \\nThe results demonstrate that, in a virtual environment, the \\nimplementation of growth mindset motivation quotes, verbal cues \\nand feedback to students is easy to do and can make a detectable \\ndifference. Faculty discussed common pitfalls with the class, \\nidentifying them as expected and making sure students had \\nstrategies to detect and overcome challenges. Faculty focused on \\nindividual and overall class improvement. The sample size for this \\nresearch was small, given the one-year timeframe of the study and \\nthe smaller class size of 20-25 which the department maintains for \\nimproved learning. The research team will continue to gather data \\nto understand if a larger study, with greater numbers, shows \\nsimilar results. In addition, the team will begin to make more \\ncomplex statistical analyses of the data as numbers increase and \\nintersectional groups (Latinas, black men) reach large enough \\nnumbers for statistical analyses to be meaningful. This academic \\nstudy is unique in that the diverse population allows for \\nmarginalized groups to reach critical mass for statistical study. \\nThis is part of an ongoing effort towards growth mindset teaching, \\nand it is a promising finding that faculty who have been \\ndeveloping the growth mindset framework in computer science \\nteaching and learning for multiple semesters saw an increased \\nbump in achievement with the addition of explicit quotes and \\nstatements. \\nThe research study presented in this paper has also developed an \\ninitial set of faculty interview codes that denote fixed and growth \\nmindset beliefs of educators in computer science. Future work will \\ninclude an external evaluator who will add observations of lessons \\nand host focus groups with students to better understand how the \\nideas of incremental intelligence are evident (or not evident). \\nTriangulating these sources could provide new information \\nregarding how instructional practices that emphasize effort may \\ninfluence student learning.  \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n883ACKNOWLEDGMENTS \\nThis work is supported by the National Science Foundation under \\ngrant numbers 1834620 and 1928452. \\nREFERENCES \\n[1] Carol S. Dweck, Mindset: The New Psychology of Success , Ballentine, 2006. \\n[2] Carol S. Dweck, (2010). Even Geniuses Work Hard. Educational leadership: \\nJournal of the Department of Supervision and Curriculum Development , N.E.A. 68. \\n16-20.  \\n[3] Joshua Aronson, Carrie B. Fried, Catherine Good. Reducing the Effects of \\nStereotype Threat on African American College Students by Shaping Theories \\nof Intelligence. Journal of Experimental Social Psychology , 38, pp. 113-125, 2002. \\n[4] Ilan Dar-Nimrod and Steven J. Heine. Exposure to Scientific Theories Affects \\nWomen’s Math Performance. Science, Vol. 314, October 20, 2006, pp. 435.  \\n[5] Annie Brock, and Heather Hundley. The Growth Mindset Coach . Ulysses Press, \\n2016. \\n[6] Annie Brock, and Heather Hundley. The Growth Mindset Playbook: A Teacher’s \\nGuide to Promoting Student Success . Ulysses Press, 2017. \\n[7] C. Anne Gutshall. Teachers’ Mindsets for Students with and without \\nDisabilities. Psychology in the Schools , Volume 50, pp. 1073-1084. \\n[8] Justin D. Garwood, and Abby A. Ampuja. Inclusion of Students With Learning, \\nEmotional, and Behavioral Disabilities Through Strength-Based Approaches. \\nIntervention in School and Clinic , Vol. 55, No. 1, Sept. 2019, pp. 46–51. \\n[9] David Yeager, Paul Hanselman, Gregory Walton, et al. A national experiment \\nreveals where a growth mindset improves achievement. Nature 573, 364-369 \\n(2019). \\n[10] Brian Spitzer and Joshua Aronson. Minding and mending the gap: Social \\npsychological interventions to reduce educational disparities. The British Journal \\nof Educational Psychology . 2015, 85(1):1-18.  \\n[11] Emily Rhew, Jody S. Piro, Pauline Goolkasian & Patricia Cosentino. (2018) The \\neffects of a growth mindset on self-efficacy and motivation , Cogent Education, 5:1. \\n[12] Frederika Brown and Kelly Cross. Engineering Faculty’s Mindset: An Analysis \\nof Instructional Practice, Learning Environment, and Teacher Authenticity. 2019 \\nIEEE Frontiers in Education Conference (FIE) , 2019, pp. 1-4. \\n[13] Jeni L. Burnette, Crystal L. Hoyt, V. Michelle Russle, Barry Lawson, Carol S. \\nDweck, and Eli Finkel. A Growth Mind-Set Intervention Improves Interest but \\nNot Academic Performance in the Field of Computer Science. Social \\nPsychological and Personality Science, 2020;11(1):107-116.  [14] Charles Dierback, Blair Taylor, Harry Zhou, and Iliana Zimand. Experiences \\nwith a CS0 Course Targeted for CS1 Success. Proceedings of the ACM 36th \\nSIGCSE Technical Symposium on Computer Science Education , 2005, pp. 317-320. \\n[15] Laurie Murphy and Lynda Thomas. Dangers of a fixed mindset: implications of \\nself-theories research for computer science education. Proceedings of the 13th \\nAnnual Conference on Innovation and technology in Computer Science Education \\n(ITiCSE ‘08) , 2008, ACM, USA, pp. 271-275. \\n[16] Antti-Juhani Kaijanaho and Ville Tirronen. 2018. Fixed versus Growth Mindset \\nDoes not Seem to Matter Much: A Prospective Observational Study in Two Late \\nBachelor level Computer Science Courses. Proceedings of the 2018 ACM \\nConference on International Computing Education Research (ICER '18) . ACM, pp. \\n11–20. \\n[17] Michael Lodi. 2019. Does Studying CS Automatically Foster a Growth Mindset?, \\nProceedings of the 2019 ACM Conference on Innovation and Technology in \\nComputer Science Education (ITiCSE '19) . ACM, New York, NY, USA, 147–153.  \\n[18] James Prather, Brett A. Becker, Michelle Craig, Paul Denny, Dastyni 12 Loksa, \\nand Lauren Margulieux. 2020. What Do We Think We Think We Are Doing? \\nMetacognition and Self-Regulation in Programming. Proceedings of the 2020 \\nACM Conference on International Computing Education Research (ICER '20) . \\nACM, USA, 2–13. \\n[19] Keith Quille and Susan Bergin. 2020. Promoting a Growth Mindset in CS1: Does \\nOne Size Fit All? A Pilot Study. Proceedings of the 2020 ACM Conference on \\nInnovation and Technology in Computer Science Education , ACM, USA, 12–18. \\n[20] Jane G. Stout & Jennifer M. Blaney (2017). But it doesn’t come naturally: how \\neffort expenditure shapes the benefit of growth mindset on women’s sense of \\nintellectual belonging in computing, Computer Science Education , 27:3-4, 215-\\n228. \\n[21] Inkeri Rissanen, Elina Kuusisto, Moona Tuominen, Kirsi Tirri. In search of a \\ngrowth mindset pedagogy: A case study of one teacher's classroom practices in \\na Finnish elementary school. Teaching and Teacher Education , Volume 77, 2019, \\npp. 204-213. \\n[22] H. Zeeb, J. Ostertag, and A. Renkl. Towards a Growth Mindset Culture in the \\nClassroom: Implementation of a Lesson-Integrated Mindset Training. Education \\nResearch International , Vol. 2020, p. 8067619, March 2020. \\n[23] Patricia Morreale, J. Jenny Li, Ching-Yu Huang, Daehan Kwak, Jean Chu, Yulia \\nKumar, and Paolien Wang. 2021. Framework for a Growth Mindset Classroom. \\nIn Proceedings of the 52nd ACM Technical Symposium on Computer Science \\nEducation (SIGCSE '21) . ACM, New York, NY, USA, 1269. DOI: \\nhttps://doi.org/10.1145/3408877.3439631   \\n \\n \\nVirtual Session: Interventions\\n \\nSIGCSE ’22, March 3–5, 2022, Providence RI, USA\\n884\",\n",
       "  ['race and ethnicity', 'gender', 'cs1']),\n",
       " ('Why, when, and from whom: considerations for collecting and \\nreporting race and ethnicity data in HCI \\nYiqun T. Chen \\nStanford University \\nStanford, CA, United States \\nyiqunc@stanford.edu \\nKatharina Reinecke \\nUniversity of Washington, Seattle \\nSeattle, WA, United States \\nreinecke@cs.washington.edu \\nABSTRACT \\nEngaging diverse participants in HCI research is critical for creating \\nsafe, inclusive, and equitable technology. However, there is a lack \\nof guidelines on when, why, and how HCI researchers collect study \\nparticipants’ race and ethnicity. Our paper aims to take the frst \\nstep toward such guidelines by providing a systematic review and \\ndiscussion of the status quo of race and ethnicity data collection in \\nHCI. Through an analysis of 2016–2021 CHI proceedings and a sur-\\nvey with 15 authors who published in these proceedings, we found \\nthat reporting race and ethnicity of participants is very rare (<3%) \\nand that researchers are far from consensus. Drawing from multi-\\ndisciplinary literature and our fndings, we devise considerations \\nfor HCI researchers to decide why, when, and from whom to collect \\nrace and ethnicity data. For truly inclusive, equitable technologies, \\nwe encourage deliberate decisions rather than default omissions. \\nCCS CONCEPTS \\n• Human-centered computing → Empirical studies in HCI; • \\nSocial and professional topics → Race and ethnicity . \\nKEYWORDS \\nrace, ethnicity, systematic literature review, HCI research, survey \\nACM Reference Format: \\nYiqun T. Chen, Angela D. R. Smith, Katharina Reinecke, and Alexandra \\nTo. 2023. Why, when, and from whom: considerations for collecting and \\nreporting race and ethnicity data in HCI. In Proceedings of the 2023 CHI \\nConference on Human Factors in Computing Systems (CHI ’23), April 23– \\n28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 15 pages. https: \\n//doi.org/10.1145/3544548.3581122 \\n1 INTRODUCTION \\nAs identities of study participants have proven to infuence the up-\\ntake, experience, and benefts of technologies, the Human-Computer \\nPermission to make digital or hard copies of all or part of this work for personal or \\nclassroom use is granted without fee provided that copies are not made or distributed \\nfor proft or commercial advantage and that copies bear this notice and the full citation \\non the frst page. Copyrights for components of this work owned by others than the \\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or \\nrepublish, to post on servers or to redistribute to lists, requires prior specifc permission \\nand/or a fee. Request permissions from permissions@acm.org. \\nCHI ’23, April 23–28, 2023, Hamburg, Germany \\n© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. \\nACM ISBN 978-1-4503-9421-5/23/04...$15.00 \\nhttps://doi.org/10.1145/3544548.3581122 Angela D. R. Smith \\nUniversity of Texas at Austin \\nAustin, TX, United States \\nadrsmith@utexas.edu \\nAlexandra To \\nNortheastern University \\nBoston, MA, United States \\na.to@northeastern.edu \\nInteraction (HCI) community has made considerable eforts to-\\nwards inclusive and diverse research practices over the past few \\nyears [1, 16, 25, 47, 104, 105, 110]. For instance, gender HCI has \\nemerged as a mature subfeld of HCI that focuses on how people of \\ndiferent genders interact with technology [109]. Likewise, HCI for \\ndevelopment is a growing subfeld that considers how designs and \\ntechnologies interact with the under-resourced and economically \\ndisadvantaged communities [112, 116]. In this work, we focus on \\none such diversity dimension — race and ethnicity of participants \\nin HCI — which has remained relatively under-explored in current \\nresearch. In 2017, Schlesinger et al. [104] found that less than 0.1% of \\nthe papers in the CHI proceedings between 1981 and 2016 engaged \\nmeaningfully with race, compared to 0.2% and 0.6% for gender and \\nsocioeconomic class, respectively. Similar fndings were reported \\non the basis of a quantitative content analysis of accepted papers \\nin CHI 2006, 2011, and 2016 [47]. The authors highlighted the im-\\nportance of intersectionality (i.e., an identity framework that seeks \\nto understand the complexity of multiple, overlapping, intersecting \\nsocial identities [17, 28, 95]) when examining the composition of \\nthe participants in HCI research. In particular, they emphasized \\nthat HCI researchers should take an interest in understanding how \\nvarious dimensions of participants’ identities (e.g., race, gender, \\nsocioeconomic status) interact with each other and provided rec-\\nommendations for deeper engagements with the resulting complex \\nidentities. \\nOther research has explored the issue of race, ethnicity, and bias \\nin HCI research and the HCI community via a critical race theory \\nlens [83, 107]. Ogbonnaya-Ogburu et al. [83] argued that racism \\nis pervasive in social-technical systems and implored that HCI re-\\nsearch should be attuned to the issue of race; they suggested that \\nparticipation of under-represented minorities must be sought after \\nin all research activities. Concerted eforts among HCI researchers \\nalso led to a workshop titled “engaging in race in HCI” in CHI \\n2020 that aimed to identify better practices for engaging with race \\nand improving racial inclusiveness and equity in the broader HCI \\ncommunity [107]. This workshop allowed the community to begin \\nassembling recommendations for like-minded researchers to discuss \\nthe role and implication of race in HCI, which ultimately led to a \\nseries of zines featuring the relationship among race, inclusiveness, \\nand HCI research [91]. One of the zines in the series, in particular, \\nurged HCI researchers, practitioners, and designers to consider \\nrace and the implication of race throughout their research process,CHI ’23, April 23–28, 2023, Hamburg, Germany \\nand provided an infographic to aid the process of investigating and \\nreporting race in HCI research. In addition, recent works in HCI \\nhave highlighted the importance of engaging with traditionally \\nexcluded groups in HCI such as Black women [43, 44, 93–95]. They \\ncollectively demonstrated how current technology marginalized \\nBlack people and how race, gender, and economic class infuence \\nthe design choice of various pieces of technology. Finally, this paper \\nbuilds directly on the “prequel” on race and ethnicity collection of \\nparticipants in HCI research [96] by providing considerations on \\nwhen, why, and from whom HCI researchers collect study partici-\\npants’ race and ethnicity. \\nIn addition to HCI, other felds of research have explored the \\ntopic of race and ethnicity in their research as well. For instance, in \\nmedical sciences, the American Medical Association (AMA) Man-\\nual of Style states that “specifying the race or ethnicity of study \\nparticipants can provide information about the generalizability of \\nthe results of a specifc study ”; therefore, it recommends reporting \\naggregate race and ethnicity for all study participants [33]. The \\nAmerican Psychological Association (APA) [3] has made similar \\nsuggestions for empirical studies in psychology. However, given \\nthe breadth of research interests and methodologies in HCI, we \\nshould neither thoughtlessly copy existing recommendations nor \\nignore established practices from other disciplines. In particular, \\nHCI has the tradition of illustrating how groups of users interact \\nwith technology, which situates the feld in a unique position to \\nnarrate and refect on the lived experiences of racial and ethnic \\nminorities through data [28]. \\nWhen put together, the existing work calls for (i) a deeper under-\\nstanding of the current practice of reporting race in HCI; and (ii) \\na guideline consisting of considerations and recommendations for \\nwhen, why, and from whom to collect and report race in research. \\nTherefore, our work makes strides toward this goal by answering \\nthe following research questions: \\nRQ1: When are the study participants’ race and ethnicity reported \\nin HCI research? \\nRQ2: What are some considerations that speak for and against \\ncollecting this information? \\nRQ3: What are relevant considerations on how to collect, report, \\nand use this information? \\nBy answering the three research questions, primarily in the \\ncontext of race and ethnicity in the United States, we make the \\nfollowing contributions to the literature: \\n• We provide an empirical analysis of the frequency of report-\\ning CHI participants’ race and ethnicity, showing that less \\nthan 3% of CHI papers in the proceedings from 2016 to 2021 \\nhave included such information. Moreover, nearly half of \\nthese papers were published in CHI 2021, suggesting a recent \\nincrease in research eforts on race and ethnicity data in HCI. \\n• Through a survey with authors who published in CHI and \\nwere afliated with a U.S. institution during the time of pub-\\nlication, we summarize the motivations and considerations \\nfor and against reporting race and ethnicity in HCI research. \\n• We also synthesize existing discussions on racial and ethnic \\ndata in related felds to examine the potential benefts and \\n(unintended) consequences of reporting racial and ethnic \\ndata in HCI research. • Finally, we close with a set of considerations designed for \\nHCI researchers to reason about racial and ethnic data col-\\nlection and analysis; in particular, we call for careful inter-\\npretations of race and ethnicity data, which serves to narrate \\nthe lived experiences of racial and ethnic minorities when \\nthey interact with the technology under study. While our \\nwork does not claim or intend to provide a set of absolute \\nand complete rules, we hope it will spark the discussion on \\nthe topic of collecting racial and ethnic information in the \\nbroader HCI community. After all, racial and ethnic data col-\\nlection is a start rather than an end in combating systemic \\ninequality, discrimination, and oppression in HCI, a jour-\\nney requiring much caution and nuance from researchers to \\nminimize harmful narratives and misrepresentations. \\nThe authors humbly acknowledge that collecting and discussing \\nrace and ethnicity data is extremely complex and researchers’ per-\\nspectives on this topic are greatly shaped by their own personal \\nexperience and research background. Collectively, we are U.S.-based \\nHCI researchers with diferent research themes (large-scale cross-\\ncultural online studies, narratives and participatory methods for \\nmore inclusive design, and statistical models), methodological fo-\\ncuses (quantitative, qualitative, design, and mixed-methods) and \\ncareer stages (graduate students, early tenure-track assistant pro-\\nfessors, and tenured associate professors). While our team has good \\ncoverage of HCI research methodologies, we are largely limited to \\nspecifc social and cultural contexts of race and ethnicity in the U.S., \\nwhere all authors currently work and live in. Thus, our work might \\nhave limited discussion on race and ethnicity in certain non-U.S. \\ncultures and societies. We hope that our work, despite its U.S.-focus, \\ncould encourage broader conversations across research domains \\nand cultural contexts (especially non-Western ones) in HCI. \\n2 UNDERSTANDING THE HISTORY OF \\nRACIAL DATA COLLECTION \\nOur work is motivated by both the practice of racial data collection \\noutside of HCI and the growing eforts to improve the collection of \\nrelated demographic variables, such as gender and socioeconomic \\nstatus [19, 49, 82, 102, 108] within HCI. In this section, we frst \\nbriefy review racial categorization in the U.S., which provides the \\nfoundation for our quantitative analysis of CHI papers. Furthermore, \\nwe provide a selective overview of the practices for collecting and \\nanalyzing racial data in several research disciplines. We end the \\nsection by sketching how those approaches might inspire parallel \\neforts on racial data in HCI. \\n2.1 Racial and ethnic categorization in the U.S. \\nRacial categories have been included in every U.S. census since 1790, \\nand the history of the U.S. census reveals the complexity of racial \\nand ethnic data collection [88]. Firstly, prior to the 1960 census, \\nan individual’s race was determined by census enumerators (i.e., \\nprofessionals who are hired to visit and survey residents to compile \\ndata for the U.S. census), rather than through self-report. More-\\nover, categories used in the census changed almost every decade to \\nrefect the politics and societal values at the time. For instance, “Na-\\ntive Hawaiian or Other Pacifc Islander” was historically grouped \\nwith Asians and only became a new category in 2000. As anotherWhy, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\nexample, “Mexicans” were counted as a racial category in 1930, but \\nthat category has since disappeared. Since then, many of Mexican \\ndescents resort to the “other race” option and have been grouped \\nunder the Hispanic ethnicity only. The ability to identify as multi-\\nracial (i.e. “mark one or more racial categories”) was only won \\nthrough extensive advocacy in 2000 [121]. Racial categorization \\nhas always been extremely political, and the miscategorization and \\nundercount of people from racial minority groups have contributed \\nto systemic oppression and exclusions [5]. In particular, these ex-\\namples from the U.S. census illustrate that racial and ethnic data \\ncollection does not automatically lead to racial and ethnic equality. \\nWhile the Census Bureau now collects racial and ethnic informa-\\ntion to “make policy decisions for civil rights, to promote equal \\nemployment opportunities, and to assess racial disparities in health \\nand environmental risks” [113], such data was in fact exploited by \\nthe government to prosecute and oppress racial minorities in the \\npast [13]. \\nU.S.-based researchers may also be familiar with the standards \\npublished by the U.S. Ofce of Management and Budget (OMB) in \\n1997, which mandates minimum standards for collecting and pre-\\nsenting data on race and ethnicity to support data analysis across \\ndiferent racial and ethnic groups. The OMB standards have two \\ncategories for ethnicity (Hispanic versus Non-Hispanic) and fve \\ncategories for racial data at a minimum — White, Black or African \\nAmerican, American Indian or Alaska Native, Asian, and Native \\nHawaiian or Other Pacifc Islander. The OMB standards have been \\nthe guideline for collecting and presenting data on race and eth-\\nnicity for all federal reporting, including the decennial census and \\nthe mandates by certain U.S. grant funding agencies such as the \\nNational Institutes of Health (NIH) and the National Science Foun-\\ndation (NSF) [79, 80]. Despite its general success in presenting data \\non race and ethnicity in federal programs, the OMB standards are \\nfar from a “gold standard”, and performed especially poor on histor-\\nically marginalized groups and/or multi-racial populations [62, 63]. \\nFrom the history of racial categories in the U.S., we see that racial \\nand ethnic data not only refects the present social and political \\nenvironment but also directly infuences the future socialization \\nand construction of the concept of race and ethnicity. Therefore, we \\nconclude this section by emphasizing that the goal of collecting and \\nreporting racial and ethnic data is not to arrive at a list of “perfect” \\nracial and ethnic classifcations, but more importantly, to leverage \\nsuch data to refect, acknowledge, and reject racist and oppressive \\npower structures in current research. \\n2.2 Collection and analysis of racial data in \\nresearch \\nIn this section, we look at how the social sciences, medical sciences, \\nand computer sciences have collected and analyzed racial data as \\ncomparative case studies. \\nSocial science scholars have long acknowledged the role of race \\nin shaping individuals’ social status and everyday life experience [9, \\n39, 39, 69]. However, there is less consensus on whether the feld \\nshould use racial classifcations to assess the role and consequences \\nof race. Some argue that collecting and reporting data on race \\nand ethnicity would promote racial division and further the status \\nquo of racial discrimination. In contrast, others take a “what we cannot measure, we cannot understand” approach and continue to \\nreport observed racial diferences from profling in law enforcement \\nto disparity in healthcare systems [9, 14]. In 2003, the American \\nSociological Association (ASA) issued a statement in support of \\nthe continual collection and research of data on race [4]. Their \\nreasoning is summarized as follows: (i) racial identities are central \\nto the social organization and relationships and, therefore, the very \\ncore of social science research; (ii) taking a “colour-blind\" approach \\nand ignoring participants’ race in research does not eliminate the \\nuse of racial categories and racism in everyday life, as well as the \\nresulting impact on societal outcomes; and (iii) understanding the \\nrole of race is central to challenging the existing systems of racial \\ndiscrimination and stratifcation. \\nIn contrast to the debates over racial data collection in social sci-\\nences, race and ethnicity of study participants are widely collected \\nand used in healthcare databases to ascertain important group-level \\ndiferences in healthcare outcomes in the U.S. [64, 86]. The basis of \\nthe observed race-associated diferences in healthcare outcomes, \\nhowever, remains under-explored, further illustrating that data col-\\nlection itself does not automatically translate into racial and ethnic \\nequality. One exception is Jones [59], who argued that as a social \\nconstruct, race only serves as a very rough proxy for variables of \\ninterest such as social class and culture. Instead, race often appears \\npredictive of healthcare outcomes because of the racism that has \\noperated throughout U.S. history and to date. As an example, an \\nanalysis by Jones et al. [60] demonstrated that being classifed by \\nothers as “White” is associated with better health status, regardless \\nof one’s self-identifcation. In view of the complex interpretation of \\n“race”, multiple threads of work have urged researchers in public \\nhealth to take an interest in elucidating the underlying causes of the \\nobserved diferences across race and ethnicity groups, e.g., by gener-\\nating hypotheses about the basis and designing data collection and \\nanalysis plans to test the hypotheses. For instance, the diference \\nin rates of estrogen-receptor-negative breast cancer between Black \\nand White women in the U.S. is well-documented. Building on this \\nobservation, Krieger et al. [67] demonstrated that being born in the \\nstates that practiced Jim Crow laws (i.e., legal racial segregation) \\nis associated with higher odds of cancer, thereby attributing the \\nobserved diferences to racially discriminating laws. \\nIn computer science, fair machine learning is one of many re-\\nsearch areas that share similar interests in analyzing racial and \\nethnic data. With the growing role of predictive algorithms in criti-\\ncal felds such as credit reporting and employment assessment, a \\nplethora of work has investigated the fairness of the algorithmic \\ndecisions across “sensitive attributes” such as diferent racial and \\nethnic groups [8, 11, 57]. Even though contextual discriminatory \\nimpacts of algorithms, such as the disparity between White ver-\\nsus Black Americans in the criminal justice system, are often cited \\nas motivating examples for this line of research, researchers have \\nlargely treated racial or gender groups as an abstract feature of \\nindividuals. A few recent exceptions include Benthall and Haynes \\n[8], Hanna et al. [42], and Hu and Kohler-Hausmann [50]. Benthall \\nand Haynes [8] argued that people who are labelled as “Black\" in \\nthe U.S. are subject to systemic diferences through spatial, political, \\nand social segregation. Therefore, the use of race categories such \\nas “Black\" might risk reifying racialized social inequality, if the \\nobserved diferences are attributed to the race instead of to theCHI ’23, April 23–28, 2023, Hamburg, Germany \\nunderpinning systemic inequalities. Hence, Benthall and Haynes \\n[8] proposed replacing racial categories with group labels learned \\nthrough data to dynamically capture the underpinning inequalities, \\nwithout furthering the status quo of disadvantaged racial groups. \\nOn the other hand, Hu and Kohler-Hausmann [50] investigated the \\nsocial meaning of racial and gender membership. Their key insight \\nis that prevailing analyses of socially salient categories such as \\nrace neglect the fact that many of the “efects\" (e.g., career choices, \\nfamily, and neighbourhood wealth) attributed to race are, in fact, \\nits constitutive features. They implored for more care in conceptual-\\nizing and interpreting the “causal efects” of race. Finally, Hanna \\net al. [42] argued that most existing research ignores the “multi-\\ndimensionality” of race and instead treated racial and ethnic cat-\\negories as a fxed attribute. They urged researchers in the feld to \\ncontextualize the meaning of race, and focus on the underlying \\nmechanism that produces and reinforces racial inequality. \\nWithin HCI, several authors have argued, through qualitative, \\nmixed-methods, or quantitative methods, that there is a general lack \\nof meaningful engagement with race and ethnicity [29, 47, 83, 91, \\n93, 104]. However, the current practice, as well as the motivations, \\nfor collecting and reporting study participants’ race and ethnicity \\nremains under-explored. In this work, we analyzed recent CHI \\nproceedings to understand the existing practice. Furthermore, we \\nalso surveyed authors to identify the motivations and methods for \\ncollecting racial and ethnic data of their participants. Motivated by \\nparallel eforts on the gender of study participants [82, 102, 108], \\nwe drew from related disciplines and outlined considerations for \\nHCI researchers when it comes to racial data collection. \\n3 STUDY OF RACE AND ETHNICITY DATA \\nCOLLECTION IN HCI \\nTo answer our research questions, we conducted a systematic litera-\\nture analysis of the published papers in CHI proceedings from 2016 \\nto 2021. We followed up with a survey of the authors whose papers \\nreported race and ethnicity data of their participants in this period. \\nThe restriction on time is motivated by (i) the goal to understand \\nthe current practice on collecting racial and ethnic data of study \\nparticipants, and (ii) the observation that older literature rarely \\nreports this information [47]. \\n3.1 Dataset curation \\nWe started with a total of 3,910 research articles published in \\n2016-2021 CHI proceedings on ACM digital library and proceeded \\nwith a keyword-search method informed by prior work [47, 104]. \\nWe initially fltered the collection using the keywords “race” OR \\n“racial” OR “ethnicity”, which narrowed down the corpus to 663 \\narticles. We then experimented with adding potentially defning \\nkeywords (race/racial, ethnicity/ethnic, White/Caucasian, African \\nAmerican/Black, Asian, Hispanic, Native American/American In-\\ndian, Pacifc Islanders), and sampled the frst 10 articles returned \\nin each year to judge the quality of our search (the quality here is \\ndefned to be the number of search results that contained detailed \\nrace and ethnicity information of the study participants). The fnal \\nkeyword set that yielded the most relevant articles empirically is \\n(i) “race” OR “racial” OR “ethnicity” AND (ii) “Hispanic” OR “Black \\nAmerican” OR “Asian” OR “Caucasian”. After identifying this initial collection of 340 articles, we then checked for the racial and ethnic \\ncomposition of study participants in each article manually. Next, \\nwe aggregated the racial and ethnic information of study partici-\\npants over all articles that detailed this information. Because of the \\nfocus on race and ethnicity in the U.S. context for this work, we \\nfurther limit the corpus to the articles for which the authors had \\nU.S. afliations or the participants were recruited in the U.S. The \\ndataset curation process is displayed in Figure 1. \\nFor reference, we obtained racial and ethnic composition from \\ntwo additional sources: (i) 2015–2019 demographics estimates of \\nthe U.S. collected by the United States Census Bureau [114]; and \\n(ii) 2015–2019 demographics estimates of U.S.-based drug trials \\ncollected by the U.S. Food and Drug Administration (FDA) [12]. \\n3.2 Dataset analysis \\nAs stated in Section 2.1, categorizing race and ethnicity is extremely \\ncomplex. In this paper, for simplicity, we adopted the following pro-\\ncedures to group the race and ethnicity categories across diferent \\nstudies. Firstly, since most of the surveyed CHI studies collected \\nethnicity and race using one single question for race and ethnic-\\nity, we make the assumption that the racial categories reported by \\nstudies in our fnal corpus refer to the corresponding Non-Hispanic \\nsubset (e.g., reported White participants in a paper refers to the \\nnon-Hispanic White participants). Second of all, we aggregated the \\nparticipants into the following categories that roughly align with \\nthe OMB standards: White, Black or African American, Asian, and \\nOthers (including American Indian and Alaska Native, Mixed races, \\nNative Hawaiian and Other Pacifc Islander). This choice of analysis \\nis largely driven by the existing racial categories in papers pub-\\nlished in CHI, which certainly does not capture the full complexity \\nof race and ethnicity of the study participants. \\nIn addition to the aggregate analysis of studies in the fnal corpus, \\nwe will also report the following summary statistics: (i) the number \\nof studies that collected ethnicity separately from race, and (ii) \\nthe racial and ethnic breakdown of large (> the median number \\nparticipants across all studies in the fnal corpus) studies and small-\\nto-medium-scale studies (≤ the median number of participants \\nacross all studies in the fnal corpus). \\n3.3 Survey \\nWhile all papers in our fnal corpus reported their participants’ \\nrace and ethnicity, many left the reason and process of collecting \\nthe race and ethnicity of their participants implicit. We, therefore, \\nconducted an additional survey to fnd out why researchers collect \\nracial and ethnic data and to learn about potential challenges they \\nmay have experienced. For each publication in the fnal curated \\ncorpus, we emailed the frst and senior authors with the following \\nlist of open-ended questions on why and how they collected the \\nrace and ethnicity information of their participants. \\n(1) Why did you decide to collect the racial and ethnic informa-\\ntion of your participants? \\n(2) How did you collect the racial and ethnic information of \\nyour participants? (i.e., what were the specifc questions that \\nyou used?)Why, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\n(3) Did you use any resources to decide on whether and how \\nto collect racial and ethnic information about your partic-\\nipants? (e.g., U.S. Census for U.S.-based studies, prior CHI \\npublications, IRB recommendations) \\n(4) If you happen to still have your original questionnaire and \\nwould be willing to share it with us, that would be greatly \\nappreciated as well! \\nAfter excluding those authors with an inactive email address \\n(e.g., due to a change of afliation), we emailed a total of 106 authors \\n(counting both frst and last authors) whose papers were in our \\nfnal corpus. Among those, 15 (14.2%) participated in our survey, \\nall of whom were currently afliated with a U.S. institution or \\ncorporation. \\nIn addition to the survey responses, we note that some papers al-\\nready highlighted the importance of considering race and ethnicity \\nfor the piece of technology under study. For instance, Passmore et al. \\n[87] surveyed gamers from diverse racial and ethnic backgrounds \\nand established “signifcant diferences between players of color and \\nWhite players on the perception of racial norms in gaming, efects of \\nbehavior, emotions, player satisfaction, engagement, and beliefs stem-\\nming from a lack of diversity.” Moreover, they emphasized that the \\ndiverse recruitment amounted to “higher dissatisfaction [in diversity \\nin digital games] than previous research.” \\n \\n \\n \\n   \\n \\n    \\n \\n \\n \\n       \\n \\n       \\n \\n \\n \\n \\n* Keywords: (i) ``race\\'\\' OR ``racial\\'\\' OR ``ethnicity\\'\\' AND (ii) ``Hispanic\\'\\' OR ``African American\\'\\' OR ``Asian\\'\\' OR ``Caucasian\\'\\' . \\nArticles not containing these keywords are excluded using the automatic search tools of the ACM digital library . \\n** Since a n overwhelming  majority of the articles that reported  detailed racial and ethnic data recruited their participants  from the \\nU.S., we adopted this criterion to focus our dis cussion of race and ethnicity in the U. S.  \\n \\n \\n \\n Published articles  from 2016 -\\n2021 CHI proceedings  available \\nat ACM digital library  (n=3,910 ) \\n \\nArticles  screened  for keywords*  \\n(n = 340) Articles  excluded  (n = 3,670 ) \\nArticles assessed manually for (i)  \\nwhether detail ed racial and \\nethnic data were  reported for  \\nstudy participants ; and (ii) \\nwhether the participants are  \\nbased in the U.S. (n = 93 ) Articles  excluded  (n = 247).  \\nReasons for exclusion : \\n• No participants in the \\nstudy  \\n• No race and ethnicity \\ninformation provided  \\n• Only has White versus \\nnon-White categories  \\n• Non-U.S. based \\nparticipants ** \\nStudies included in the final \\ncorpus (n = 93) Identification of final corpus  from  2016 -2021 CHI proceedings  \\nIdentification  \\n Screening  \\n \\nIncluded  \\nFigure 1: The fow of information through diferent phases \\nof the corpus curation process as described in Section 3.1. \\nWe displayed the inclusion and exclusion criteria, as well \\nas the fnal number of resulting publications of each stage. \\nHere, articles screened for keywords ( = 340) are the articles \\nthat discussed or mentioned race and ethnicity regardless of \\nwhether detailed participant-level data are reported. \\n4 RESULT \\n4.1 RQ1: When are the study participants’ race \\nand ethnicity reported in HCI research? \\nAt the time of data collection, the ACM digital library includes \\n3,910 CHI papers published in 2016–2021 CHI proceedings. We analyzed 340 manuscripts that mentioned keywords related to race \\nand ethnicity (see Figure 1 for details), of which only 93 provided \\ndescriptive statistics on the racial and ethnic breakdown of their \\nstudy participants. In other words, our analysis showed that only \\n93 (2.4%) of 3,910 CHI papers included descriptive information \\non participants’ race and ethnicity. This is likely an undercount \\ngiven that the fnal corpus only included studies with the specifed \\nkeywords. \\nOut of the 93 manuscripts, the median number of reported racial \\nand ethnic groups is 4 (IQR: 3–5). Only a small number (17; 18.2%) of \\nstudies mentioned (or was inferred of) using two separate questions \\nfor race and ethnicity. The median number of participants in the \\nstudies in the fnal corpus is 28 (IQR: 18–187); the largest study \\nreported the racial and ethnic breakdown of 2,041 participants [115], \\nand the smallest study in our corpus had only six participants [40]. \\nIn addition, almost all (>90%) of the authors in our fnal corpus \\nwere afliated with a U.S. institution at the time of writing. The \\nnumber of manuscripts is also not evenly distributed across time, \\nwith more than 43 out of 93 (46%) published in CHI 2021, followed \\nby 14 in 2017, 11 in 2019, 11 in 2018, 9 in 2020, and 5 in 2016. \\nThe aggregated studies reported 19,684 participants in total, \\n12,627 (64.1%) of whom are (Non-Hispanic) White; 2,028 (10.3%) \\nBlack; 1,766 (8.9%) Hispanic; 1,327 (6.7%) Asian; and 1,939 (4.6%) \\nOthers (with 205 Mixed races and 98 American Indian or Alaska \\nNatives). By contrast, according to the estimated demographic data \\nby the U.S. census for 2015–2019, 60.7% of the population in the U.S. \\nis (Non-Hispanic) White; 12.3% Black; 18% Hispanic; 5.5% Asian; \\nand 3.5% Others (2.4% Mixed and 0.7% American Indian or Alaska \\nNatives). Regarding the U.S.-based FDA drug trials during 2015 and \\n2019, (Non-Hispanic) White accounted for 64.5% of the participants, \\nfollowed by 16% for Black and 15% for Hispanics. Asians and Other \\ngroups account for 2% and 3.5% of the trial participants, respectively. \\nRacial and ethnic compositions from the three diferent sources \\n(CHI, U.S. Census, and the FDA drug trials) are displayed in Figure 2. \\nWe see that compared to the U.S. Census, CHI studies in our fnal \\ncorpus have slightly more Non-Hispanic Whites and slightly fewer \\nHispanics. In addition, Figure 2 suggests that participants in neither \\nCHI studies nor FDA trials are representative of the aggregated U.S. \\ndemographics. However, we note that many CHI studies actively \\nrecruited a representative sample of their interest, which may or \\nmay not agree with the aggregated demographics of the U.S. For \\ninstance, Lopez et al. [74] was a Non-Hispanic-White-focused study, \\nand Dosono and Semaan [24] specifcally looked at the engagement \\nand dynamics of the Asian American and Pacifc Islander online \\ncommunities. As a result, both studies will not resemble the U.S. \\ndemographics by design, rather than by omission. \\nWe also looked at the longitudinal trend of compositions of \\nreported racial and ethnic groups across the six years of CHI pro-\\nceedings. Overall, the racial and ethnic compositions of study par-\\nticipants appear stable over the course of six years, with a more \\nnoticeable increase of Non-White participants from 2020 onwards. \\nFigure 3 displays the racial and ethnic composition over the six \\nyears, stratifed by the size of the study, where a study is classi-\\nfed as “large” if it has more than 28 (i.e., the median number of \\nparticipants across all studies) participants and “small-to-medium” \\notherwise. We see that large-scale studies tend to have more White \\nparticipants. This is partly due to the use of online platforms (e.g.,CHI ’23, April 23–28, 2023, Hamburg, Germany \\nTwitter or Mechanical Turk) for participant recruitment, which \\nhas been known to skew towards White samples [70, 117]. On the \\nother hand, small-to-medium studies are more likely to target spe-\\ncifc populations of interest (e.g., studying particular technology of \\ninterest in low-income neighbourhoods or among Black females; \\nsee, e.g., Ogbonnaya-Ogburu et al. [83] and Wheeler and Dillahunt \\n[120] ). As a result, small-to-medium studies might appear to have \\na larger proportion of non-White participants than larger studies. \\nFigure 2: Racial and ethnic compositions of participants (in \\nfve groups) from three diferent sources: CHI proceedings \\n(2016–2021, leftmost); demographics projection of the U.S. \\ncensus (2015–2019, middle); participants of U.S.-based FDA \\ndrug trials (2015–2019, rightmost). \\nFigure 3: Racial and ethnic compositions of study partici-\\npants from CHI proceedings between 2016 and 2021, by year \\nand study size (large, >28 participants; small-to-medium, \\n≤ 28 participants). \\n4.2 RQ2: What are some considerations that \\nspeak for and against collecting this information? \\nResponses from authors who participated in our open-ended sur-\\nvey are summarized in Table 1. Because the authors could cite \\nmultiple reasons in their open-ended responses, occurrences of the \\nsummarized categories in Table 1 may add up to over 15. \\nThe most common reason (“Why” in Table 1) for collecting and \\nreporting racial and ethnic information is external validity, that \\nis, the degree to which the conclusion in one study would hold for other persons in other places and times [ 36]. For instance, one \\nsurveyed researcher noted in their response that “If my data is \\nreally only from a sample of white people, then I need to acknowledge \\nthat as a limitation of the study and ensure that my analysis is \\ncontextualized in that particular identity.” A majority of the surveyed \\nresearchers also named prior work (including a priori hypothesis on \\nthe diferences across racial and ethnicity groups) as a driving factor. \\nAs an example, one respondent noted that “We were specifcally \\ninterested in experiences of intra-community marginalization, which \\nincludes systemic biases such as racism, and we wanted to ensure \\nthat our sample could capture such dynamics.” In addition, the \\ninterplay between race, racism, and socioeconomic class in the U.S. \\nmotivates some researchers to “always collect these data” because \\nwhen it comes to disparities as they afect technology, “race and \\nincome are so woefully correlated in this country [the U.S.] (and \\nothers) — it was important to collect racial/ethnic information.” Only \\ntwo out of 15 responses mentioned “external requirement” as the \\nprimary reason for collecting and reporting participants’ race and \\nethnicity. Out of the two respondents, one stated that “I was in \\na Biomedical and Health Informatics program, and health studies \\noften have people collect this data (perhaps tied to NIH funding/grant \\nrequirements) ”; the other one mentioned that “our multi-year federal \\ngrant that funded this research had annual reporting requirements \\nabout, among other details, the demographics of our participants in \\neach study ”. Both responses speak to the possibility of leveraging \\npractices and training from related disciplines, such as biomedical \\nsciences, to improve the collection and report standards of race in \\nHCI. \\nRegarding the method of collection (“How” in Table 1), all of \\nthe surveyed researchers administrated a questionnaire, but the \\ndetails of the administration varied: The same number of authors \\nused two separate questions (seven out of 15) versus one single \\nquestion (seven out of 15) when obtaining the race and ethnicity \\ninformation of their study participants. This is in contrary to the \\nobservation made from our systematic literature analysis, where \\nless than 20% of the studies used two separate questions. Several \\nresearchers also used a combination of categorized and open-ended \\nresponses, where the study participants can provide their own race \\nand ethnicity. \\n4.3 RQ3: What are relevant considerations on how to collect, report, and use this information? \\nAll responses mentioned existing resources that informed their \\ndata collection process (see the “Reference” column in Table 1). \\nAmong those responses, eight out of 15 cited the U.S. census as the \\nprimary reference for designing the categories in the questionnaires. \\nHowever, researchers are aware of the limitations of U.S. census \\nas a reference, “I found the categories somewhere — either NIH or \\ncensus perhaps but unfortunately not sure. I know that I tried to fnd \\nexisting categories that the government recommended as I thought \\nthese would be the absolute best practice/best way of going about this \\nand then later learned these may not tie best to how people identify \\nthemselves.” \\nAnother response mentioned categories informed by the social \\nsciences, partly due to their expertise in race and ethnicity researchWhy, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\nOccurrence \\nWhy \\nExternal validity 8 \\nTargeted studies 4 \\nMotivated by prior work 8 \\nExternal requirement 2 \\nMotivate future studies 4 \\nHow \\nSeparate questions for race and ethnicity 7 \\nOne question for race and ethnicity 5 \\nOpen-ended responses 4 \\nReference \\nU.S. Census 8 \\nSociology research 1 \\nA priori population of interest 3 \\nPilot study and prior work 4 \\nTable 1: A summary of surveyed authors’ responses. Note \\nthat multiple reasons and sources are allowed. Therefore, \\noccurrences in each individual category could add up to more \\nthan 15. The categories are determined through qualitative \\ncoding of the responses. \\n— “we reviewed a number of articles from top sociology journals in \\nthe years immediately prior to the data being collected (e.g., ASR \\n[American Sociological Review], AJS [American Journal of Sociology]), \\nas sociologists tend to think more about these issues, and also use \\nrepresentative survey data, than communication scholars.” In addition \\nto the categories informed by the U.S. census, a few studies also \\nidentifed a target population of interest via prior work, e.g., one \\nrespondent noted that “we looked at previous research in this area and \\nconsulted with our community partners who collect this information \\nas part of their data collection and capacity building activities.” \\n5 CONSIDERATIONS \\nIn the following, we discuss some considerations for collecting \\nrace and ethnicity data, synthesized from our multidisciplinary \\nliterature review and survey data. \\n5.1 Why: Whether to collect racial and ethnic \\ndata \\nAnswering the question of whether to collect racial and ethnic \\ndata of study participants requires substantial care. On the one \\nhand, researchers in HCI and many related scientifc disciplines are \\ntrained and required to justify the collection and planned analysis \\nof demographic variables such as race. Documented reasons for not \\ncollecting this data may be privacy concerns for the study partici-\\npants [6], or preventing survey disengagement and fatigue [38, 51]. \\nMoreover, some fear that collecting and reporting on the fndings \\nin diferent racial groups, when presented without a thorough and \\nnuanced discussion, could reify the racial inequality and stereotype, \\nand even delve into “scientifc racism” (i.e., the pseudoscientifc be-\\nlief that empirical evidence justifes racial discrimination) [4]. When \\nput together, these concerns result in recommendations such as “Re-\\nsearchers should not collect more information from participants than is needed to answer their research questions” by the Institutional Re-\\nview Board (IRB), an institutional organization that approves, mon-\\nitors, and reviews human-subject studies in U.S. academic research \\ninstitutions [ 10]. By contrast, demographics including gender, age, \\nand whether participants are in vulnerable groups (prisoners, mi-\\nnors) are routinely requested in human-subject studies applications \\nto IRB. In practice, researchers often need to cite established difer-\\nences in racial groups to justify their decisions. On the other hand, \\none cannot hope to fnd a documented “established diference” if no \\nstudy is devoted to documenting and understanding potential dif-\\nferences of the piece of technology across racial groups. This “what \\nwe do not measure, we do not understand” argument is especially \\nrelevant, given that race has been understudied, and the historic im-\\npact of race (and racism) in the U.S. and many countries have been \\nmade invisible. For example, in the U.S., the NIH started mandating \\nthe recruiting and reporting of racial and ethnic minorities in all \\nclinical trials after observing signifcant diferences between racial \\ngroups across a wide range of health care outcomes [34, 65, 78]. \\nWhile federal agencies in the U.S. (such as the NIH and the Census \\nBureau) have generally leaned towards collecting race and ethnicity \\nof participants, the legality and regulations of the collection and \\nanalysis of racial and ethnic data difer greatly around the world. \\nFor instance, in the European Union (EU), the General Data Protec-\\ntion Regulation (GDPR) [18] mandates that “processing of personal \\ndata revealing racial or ethnic origin, political opinions, religious \\nor philosophical beliefs ... shall be prohibited [in general].” Despite \\nsome listed exceptions, e.g., “archiving purposes in the public inter-\\nest, scientifc or historical research purposes or statistical purposes”, \\nthe legal and logistical challenges led to another important consid-\\neration for participants and researchers based in the EU (and other \\nregions with similar laws and regulations). \\nGiven the nuanced nature of this topic, we encourage HCI re-\\nsearchers to have an active discussion during the study design phase \\nwithin the team. Some points of discussion (as framed through quo-\\ntations from our survey respondents and fndings from our review) \\nmay include: \\n(1) Observed diferences in the outcome of interest across racial \\ngroups: \\n• e.g., Yardi and Bruckman [123] demonstrated that low-\\nincome African-American families share digital devices \\nmore often than their middle-class White-American coun-\\nterparts, motivating future work (e.g., Garg [35], Pina et al. \\n[89]) to consider the efects of race and socioeconomic \\nstatus on technology usage. \\n• e.g., One participant studied “the use of [ICTs] within and \\namong online communities engaging in identity work”. Since \\ntheir outcome of interest is interwoven with race and eth-\\nnicity, “collecting the racial and ethnic information of par-\\nticipants contextualizes the results within the perspective of \\nthe moderator’s background”. \\n(2) Potential consequences and implications of the study: \\n• e.g., Collecting and reporting racial and ethnic information \\ncould also be useful for “future studies that may want to \\ncompare their results with ours ”. While their study mainly \\nexamined the efect of gender, they still “wanted to enableCHI ’23, April 23–28, 2023, Hamburg, Germany \\nany future studies examining race to be able to compare \\ntheir context with ours.” \\n(3) Race and ethnicity of study participants is an important \\ndimension of the external validity of a study: \\n• e.g., Some researchers “always collect these data. As a per-\\nson trained as a quantitative researcher, this seems like basic, \\nkey demographic information that one should have on-hand \\nin case it is relevant to the RQs.” \\n• e.g., another respondent refected, “the 2016 paper had a \\nsample that was over 90% white, which is defnitely not \\ndiverse or representative of the trans population [their pop-\\nulation of interest]. ” Therefore, in their follow-up work in \\n2020, they made sure to “made sure to prioritize recruiting \\na diverse group on many dimensions, with a particular focus \\non race/ethnicity”. \\n(4) Concerns for privacy and race “determinism”: \\n• e.g., Widespread collection of individual race and ethnic-\\nity data may spark privacy concerns. The research team \\nshould communicate the intended use of the collected data, \\nand work with the community to understand whether the \\nbenefts outweigh the privacy and confdentiality con-\\ncerns. \\n• e.g., Presenting data and fndings with racial and ethnic \\nidentities may also risk perpetuating theories of racial \\nor genetic determinism (i.e., the belief that genetics or \\nphenotype exclusively account for human behaviour and \\nability). This is echoed by one of our survey respondents: \\n“I think it’s important to choose to include these variables \\n[race and gender variables] in the analyses very carefully. \\nI think race and gender variables especially are sometimes \\noverused because when diferences arise they can give way to \\npost-hoc explanations that reinforce stereotypes. That is, the \\nuse of these variables should be thoughtful and intentional. ” \\nOf course, given the nuanced nature of this question, we are not \\narguing to simply lower the barriers to collecting racial and ethnic \\ndata of participants for all HCI studies — as noted above, concerns \\nfor privacy and misinterpretation of the results should not be taken \\nlightly. Instead, we advocate that whether or not to collect racial \\nand ethnic data of participants should be a deliberate rather than a \\ndefault decision. \\n5.2 How to collect race and ethnicity data \\nOur fndings show that researchers who have committed to collect-\\ning race and ethnicity of their participants desire a standardized \\nmethod of collection and seek out templates and best practices. \\nBecause of our restriction to U.S.-based participants and research, \\nthe vast majority of researchers in our study used the U.S. census as \\ntheir references when designing their surveys and questionnaires \\n(see Table 1). A considerable subset of surveyed researchers also \\nused the modifed format to ask one combined question about race \\nand ethnicity, rather than two separate questions. While this may \\nbe an efective starting place, the U.S. census is only conducted ev-\\nery 10 years, and its categories often represent the political values \\nat the time rather than the best research practice [5, 69]. Therefore, \\nthere is a need for best practice that is research-driven and takes \\ninto account how participants actually want to be identifed by race. In this section, we ofer some specifc, practical advice on how to \\ncollect participants’ race and ethnicity using surveys. \\nAs an example, consider the collection of race and ethnicity in the \\nU.S. census — census forms now have two separate questions about \\nrace (i.e., what is this person’s race) and ethnicity (i.e., whether this \\nperson is of Hispanic, Latino, or Spanish origin). This separation \\nwas largely motivated by the growing diversity within the Hispanic-\\nAmerican population. However, in the 2010 Census, 37% of surveyed \\nHispanic or Latinx chose not to identify with any of the provided \\nrace categories [52], which reveals the gap between the provided \\ncategories in the status quo and how people identify themselves. \\nWhile there is certainly no one-size-fts-all solution to collecting \\nrace and ethnicity of study participants, we hope the following \\npointers could be of help to researchers during data collection: \\n(1) When the race and ethnicity information is collected via \\nmultiple choice questions, the research team should consider \\nallowing the option of identifying with more than one race and \\nethnicity, and including an open-ended option for participants \\nto self-describe their race and ethnicity [20, 98]. \\n(2) If researchers want to provide racial and ethnic categories in \\na questionnaire (e.g., in a large-scale online study), the U.S. \\ncensus is a good starting point for U.S.-based studies (e.g., \\nin the U.S. context, consider asking separate questions for \\nrace and ethnicity). However, depending on the nature of the \\nstudy, categories used in the census (e.g., Asian Americans \\nand Pacifc Islanders) do not necessarily capture the underly-\\ning diversity of the group, and more granular choices might \\nneed to be included to refect and communicate participants’ \\nidentities [48, 53, 62]. As one of our survey participants put \\nit, “highlighting the inner diversity of the sample also commu-\\nnicates that the AAPI [Asian Americans and Pacifc Islanders] \\numbrella should not be construed as a monolith (e.g., the so-\\ncioeconomic experiences of East Asians vary from Southeast \\nAsians).” \\n(3) When there are limited resources on racial and ethnic cat-\\negories of participants for a study, consider asking partici-\\npants to self-describe and then cluster the responses after-\\nwards, which could serve as a starting point for future studies \\nengaging with similar populations. In resource-limited set-\\ntings, researchers could employ similar strategies in a smaller \\npilot study with a subset of participants. If resources permit, \\nwe also encourage researchers to solicit feedback from par-\\nticipants on whether the proposed categories capture their \\nidentifed race and ethnicity. \\n(4) In addition to race and ethnicity, consider alternative data \\nthat captures a particular dimension of racial diferences \\nand could address the research questions of interest more \\ndirectly: languages spoken at home, household disposable \\nincome, access to quality healthcare (or the lack thereof), etc. \\nFor instance, Hanna et al. [42] and Roth [99] have outlined \\nsome of these particular dimensions or “proxies”. \\n5.3 From whom to collect race and ethnicity data \\nEven equipped with perfect survey questions, researchers are likely \\nto run into a series of additional practical challenges — Who shouldWhy, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\nbe recruited for the study? Should we always strive for racially-\\nrepresentative samples? How to recruit and retain racial and ethnic \\nminorities? While answers to these questions will be context- and \\nstudy-specifc, we briefy summarize some common motivations \\nand barriers for recruiting racially- and ethnically-diverse or ho-\\nmogeneous participants in this section. \\nRecall that many of our surveyed researchers cited the external \\nvalidity of their studies as a primary motivation for collecting par-\\nticipants’ race and ethnicity (see Table 1). Indeed, HCI researchers \\nroutinely draw inferences about populations by extrapolating fnd-\\nings based on data from small samples of people. Recent years have \\nseen a surge of work on how studies in HCI, along with psychology, \\nmight have relied too much on samples from Western, Educated, In-\\ndustrialized, Rich, and Democratic (WEIRD) samples [7, 45, 73, 90] \\nand among college students [41, 46, 90]. \\nTherefore, we encourage researchers to assess the external valid-\\nity, as it relates to participants’ race and ethnicity, of their specifc \\nstudies. For instance, Obiorah et al . [81] proposed and evaluated a \\nnovel interactive multi-person tabletop exhibit in museums, and \\nthe race and ethnicity of participants they engaged “is refective of \\ntypical museum visitors’ demographics in the U.S.”, which speaks \\nto the applicability of their fndings on the populations they are \\ninterested in. By contrast, Hamidi et al. [40] acknowledged the lack \\nof diversity in their samples as a limitation and stated that “Our \\nsample includes diverse age and gender perspectives but lacks diversity \\nof race or ethnic identity [with respect to the U.S. demographics].” \\nOf course, researchers do not always need to recruit participants \\nto match national (or regional) race and ethnicity composition — \\nexternal validity should be assessed with respect to the population \\nfrom whom we are drawing inferences. For instance, F. Maestre et al. \\n[30] recruited a majority of White participants, in keeping with the \\ndemographics of the rural region of the U.S. Midwest. Other notable \\nexamples include studies with a focus on selected racial and eth-\\nnic groups: Lee and Rich [68] recruited primarily Black American \\nparticipants because of the “substantial history and contemporary \\nissues with medical racism towards the Black community”. Simi-\\nlarly, Dosono and Semaan [24] were interested in the dynamics of \\nonline Asian American and Pacifc Islanders communities. In both \\ncases, a lack of racial diversity does not necessarily pose threat to \\nthe external validity of the results. \\nWe also want to acknowledge that relying on convenient (and, \\noften WEIRD and non-racially-diverse) samples has its practical \\nadvantages, because recruiting and retaining racial and ethnic mi-\\nnorities presents logistical, fnancial, and sometimes even legal bar-\\nriers [6, 15, 18, 85, 90]. Examining the efectiveness of sampling and \\nrecruitment strategies tailored to specifc ethnic minority groups \\n(e.g., F. Maestre et al. [30], Sadler et al. [100] ) in HCI is an important \\nthread of future work. In practice, the following questions summa-\\nrized and adapted from IRB forms at various universities might be \\nan efective starting point for researchers who want to refect on \\ntheir study design and sample collection [54–56, 97]: \\n(1) What is the estimated male-to-female ratio in the study? \\nDoes this refect the distribution of the local population? \\n(2) Is there any target population in terms of gender, race, eth-\\nnicity, sexual orientation, literacy level, health status, and \\neconomic class? (3) Is there any vulnerable population in your study who might \\nbe disproportionally afected by the research (e.g., indigenous \\npeople, minors, students)? \\n(4) Will your study participants be representative of the demo-\\ngraphics in the study region? Include an estimate of the \\npercentages that will be from minority groups, or the lack \\nthereof. \\n(5) In addition, identify any racial, ethnic, or gender groups that \\nwill be specifcally excluded from this research study. Con-\\nsider providing a compelling justifcation for such exclusion \\n(in addition to convenience samples). \\n(6) If the research involves the collection of sensitive, potentially \\nidentifable information (e.g., sexual orientation, race, gen-\\nder, age, and a combination of the aforementioned factors), \\nconsider describing what information will be obtained and \\nincluded in the fnal research output, and how permission \\nwill be sought. \\nWhile we focused on practical suggestions for collecting race in \\nsimplifed settings such as surveys in this section, there is an array \\nof important and exciting work dedicated to collecting complex, \\nqualitative racial data through interviews, focus groups, and case \\nstudies [29, 83, 84, 93, 94, 120]. We encourage researchers to explore \\nlisted papers and references therein to engage with qualitative racial \\ndata collection. \\n5.4 What to report on racial and ethnic data \\nHCI researchers might also fnd themselves analyzing race and eth-\\nnicity data of their participants, such as incorporating participants’ \\nrace in a regression model. While a well-conducted analysis will \\nenrich our knowledge of the technology under study, such analyses, \\nif done poorly, might give way to post hoc explanations that rein-\\nforce racial stereotypes and inequality [27, 32]. Below, we outline a \\nfew considerations for reporting and communicating the fndings \\nbased on the race and ethnicity of study participants. These consid-\\nerations are largely inspired and adapted from existing discussions \\nin medical sciences [33, 59, 64], fair machine learning [6, 8, 50], \\nHCI [29, 43, 93, 95, 102], and our own experience as researchers in \\nthe feld. \\nFirstly, we encourage researchers to acknowledge that race is \\na social construct and proxy, rather than an objective measure of \\nunderlying traits [8, 50, 59] when presenting observed diferences \\nin racial groups. In particular, we caution against languages that \\nsuggest the diferences in the measured outcomes can be attributed \\nto participants’ race, a slippery slope to furthering the racial dis-\\nparity and stereotypes. Instead, authors should provide qualifying \\nstatements when presenting their results across racial categories. \\nAs an example, in CHI 2021, K. Chua and Mazmanian [61] stated \\nthat “We recognize that the terms ‘Asian,’ ‘White,’ and ‘Hispanic’ \\nare broad and homogenize the experiences of people from various \\nraces, ethnicities, and national backgrounds. We chose to use these \\nterms because they are emic terms used by the vast majority of our \\nparticipants in describing themselves.” \\nSecond of all, we urge researchers to investigate the potential \\nbasis of the observed diferences across racial and ethnic groups to \\nthe extent feasible, e.g., conducting a follow-up study or drawing \\nfrom existing literature. For instance, when Dillahunt et al. [23]CHI ’23, April 23–28, 2023, Hamburg, Germany \\nfound that participants of diferent races had diferent levels of \\nengagement and outcomes with online employment resources, they \\ndrew from the existing literature on Black-White income inequal-\\nity [37, 118] and hypothesized that the observed diferences across \\nrace could be attributed to the design of online resources websites \\nwith the “Us versus Them” thinking that marginalized the racial \\nminorities. \\nThirdly, discussions about the potential impact of the study \\nshould be sought after. This efort could take many forms: for exam-\\nple, the research artifact could include a paragraph on the societal \\nimpact of the research. Such practice would be widely applicable to \\nresearch artifacts that propose new systems or software, paralleling \\neforts in the artifcial intelligence community [2]. As an example, \\nif a paper proposes a new piece of virtual reality technology, and \\nresearchers observe that the accuracies of the technology difer \\nacross racial groups, they should discuss the potential impact when \\nthe technology is deployed at a large scale. Similarly, for research \\noutputs that incorporate machine learning models as part of their \\nsystems, potential disparate impacts on minority groups, as well as \\na toolkit for mitigating such ramifcations in fair machine learning, \\ncould be discussed [6, 8]. \\nAdditionally, we want to call attention to the default practice \\nof treating racial groups as categorical variables with “White” as \\nthe reference group (e.g., by classifying participants as “White” \\nversus “Non-White”) in quantitative analyses in HCI. First of all, \\nwhile easy to the implement, the grouping approach treats race as \\na non-overlapping attribute and ignores the heterogeneity within \\neach racial and ethnic category [ 26, 42]. This could contribute to \\nfurther reifcation of the groups under study, especially when the \\ngroups exhibit large internal diferentiation such as the “Asian \\nAmericans and Pacifc Islanders” category in the U.S. census [26]. \\nAs an alternative, we encourage quantitative analysis to account \\nfor the “multi-dimensionality” of race and carefully examine which \\ndimension or representation is the most appropriate [42, 99]: for \\ninstance, researchers might adopt phenotypes such as skin type \\nin investigating computer vision applications Buolamwini and Ge-\\nbru [11]; or geography-based information such as those proposed \\nin Benthall and Haynes [8] when looking at access to public re-\\nsources. Furthermore, even when census-like racial categories are \\nsuitable, researchers should refect whether they are defaulting to \\na culturally-dominant group (e.g., male, White, high income) as the \\nreference could subtly establish and reify the notion that culturally-\\ndominant groups are the most “normal” and “interesting”, thereby \\ncreating a nested system of importance over time [27, 32, 58]. More-\\nover, using the culturally dominant group as the reference does \\nnot always lead to the most interpretable results — for instance, if \\nwe are really interested in how diferent racial and ethnic groups \\ncompare against the average in the study population, coding the dif-\\nferences against a specifc reference group is not the most informa-\\ntive approach (see more technical details and alternative modelling \\nstrategies in Dupree and Kraus [27] and references therein). \\nLast but not least, drawing from the unjust history of the U.S. cen-\\nsus, as well as the thread of excellent work exposing the oppressive \\nnature of current technology design in HCI [28, 43, 44, 94, 95, 103], \\nwe would like to emphasize that data collection and reporting is not \\nsufcient in itself — researchers should always (i) consider narrat-\\ning lived experiences of the racial and ethnic minorities; (ii) explore how oppression from race, gender, and class operates under the cur-\\nrent technology design; and (iii) actively resist the current design \\nand technology that perpetuates racism, whenever applicable. \\n5.5 Moving beyond the U.S. \\nIn this paper, we primarily engage with race and ethnicity in the U.S. \\ncontext. However, focusing on the North American experience does \\nnot fully embrace the issues of race and racism globally. Therefore, \\nin this section, we describe how our analysis and refections could \\nshed light on how HCI researchers can consider why, when, and \\nhow to collect data about race and ethnicity globally: \\n• Why and from whom: our considerations for why and from \\nto collect race and ethnicity in Sections 5.1 and 5.3 gen-\\neralize well to non-U.S. contexts. For instance, socioeco-\\nnomic and health disparities, and their subsequent impact \\non HCI research, remain persistent across diferent parts \\nof the world [31, 111, 119]. In addition, the external valid-\\nity of a study is increasingly important in a global context, \\nas the feld of HCI continues to expand beyond its focus \\non WEIRD samples and discovers more country-to-country, \\nregion-to-region diferences [72, 73]. \\n• How: as shown in Table 1, most U.S.-based researchers turn \\nto the U.S. census for categorizing their study participants’ \\nracial identities. While other countries and regions might not \\nhave such clear-cut categories, our recommendations in Sec-\\ntion 5.2 still point to a general path forward: researchers \\ncould start with existing resources such as the national \\nor region census [106] and scholarly work in related dis-\\nciplines [22, 111, 119, 124]; pilot studies could be especially \\nhelpful in a historically under-explored community in HCI. \\n• What: in addition to the considerations in Section 5.4, we \\nwant to highlight that collecting and reporting ethnic iden-\\ntities of participants around the world is merely a start to \\nmore equitable, global, and diverse HCI research. Especially \\nwith a sample of non-U.S. participants, researchers need to \\npay particular attention so that the research outputs and \\ninsights do not remain WEIRD-focused [77, 122]. \\n6 DISCUSSION \\nThe primary goal of our work is to understand the race and eth-\\nnicity data in HCI from the following aspects: (i) when are the HCI \\nstudy participants’ race and ethnicity collected (RQ1); (ii) why are \\nrace and ethnicity collected (RQ2); and (iii) how is the collection \\nadministrated, as well as curating a list of considerations and rec-\\nommendations for collecting and reporting race and ethnicity in \\nthe future (RQ3). Our analysis revealed that for studies published \\nin CHI between 2016 and 2021, less than 3% included detailed race \\nand ethnicity information about their study participants. Among \\nthose studies that are based in the United States, about 64% of total \\nparticipants identifed as Non-Hispanic White. By contrast, 9% and \\n10% identifed as Hispanic and Non-Hispanic Black, respectively. \\nRegarding “why”, our participants cited the themes and historical \\ncontexts of their research as the primary reason. Other motivating \\nfactors to collect this information are to increase a study’s exter-\\nnal validity, achieve a more representative sample, or allow future \\nresearch. Lastly, regarding how the racial data were collected, weWhy, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\nfound that the U.S. census, as well as identifed groups in prior \\nwork, were some of the most commonly cited sources. \\nMotivated by the literature review and results of our survey, \\nwe also compiled a list of considerations — corresponding to the \\nthree RQs — for when to include study participants’ race and eth-\\nnicity (RQ2), and if so, who to include (RQ1) and how to collect this \\ninformation (RQ3). Our considerations expanded on the pioneer-\\ning recommendations by Ogbonnaya-Ogburu et al. [83] and Race \\nin HCI Collective et al. [91], and are inspired by eforts in data \\ncollection and report of gender, which led to a crowd-sourced, \\npragmatic working document [102] serving to approach gender \\ninclusively in HCI research. In particular, we advocate for deliber-\\nate decisions when determining when and from whom to collect \\nrace and ethnicity information. Furthermore, while the U.S. census, \\nas well as its foreign counterpart, might serve as a good starting \\npoint for collecting this information, HCI researchers should stay \\nattuned to the complexity of race and ethnicity — more granular \\ncategories, obtained through related works or pilot studies, could \\nlead to historically-neglected insights and minority-empowering \\nresearch [106]. \\nWe also want to emphasize the intention of this paper: instead of \\na panacea for engaging with racial data, or the lack thereof, in HCI, \\nthis paper is meant to encourage more conversations on collecting \\nand reporting race and ethnicity among HCI researchers, practi-\\ntioners, and users including the participants in HCI research. Even \\nwithin the team of authors, we have divergent opinions on some \\nof the considerations, partly due to our diverse research themes, \\nmethodological focuses, and frst-hand experience with collecting \\nand reporting racial and ethnic information in HCI research. When \\nit comes to when to collect race and ethnicity from study partici-\\npants, one author who has extensive expertise in engaging with \\nrace in HCI immediately took a strong stance and argued that race \\nand ethnicity of study participants should always be collected for \\na complete contextualization and narrative of the study. However, \\nanother author on the team challenged this stance by citing their \\nown experience in collecting race and ethnicity for large-scale, \\nmulti-national online studies — a lack of community-driven and \\nresearch-driven guidance on collecting racial and ethnic data, espe-\\ncially in non-Western contexts, poses a substantial logistical and \\nmethodological challenge in their quantitative work. In addition, \\nthe team’s collective experiences with IRBs at diferent institutions \\nalso revealed a lack of standardized institutional guidance on col-\\nlecting this information. \\nWhile the team of authors cannot and, certainly, does not claim \\nto represent all research areas in HCI and identities experienced by \\nHCI researchers, we see the tension among us as an indicator of the \\npotential tensions in other research teams and the HCI community, \\nhighlighting that race and ethnicity in HCI is a nuanced and highly \\npersonal topic that needs to be handled with care. We believe that \\ntension and discussion on collecting and reporting racial and eth-\\nnic data should be welcomed: the tensions among researchers and \\npractitioners today could give rise to safe, inclusive, and equitable \\nresolutions for the HCI community tomorrow. 7 LIMITATIONS \\nOur work is subject to several limitations. One limitation is the \\nscope of the discussion of race and ethnicity: the paper and existing \\nwork surveyed herein are based on the racial and ethnic context \\nof the United States. In part, this is due to the vast collection of \\nexisting research on race and ethnicity in the U.S. However, given \\nthe high research output of U.S.-based HCI researchers [71], we \\nhope that our work will serve as a proof-of-concept piece for future \\nconversations about race and ethnicity in a global context. \\nIn terms of research methodology, our sampling could be subject \\nto selection bias — while CHI covers a broad range of research top-\\nics, published papers in CHI proceedings are a small subset of the \\nbroader HCI research outputs. For instance, we might expect confer-\\nence proceedings with a more focused theme (e.g., user experience \\nconferences such as NN/g) and conferences with a more explicit in-\\nternational, non-Western focus (e.g., CLIHC and Asian CHI) to have \\ndiferent approaches to collecting and reporting race and ethnicity \\nof study participants in HCI research. In addition, even within CHI \\nproceedings, limiting publications to 2016–2021 also potentially \\nconfounds our fndings with the longitudinal trend of research on \\nrace and ethnicity in HCI. For instance, more recent research out-\\nputs might have more discourse on race and ethnicity [47], and \\nthe year 2021 consists of the largest amount of papers in our fnal \\ncorpus. Therefore, if we repeat our methods on the CHI proceedings \\nfrom 2022, there is likely going to be a substantial amount of papers \\nwe would include. Our approach to corpus curation can also lead \\nto an undercount: there is an array of excellent work in CHI on \\nrace and ethnicity in HCI without explicitly recruiting participants \\nand collecting their race and ethnicity. These papers are likely to \\nbe omitted in our curation process. Furthermore, as the primary \\nintention of this work was to start a discussion on race and eth-\\nnicity data collection, we did not prioritize an exhaustive, iterative \\nrefning of our corpus. Therefore, putting all these factors together, \\nwe postulate that the reported results on the fnal corpus of papers \\nare likely an underestimate of the CHI publications that collected \\nand reported participants’ race and ethnic information. \\nRegarding the survey results, the 15 researchers who provided \\nprompt responses to our inquiries might not be a representative \\nsample of HCI researchers. For instance, 47% of the researchers who \\nparticipated in our survey used separate questions to collect race \\nand ethnicity, as opposed to less than 20% of the researchers in the \\nentire fnal corpus. In addition, the collection and report of racial \\nand ethnic data without a strong justifcation may be discouraged \\nby regulatory agencies. Therefore, the information curated from \\npublished studies does not necessarily represent the initial study \\ndesign or intentions of the researchers, but is likely a combination of \\nthe research agendas and constraints imposed by external resources \\nand regulations. As one surveyed researcher noted in their response, \\n“ We ran into challenges and limitations in mapping the participant \\nresponses to our grant reporting requirements. Therefore, we ended up \\ncollecting the racial/ethnic information based on US Census Bureau \\ncategories.” \\n8 CONCLUSION AND FUTURE WORK \\nAs HCI continues to engage with a racially- and ethnically-diverse \\npopulation of users, understanding the current practice of collectingCHI ’23, April 23–28, 2023, Hamburg, Germany \\nthe race and ethnicity of participants in HCI research takes on high \\nimportance. Through a systematic review of published CHI papers \\nand follow-up surveys with selected authors, we found that less \\nthan 3% of CHI papers from 2016 to 2021 collected and reported \\ntheir participants’ race and ethnicity. Among those authors who \\ndid collect this information, the primary motivations include (i) \\nstrengthening the external validity of the study, and (ii) addressing \\nthe established disparities in the uptake and use of technologies \\nbetween diferent racial groups. Most surveyed authors mentioned \\nthe U.S. census as their reference for designing the questionnaires \\nfor collecting participants’ race and ethnicity. \\nOur fndings reveal several important directions of future work. \\nFirstly, CHI is a global community and reporting on the ethnicity \\nof participants outside of the U.S. has been steadily increasing (for \\ninstance, David Bowman et al. [21] , Koushki et al. [66] , Randhawa \\net al. [92]). Extending our studies to a more global context will cham-\\npion the call for inclusiveness and representation of non-Western \\nsamples in the HCI research community. In addition, even in the U.S. \\ncontext, the nuance of the racial groups is not necessarily captured \\nby the established categories used in the U.S. census. For instance, \\nalthough Middle Eastern and North African Americans are classi-\\nfed as White in the U.S. census, a sizable subset of the community \\nbelieves that they are not treated or perceived as White, and that \\nsuch classifcation might even perpetuate further harm [75, 76, 101]. \\nMoreover, depending on the nature of the study, categories used in \\nthe U.S. census do not necessarily capture the underlying diversity \\nwithin the group, and researchers have called for more granular \\ncategories might need to be included to refect and communicate \\nparticipants’ identities [48, 53, 62]. \\nAnother avenue of future research is to investigate the challenges \\nencountered in decisions around race and ethnicity data collection \\nand analysis, especially among the researchers who decided not to \\ncollect and report such data. For instance, a systematic summary \\nof the primary barriers (e.g., privacy and legal concerns, lack of \\nsystematic categories for large-scale international studies) could \\ninform future eforts on providing resources and designing tools to \\novercome these barriers. \\nIn this paper, we also outlined a few recommendations which \\nserve to further the conversations on whether this data should be \\ncollected, and in what circumstances. Critically, we are not propos-\\ning that every HCI study should simply collect race and ethnicity \\nof its participants. Rather, we want to highlight the importance \\nof a deeper and broader consideration of racial and ethnic data \\ncollection and analysis in HCI, and certainly within the research \\nteam — as long as racial and ethnic categories continue to govern \\nsocial, political, and cultural interactions, collecting and analyzing \\nracial and ethnic data fts squarely within the agenda of HCI. \\nREFERENCES \\n[1] Julio Abascal and Colette Nicolle. 2005. Moving towards inclusive design guide-\\nlines for socially and ethically aware HCI. Interacting with Computers 17, 5 (Sept. \\n2005), 484–505. \\n[2] Grace Abuhamad and Claudel Rheault. 2020. Like a Researcher Stating Broader \\nImpact For the Very First Time. arXiv (Nov. 2020). arXiv:2011.13032 [cs.CY] \\n[3] American Psychological Association. 2019. Publication Manual of the Amer-\\nican Psychological Association: 7th Edition, 2020 Copyright (7 ed.). American \\nPsychological Association. \\n[4] American Sociological Association. 2017. The Importance of Collecting Data and \\nDoing Social Science Research on Race. https://www.asanet.org/importance-collecting-data-and-doing-social-science-research-race. Accessed: 2021-8-5. \\n[5] Margo Anderson and Stephen E Fienberg. 2000. Race and ethnicity and the \\ncontroversy over the US Census. Current Sociology 48, 3 (2000), 87–110. \\n[6] Mckane Andrus, Elena Spitzer, Jefrey Brown, and Alice Xiang. 2021. What \\nWe Can’t Measure, We Can’t Understand: Challenges to Demographic Data \\nProcurement in the Pursuit of Fairness. In Proceedings of the 2021 ACM Confer-\\nence on Fairness, Accountability, and Transparency (FAccT ’21). Association for \\nComputing Machinery, New York, NY, USA, 249–260. \\n[7] Jefrey J Arnett. 2008. The neglected 95%: why American psychology needs to \\nbecome less American. American Psychologist 63, 7 (Oct. 2008), 602–614. \\n[8] Sebastian Benthall and Bruce D Haynes. 2019. Racial categories in machine \\nlearning. In Proceedings of the Conference on Fairness, Accountability, and Trans-\\nparency (Atlanta, GA, USA) (FAT* ’19). Association for Computing Machinery, \\nNew York, NY, USA, 289–298. \\n[9] Jack M Bloom. 2019. Class, Race, and the Civil Rights Movement, Second Edition. \\nIndiana University Press. \\n[10] Institutional Review Board. 2020. Racial Equity Considerations and the Institu-\\ntional Review Board -Child Trends. https://www.childtrends.org/publications/ \\nracial-equity-considerations-and-the-institutional-review-board. Accessed: \\n2021-8-27. \\n[11] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accu-\\nracy disparities in commercial gender classifcation. In Conference on fairness, \\naccountability and transparency. PMLR, 77–91. \\n[12] Center for Drug Evaluation and Research. 2019. Drug Trials Snapshots. Accessed: \\n2021-3-8. \\n[13] Robert Chang and Lorraine Bannai. 2019. Brief of Norman Y. Mineta, the \\nSakamoto Sisters, the Council on American-Islamic Relations (National and \\nNew York, Inc.), and the Fred T. Korematsu Center for Law and Equality as \\nAmici Curiae in Support of Respondents. (2019). \\n[14] Thandeka K Chapman. 2013. You can’t erase race! Using CRT to explain the \\npresence of race and racism in majority white suburban schools. Discourse: \\nStudies in the Cultural Politics of Education 34, 4 (Oct. 2013), 611–627. \\n[15] Meghan Coakley, Emmanuel Olutayo Fadiran, L Jo Parrish, Rachel A Grifth, \\nEleanor Weiss, and Christine Carter. 2012. Dialogues on diversifying clinical \\ntrials: successful strategies for engaging women and minorities in clinical trials. \\nJournal of Women’s Health 21, 7 (July 2012), 713–716. \\n[16] Derrick L Cogburn. 2003. HCI in the so-called developing world: what’s in it \\nfor everyone. Interactions 10, 2 (March 2003), 80–87. \\n[17] Patricia Hill Collins and Sirma Bilge. 2020. Intersectionality. John Wiley & Sons. \\n[18] European Commission. 2016. General Data Protection Regulation (GDPR) – \\nOfcial Legal Text. Accessed: 2021-9-8. \\n[19] David I Conway, Alex D McMahon, Denise Brown, and Alastair H Leyland. 2021. \\nMeasuring socioeconomic status and inequalities. In Reducing social inequalities \\nin cancer: evidence and priorities for research. \\n[20] Paul R Croll and Joseph Gerteis. 2019. Race as an Open Field: Exploring Identity \\nbeyond Fixed Choices. Sociology of Race and Ethnicity 5, 1 (Jan. 2019), 55–69. \\n[21] Nicholas David Bowman, Jihhsuan Tammy Lin, and Chieh Wu. 2021. A Chinese-\\nLanguage Validation of the Video Game Demand Scale (VGDS-C): Measuring \\nthe Cognitive, Emotional, Physical, and Social Demands of Video Games. In \\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems. \\nAssociation for Computing Machinery, New York, NY, USA, 1–10. \\n[22] Francis M Deng. 1997. Ethnicity: An African Predicament. The Brookings review \\n15, 3 (1997), 28–31. \\n[23] Tawanna R Dillahunt, Aarti Israni, Alex Jiahong Lu, Mingzhi Cai, and Joey \\nChiao-Yin Hsiao. 2021. Examining the Use of Online Platforms for Employment: \\nA Survey of U.S. Job Seekers. In Proceedings of the 2021 CHI Conference on \\nHuman Factors in Computing Systems. Association for Computing Machinery, \\nNew York, NY, USA, 1–23. \\n[24] Bryan Dosono and Bryan Semaan. 2019. Moderation Practices as Emotional \\nLabor in Sustaining Online Communities: The Case of AAPI Identity Work on \\nReddit. In Proceedings of the 2019 CHI Conference on Human Factors in Computing \\nSystems. Association for Computing Machinery, New York, NY, USA, 1–13. \\n[25] Susan M Dray, David A Siegel, and Paula Kotzé. 2003. Indra’s Net: HCI in the \\ndeveloping world. Interactions 10, 2 (March 2003), 28–37. \\n[26] Lucas G Drouhot and Filiz Garip. 2021. What’s behind a racial category? Uncov-\\nering heterogeneity among Asian Americans through a data-driven typology. \\nRSF: The Russell Sage Foundation Journal of the Social Sciences 7, 2 (2021), 22–45. \\n[27] Cydney H Dupree and Michael W Kraus. 2022. Psychological Science Is Not \\nRace Neutral. Perspectives on Psychological Science 17, 1 (Jan. 2022), 270–275. \\n[28] Sheena Erete, Aarti Israni, and Tawanna Dillahunt. 2018. An intersectional \\napproach to designing in the margins. Interactions 25, 3 (April 2018), 66–69. \\n[29] Sheena Erete, Yolanda A Rankin, and Jakita O Thomas. 2021. I Can’t Breathe: \\nRefections from Black Women in CSCW and HCI. Proc. ACM Hum.-Comput. \\nInteract. 4, CSCW3 (Jan. 2021), 1–23. \\n[30] Juan F. Maestre, Tawanna Dillahunt, Alec Andrew Theisz, Megan Furness, \\nVaishnav Kameswaran, Tifany Veinot, and Patrick C. Shih. 2021. Examining \\nMobility Among People Living with HIV in Rural Areas. In Proceedings of the \\n2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan)Why, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\n(CHI ’21, Article 201). Association for Computing Machinery, New York, NY, \\nUSA, 1–17. \\n[31] René Flores and Edward Telles. 2012. Social stratifcation in Mexico: Disen-\\ntangling color, ethnicity, and class. American sociological review 77, 3 (2012), \\n486–494. \\n[32] Marion Fourcade and Kieran Healy. 2017. Categories All the Way Down. His-\\ntorische Sozialforschung 42 (2017), 286–296. \\n[33] Tracy Frey and Roxanne K. Young. 2020. Race/Ethnicity. AMA Manual of \\nStyle. https://www.amamanualofstyle.com/view/10.1093/jama/9780190246556. \\n001.0001/med-9780190246556-chapter-11-div2-23. Accessed: 2021-8-5. \\n[34] Georita M Frierson, David M Williams, Shira Dunsiger, Beth A Lewis, Jessica A \\nWhiteley, Anna E Albrecht, John M Jakicic, Santina M Horowitz, and Bess H \\nMarcus. 2008. Recruitment of a racially and ethnically diverse sample into a \\nphysical activity efcacy trial. Clinical Trials 5, 5 (2008), 504–516. \\n[35] Radhika Garg. 2021. Understanding Tensions and Resilient Practices that Emerge \\nfrom Technology Use in Asian India Families in the U.S.: The Case of COVID-19. \\nProc. ACM Hum.-Comput. Interact. 5, CSCW2 (Oct. 2021), 1–33. \\n[36] Darren Gergle and Desney S Tan. 2014. Experimental Research in HCI. In Ways \\nof Knowing in HCI, Judith S Olson and Wendy A Kellogg (Eds.). Springer New \\nYork, New York, NY, 191–227. \\n[37] Jonathan Gordils, Nicolas Sommet, Andrew J Elliot, and Jeremy P Jamieson. 2020. \\nRacial Income Inequality, Perceptions of Competition, and Negative Interracial \\nOutcomes. Social Psychological and Personality Science 11, 1 (Jan. 2020), 74–87. \\n[38] Robert M Groves, Eleanor Singer, and Amy Corning. 2000. Leverage-Saliency \\nTheory of Survey Participation: Description and an Illustration. Public Opinion \\nQuarterly 64, 3 (2000), 299–308. \\n[39] Maureen T Hallinan. 2001. Sociological Perspectives on Black-White Inequalities \\nin American Schooling. Sociology of Education 74 (2001), 50–70. \\n[40] Foad Hamidi, Lydia Stamato, Lisa Scheifele, Rian Ciela Visscher Hammond, and \\nS Nisa Asgarali-Hofman. 2021. “Turning the Invisible Visible”: Transdisciplinary \\nBioart Explorations in Human-DNA Interaction. In Proceedings of the 2021 CHI \\nConference on Human Factors in Computing Systems. Association for Computing \\nMachinery, New York, NY, USA, 1–15. \\n[41] Paul H P Hanel and Katia C Vione. 2016. Do Student Samples Provide an Accurate \\nEstimate of the General Public? PLOS One 11, 12 (Dec. 2016), e0168354. \\n[42] Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020. To-\\nwards a critical race methodology in algorithmic fairness. In Proceedings of the \\n2020 conference on fairness, accountability, and transparency. 501–512. \\n[43] Christina Harrington and Tawanna R Dillahunt. 2021. Eliciting Tech Futures \\nAmong Black Young Adults: A Case Study of Remote Speculative Co-Design. In \\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems \\n(Yokohama, Japan) (CHI ’21, Article 397). Association for Computing Machinery, \\nNew York, NY, USA, 1–15. \\n[44] Christina N Harrington, Shamika Klassen, and Yolanda A Rankin. 2022. “All that \\nYou Touch, You Change”: Expanding the Canon of Speculative Design Towards \\nBlack Futuring. In Proceedings of the 2022 CHI Conference on Human Factors in \\nComputing Systems (New Orleans, LA, USA) (CHI ’22, Article 450). Association \\nfor Computing Machinery, New York, NY, USA, 1–10. \\n[45] Joseph Henrich, Steven J Heine, and Ara Norenzayan. 2010. The weirdest people \\nin the world? Behavioral and Brain Sciences 33, 2-3 (June 2010), 61–83; discussion \\n83–135. \\n[46] P J Henry. 2008. Student Sampling as a Theoretical Problem. Psychological \\nInquiry 19, 2 (2008), 114–126. \\n[47] Julia Himmelsbach, Stephanie Schwarz, Cornelia Gerdenitsch, Beatrix Wais-\\nZechmann, Jan Bobeth, and Manfred Tscheligi. 2019. Do We Care About Diver-\\nsity in Human Computer Interaction: A Comprehensive Content Analysis on \\nDiversity Dimensions in Research. In Proceedings of the 2019 CHI Conference on \\nHuman Factors in Computing Systems (Glasgow, Scotland Uk) (CHI ’19, Paper \\n490). Association for Computing Machinery, New York, NY, USA, 1–16. \\n[48] Ariel T Holland and Latha P Palaniappan. 2012. Problems with the collection \\nand interpretation of Asian-American health data: omission, aggregation, and \\nextrapolation. Annals of Epidemiology 22, 6 (June 2012), 397–405. \\n[49] Catherine Hu, Christopher Perdriau, Christopher Mendez, Caroline Gao, Abrar \\nFallatah, and Margaret Burnett. 2021. Toward a Socioeconomic-Aware HCI: \\nFive Facets. (Aug. 2021). arXiv:2108.13477 [cs.HC] \\n[50] Lily Hu and Issa Kohler-Hausmann. 2020. What’s sex got to do with machine \\nlearning?. In Proceedings of the 2020 Conference on Fairness, Accountability, \\nand Transparency (Barcelona, Spain) (FAT* ’20). Association for Computing \\nMachinery, New York, NY, USA, 513. \\n[51] Jennifer L Hughes, Abigail A Camden, and Tenzin Yangchen. 2016. Rethinking \\nand updating demographic questions: Guidance to improve descriptions of \\nresearch samples. Psi Chi Journal of Psychological Research 21, 3 (2016), 138– \\n151. \\n[52] Karen R Humes, Nicholas A Jones, Roberto R Ramirez, and Others. 2011. \\nOverview of race and Hispanic origin: 2010. (2011). \\n[53] Institute of Medicine (US) Subcommittee on Standardized Collection of \\nRace/Ethnicity Data for Healthcare Quality Improvement. 2014. Race, Eth-\\nnicity, and Language Data: Standardization for Health Care Quality Improvement. National Academies Press (US), Washington (DC). \\n[54] Northwestern University Institutional Review Board Ofce. [n.d.]. Protocol Tem-\\nplates and Forms. https://irb.northwestern.edu/resources-guidance/protocol-\\ntemplates-forms/index.html. Accessed: 2022-8-31. \\n[55] University of Michigan Institutional Review Board Ofce. [n.d.]. IRB Applica-\\ntion Process. https://research-compliance.umich.edu/irb-application-process. \\nAccessed: 2022-8-31. \\n[56] Institutional Review Board Ofce, Carnegie Mellon University. [n.d.]. Guid-\\nance & Forms - Ofce of Research Integrity and Compliance - Carnegie Mel-\\nlon University. https://www.cmu.edu/research-compliance/human-subjects-\\nresearch/guidance-forms.html. Accessed: 2022-8-13. \\n[57] Abigail Z Jacobs and Hanna Wallach. 2019. Measurement and Fairness. arXiv \\n(Dec. 2019). arXiv:1912.05511 [cs.CY] \\n[58] Sasha Shen Johfre and Jeremy Freese. 2021. Reconsidering the Reference Cate-\\ngory. Sociological Methodology 51, 2 (Aug. 2021), 253–269. \\n[59] C P Jones. 2001. Invited commentary: “race,” racism, and the practice of epidemi-\\nology. American Journal of Epidemiology 154, 4 (Aug. 2001), 299–304; discussion \\n305–6. \\n[60] Camara Phyllis Jones, Benedict I Truman, Laurie D Elam-Evans, Camille A Jones, \\nClara Y Jones, Ruth Jiles, Susan F Rumisha, and Geraldine S Perry. 2008. Using \\n“socially assigned race” to probe white advantages in health status. Ethnicity & \\nDisease 18, 4 (2008), 496–504. \\n[61] Phoebe K. Chua and Melissa Mazmanian. 2021. What Are You Doing With \\nYour Phone? How Social Class Frames Parent-Teen Tensions around Teens’ \\nSmartphone Use. In Proceedings of the 2021 CHI Conference on Human Factors \\nin Computing Systems. Association for Computing Machinery, New York, NY, \\nUSA, 1–12. \\n[62] Bliss Kaneshiro, Olga Geling, Kapuaola Gellert, and Lynnae Millar. 2011. The \\nchallenges of collecting data on race and ethnicity in a diverse, multiethnic state. \\nHawaii Medical Journal 70, 8 (Aug. 2011), 168–171. \\n[63] J S Kaufman. 1999. How inconsistencies in racial classifcation demystify the \\nrace construct in public health statistics. Epidemiology 10, 2 (March 1999), \\n101–103. \\n[64] J S Kaufman and R S Cooper. 2001. Commentary: considerations for use of \\nracial/ethnic classifcation in etiologic research. American Journal of Epidemiol-\\nogy 154, 4 (Aug. 2001), 291–298. \\n[65] Lindsey Konkel. 2015. Racial and Ethnic Disparities in Research Studies: The \\nChallenge of Creating More Diverse Cohorts. Environmental Health Perspectives \\n123, 12 (Dec. 2015), A297–302. \\n[66] Masoud Mehrabi Koushki, Borke Obada-Obieh, Jun Ho Huh, and Konstantin \\nBeznosov. 2021. On Smartphone Users’ Difculty with Understanding Implicit \\nAuthentication. In Proceedings of the 2021 CHI Conference on Human Factors \\nin Computing Systems. Association for Computing Machinery, New York, NY, \\nUSA, 1–14. \\n[67] Nancy Krieger, Jaquelyn L Jahn, and Pamela D Waterman. 2017. Jim Crow \\nand estrogen-receptor-negative breast cancer: US-born black and white non-\\nHispanic women, 1992-2012. Cancer Causes & Control 28, 1 (Jan. 2017), 49–59. \\n[68] Min Kyung Lee and Katherine Rich. 2021. Who Is Included in Human Perceptions \\nof AI?: Trust and Perceived Fairness around Healthcare AI and Cultural Mistrust. \\nIn Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\\n(Yokohama, Japan) (CHI ’21, Article 138). Association for Computing Machinery, \\nNew York, NY, USA, 1–14. \\n[69] Sharon M Lee. 1993. Racial classifcations in the US census: 1890–1990. Ethnic \\nand racial studies 16, 1 (Jan. 1993), 75–94. \\n[70] Kevin E Levay, Jeremy Freese, and James N Druckman. 2016. The Demographic \\nand Political Composition of Mechanical Turk Samples. SAGE Open 6, 1 (Jan. \\n2016), 2158244016636433. \\n[71] ACM Digital Library. [n.d.]. \\n[72] Sebastian Linxen, Vincent Cassau, and Christian Sturm. 2021. Culture and HCI: \\nA still slowly growing feld of research. Findings from a systematic, comparative \\nmapping review. In Proceedings of the XXI International Conference on Human \\nComputer Interaction (Málaga, Spain) (Interacción ’21, Article 25). Association \\nfor Computing Machinery, New York, NY, USA, 1–5. \\n[73] Sebastian Linxen, Christian Sturm, Florian Brühlmann, Vincent Cassau, Klaus \\nOpwis, and Katharina Reinecke. 2021. How WEIRD is CHI?. In Proceedings of \\nthe 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, \\nJapan) (CHI ’21, Article 143). Association for Computing Machinery, New York, \\nNY, USA, 1–14. \\n[74] Sarah Lopez, Yi Yang, Kevin Beltran, Soo Jung Kim, Jennifer Cruz Hernandez, \\nChelsy Simran, Bingkun Yang, and Beste F Yuksel. 2019. Investigating Implicit \\nGender Bias and Embodiment of White Males in Virtual Reality with Full Body \\nVisuomotor Synchrony. In Proceedings of the 2019 CHI Conference on Human \\nFactors in Computing Systems. Association for Computing Machinery, New York, \\nNY, USA, 1–12. \\n[75] Neda Maghbouleh, Ariela Schachter, and René D Flores. 2022. Middle Eastern \\nand North African Americans may not be perceived, nor perceive themselves, \\nto be White. PNAS 119, 7 (Feb. 2022).CHI ’23, April 23–28, 2023, Hamburg, Germany \\n[76] Patrick L Mason and Andrew Matella. 2014. Stigmatization and racial selection \\nafter September 11, 2001: self-identity among Arab and Islamic Americans. IZA \\nJournal of Migration 3, 1 (Oct. 2014), 1–21. \\n[77] Omar Mubin, Fady Alnajjar, and Mudassar Arsalan. 2022. HCI Research in the \\nMiddle East and North Africa: A Bibliometric and Socioeconomic Overview. \\nInternational Journal of Human–Computer Interaction 38, 16 (Oct. 2022), 1546– \\n1562. \\n[78] Vivek H Murthy, Harlan M Krumholz, and Cary P Gross. 2004. Participation in \\ncancer clinical trials: race-, sex-, and age-based disparities. JAMA 291, 22 (June \\n2004), 2720–2726. \\n[79] National Institutes of Health. 2001. NOT-OD-01-053: NIH POLICY ON REPORT -\\nING RACE AND ETHNICITY DATA: SUBJECTS IN CLINICAL RESEARCH. \\nhttps://grants.nih.gov/grants/guide/notice-fles/NOT-OD-01-053.html. Ac-\\ncessed: 2021-8-5. \\n[80] National Science Foundation. 2017. Technical Notes. https://www.nsf.gov/ \\nstatistics/2017/nsf17310/technical-notes.cfm. Accessed: 2021-8-5. \\n[81] Mmachi God’sglory Obiorah, James K L Hammerman, Becky Rother, Will \\nGranger, Haley Margaret West, Michael Horn, and Laura Trouille. 2021. \\nU!Scientist: Designing for People-Powered Research in Museums. In Proceedings \\nof the 2021 CHI Conference on Human Factors in Computing Systems. Association \\nfor Computing Machinery, New York, NY, USA, 1–14. \\n[82] Anna Ofenwanger, Alan John Milligan, Minsuk Chang, Julia Bullard, and Dong-\\nwook Yoon. 2021. Diagnosing Bias in the Gender Representation of HCI Research \\nParticipants: How it Happens and Where We Are. In Proceedings of the 2021 CHI \\nConference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ’21, \\nArticle 399). Association for Computing Machinery, New York, NY, USA, 1–18. \\n[83] Ihudiya Finda Ogbonnaya-Ogburu, Angela D R Smith, Alexandra To, and Ken-\\ntaro Toyama. 2020. Critical Race Theory for HCI. In Proceedings of the 2020 CHI \\nConference on Human Factors in Computing Systems. Association for Computing \\nMachinery, New York, NY, USA, 1–16. \\n[84] Ihudiya Finda Ogbonnaya-Ogburu, Kentaro Toyama, and Tawanna R Dillahunt. \\n2019. Towards an Efective Digital Literacy Intervention to Assist Returning Citi-\\nzens with Job Search. In Proceedings of the 2019 CHI Conference on Human Factors \\nin Computing Systems (Glasgow, Scotland Uk) (CHI ’19, Paper 85). Association \\nfor Computing Machinery, New York, NY, USA, 1–12. \\n[85] Sam S Oh, Joshua Galanter, Neeta Thakur, Maria Pino-Yanes, Nicolas E Barcelo, \\nMarquitta J White, Danielle M de Bruin, Ruth M Greenblatt, Kirsten Bibbins-\\nDomingo, Alan H B Wu, Luisa N Borrell, Chris Gunter, Neil R Powe, and Este-\\nban G Burchard. 2015. Diversity in Clinical and Biomedical Research: A Promise \\nYet to Be Fulflled. PLOS Medicine 12, 12 (Dec. 2015), e1001918. \\n[86] Newton G Osborne and Marvin D Feit. 1992. The Use of Race in Medical \\nResearch. JAMA 267, 2 (Jan. 1992), 275–279. \\n[87] Cale J Passmore, Max V Birk, and Regan L Mandryk. 2018. The Privilege of \\nImmersion: Racial and Ethnic Experiences, Perceptions, and Beliefs in Digital \\nGaming. In Proceedings of the 2018 CHI Conference on Human Factors in Com-\\nputing Systems (Montreal QC, Canada) (CHI ’18, Paper 383). Association for \\nComputing Machinery, New York, NY, USA, 1–19. \\n[88] Pew Research Center. 2015. Multiracial in America: Proud, Diverse and Growing \\nin Numbers. https://www.pewsocialtrends.org/wp-content/uploads/sites/3/ \\n2015/06/2015-06-11_multiracial-in-america_fnal-updated.pdf. Accessed: 2021-\\n6-25. \\n[89] Laura R Pina, Carmen Gonzalez, Carolina Nieto, Wendy Roldan, Edgar Onofre, \\nand Jason C Yip. 2018. How Latino Children in the U.S. Engage in Collaborative \\nOnline Information Problem Solving with their Families. Proc. ACM Hum.-\\nComput. Interact. 2, CSCW (Nov. 2018), 1–26. \\n[90] Thomas V Pollet and Tamsin K Saxton. 2019. How Diverse Are the Samples Used \\nin the Journals ‘Evolution & Human Behavior’ and ‘Evolutionary Psychology’? \\nEvolutionary Psychological Science 5, 3 (Sept. 2019), 357–368. \\n[91] Race in HCI Collective, Angela D R Smith, Adriana Alvarado Garcia, Ian Arawjo, \\nAudrey Bennett, Khalia Braswell, Bryan Dosono, Ron Eglash, Denae Ford, Daniel \\nGardner, Shamika Goddard, Jaye Nias, Cale Passmore, Yolanda Rankin, Naba \\nRizvi, Carol F Scott, Jakita Thomas, Alexandra To, Ihudiya Finda Ogbonnaya-\\nOgburu, and Marisol Wong-Villacres. 2021. Keepin’ it real about race in HCI. \\nInteractions 28, 5 (Aug. 2021), 28–33. \\n[92] Shan M Randhawa, Tallal Ahmad, Jay Chen, and Agha Ali Raza. 2021. Karamad: \\nA Voice-based Crowdsourcing Platform for Underserved Populations. In Pro-\\nceedings of the 2021 CHI Conference on Human Factors in Computing Systems. \\nAssociation for Computing Machinery, New York, NY, USA, 1–15. \\n[93] Yolanda A Rankin and Na-Eun Han. 2019. Exploring the Plurality of Black \\nWomen’s Gameplay Experiences. In Proceedings of the 2019 CHI Conference on \\nHuman Factors in Computing Systems (Glasgow, Scotland Uk) (CHI ’19, Paper \\n139). Association for Computing Machinery, New York, NY, USA, 1–12. \\n[94] Yolanda A Rankin and Kallayah K Henderson. 2021. Resisting Racism in Tech \\nDesign: Centering the Experiences of Black Youth. Proc. ACM Hum.-Comput. \\nInteract. 5, CSCW1 (April 2021), 1–32. \\n[95] Yolanda A Rankin and Jakita O Thomas. 2019. Straighten up and fy right: \\nrethinking intersectionality in HCI research. Interactions 26, 6 (Oct. 2019), \\n64–68. [96] REDACTED. 2022. Collecting and Reporting Race and Ethnicity Data in HCI. In \\nExtended Abstracts of the 2022 CHI Conference on Human Factors in Computing \\nSystems (New Orleans, LA, USA) (CHI EA ’22, Article 327). Association for \\nComputing Machinery, New York, NY, USA, 1–8. \\n[97] University of Toronto Research Ethics Boards. [n.d.]. Research Ethics \\nBoards. https://research.utoronto.ca/ethics-human-research/research-ethics-\\nboards. Accessed: 2022-8-31. \\n[98] Zarine L Rocha and Peter J Aspinall. 2020. Introduction: Measuring Mixedness \\nAround the World. In The Palgrave International Handbook of Mixed Racial and \\nEthnic Classifcation. Springer International Publishing, 1–25. \\n[99] Wendy D Roth. 2016. The multiple dimensions of race. Ethnic and Racial Studies \\n39, 8 (2016), 1310–1338. \\n[100] Georgia Robins Sadler, Hau-Chen Lee, Rod Seung-Hwan Lim, and Judith Fuller-\\nton. 2010. Recruitment of hard-to-reach population subgroups via adaptations \\nof the snowball sampling strategy. Nursing & Health Sciences 12, 3 (Sept. 2010), \\n369–374. \\n[101] Helen Hatab Samhan. 2001. Who Are Arab Americans? \\n[102] Morgan Klaus Scheuerman, Katta Spiel, Oliver L. Haimson, Foad Hamidi, and \\nStacy M. Branham. 2021. HCI Gender Guidelines. https://www.morgan-klaus. \\ncom/gender-guidelines.html. Accessed: 2021-8-6. \\n[103] Dean Schillinger and Urmimala Sarkar. 2009. Numbers don’t lie, but do they \\ntell the whole story? Diabetes Care 32, 9 (Sept. 2009), 1746–1747. \\n[104] Ari Schlesinger, W Keith Edwards, and Rebecca E Grinter. 2017. Intersectional \\nHCI: Engaging Identity through Gender, Race, and Class. In Proceedings of the \\n2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, \\nUSA) (CHI ’17). Association for Computing Machinery, New York, NY, USA, \\n5412–5427. \\n[105] Jonathan Schwabish and Alice Feng. 2020. Applying Racial Equity Awareness \\nin Data Visualization. (Aug. 2020). \\n[106] Patrick Simon, Victor Piche, and Amelie A Gagnon (Eds.). 2015. Social statistics \\nand ethnic diversity: Cross-national perspectives in classifcations and identity \\npolitics (1 ed.). Springer International Publishing, Cham, Switzerland. \\n[107] Angela D R Smith, Alex A Ahmed, Adriana Alvarado Garcia, Bryan Dosono, \\nIhudiya Ogbonnaya-Ogburu, Yolanda Rankin, Alexandra To, and Kentaro \\nToyama. 2020. What’s Race Got To Do With It? Engaging in Race in HCI. \\nIn Extended Abstracts of the 2020 CHI Conference on Human Factors in Com-\\nputing Systems (Honolulu, HI, USA) (CHI EA ’20). Association for Computing \\nMachinery, New York, NY, USA, 1–8. \\n[108] Katta Spiel, Oliver L Haimson, and Danielle Lottridge. 2019. How to do better \\nwith gender on surveys: a guide for HCI researchers. Interactions 26, 4 (June \\n2019), 62–65. \\n[109] Simone Stumpf, Anicia Peters, Shaowen Bardzell, Margaret Burnett, Daniela \\nBusse, Jessica Cauchard, and Elizabeth Churchill. 2020. Gender-Inclusive HCI \\nResearch and Design: A Conceptual Review. Now Foundations and Trends. \\n[110] Christian Sturm, Alice Oh, Sebastian Linxen, Jose Abdelnour Nocera, Susan \\nDray, and Katharina Reinecke. 2015. How WEIRD is HCI? Extending HCI \\nPrinciples to other Countries and Cultures. In Proceedings of the 33rd Annual \\nACM Conference Extended Abstracts on Human Factors in Computing Systems \\n(Seoul, Republic of Korea) (CHI EA ’15). Association for Computing Machinery, \\nNew York, NY, USA, 2425–2428. \\n[111] Edward Telles. 2014. Pigmentocracies: Ethnicity, race, and color in Latin America. \\nUNC Press Books. \\n[112] Kentaro Toyama. 2010. Human–Computer Interaction and Global Development. \\nFoundations and Trends® in Human–Computer Interaction 4, 1 (2010), 1–79. \\n[113] US Census Bureau. [n.d.]. 2020 Census Frequently Asked Questions About Race \\nand Ethnicity. https://www.census.gov/programs-surveys/decennial-census/ \\ndecade/2020/planning-management/release/faqs-race-ethnicity.html. Accessed: \\n2022-8-5. \\n[114] US Census Bureau. 2019. National Demographic Analysis Tables: 2020. Accessed: \\n2021-3-8. \\n[115] Tavish Vaidya, Daniel Votipka, Michelle L Mazurek, and Micah Sherr. 2019. \\nDoes Being Verifed Make You More Credible? Account Verifcation’s Efect on \\nTweet Credibility. In Proceedings of the 2019 CHI Conference on Human Factors \\nin Computing Systems. Association for Computing Machinery, New York, NY, \\nUSA, 1–13. \\n[116] Judy van Biljon and Karen Renaud. 2019. Human-Computer Interaction for \\nDevelopment (HCI4D): The Southern African Landscape. In Information and \\nCommunication Technologies for Development. Strengthening Southern-Driven \\nCooperation as a Catalyst for ICT4D. Springer International Publishing, 253–266. \\n[117] Kelly Walters, Dimitri A Christakis, and Davene R Wright. 2018. Are Mechanical \\nTurk worker samples representative of health status and health behaviors in \\nthe U.S.? PLOS One 13, 6 (June 2018), e0198835. \\n[118] Connie Wanberg, Gokce Basbug, Edwin A J Van Hooft, and Archana Samtani. \\n2012. Navigating the black hole: Explicating layers of job search context and \\nadaptational responses. Personnel Psychology 65, 4 (Dec. 2012), 887–926. \\n[119] Michael Weiner. 2022. Routledge Handbook of Race and Ethnicity in Asia. Rout-\\nledge.Why, when, and from whom: considerations for collecting and reporting race and ethnicity data in HCI CHI ’23, April 23–28, 2023, Hamburg, Germany \\n[120] Earnest Wheeler and Tawanna R Dillahunt. 2018. Navigating the Job Search as a pluriverse. Interactions 28, 2 (March 2021), 56–63. \\nas a Low-Resourced Job Seeker. In Proceedings of the 2018 CHI Conference on [123] Sarita Yardi and Amy Bruckman. 2012. Income, race, and class: exploring \\nHuman Factors in Computing Systems (Montreal QC, Canada) (CHI ’18, Paper socioeconomic diferences in family technology use. In Proceedings of the SIGCHI \\n48). Association for Computing Machinery, New York, NY, USA, 1–10. Conference on Human Factors in Computing Systems (Austin, Texas, USA) (CHI\\n[121] Kim M Williams. 2006. Mark one or more. University of Michigan Press. ’12). Association for Computing Machinery, New York, NY, USA, 3041–3050. \\n[122] Marisol Wong-Villacres, Adriana Alvarado Garcia, Karla Badillo-Urquiola, [124] Henri-Michel Yéré, Mavis Machirori, and Jantina De Vries. 2022. Unpacking \\nMayra Donaji Barrera Machuca, Marianela Ciolf Felice, Laura S Gaytán-Lugo, race and ethnicity in African genomics research. Nature reviews. Genetics 23, 8 \\nOscar A Lemus, Pedro Reynolds-Cuéllar, and Monica Perusquía-Hernández. (Aug. 2022), 455–456. \\n2021. Lessons from Latin America: embracing horizontality to reconstruct HCI',\n",
       "  ['empirical studies in hci', 'race and ethnicity']),\n",
       " ('Replication and Expansion Study on Factors Influencing Student\\nPerformance in CS2\\nMargaret Ellis\\nVirginia Tech\\nBlacksburg, VA, USA\\nmaellis1@vt.eduSara Hooshangi\\nVirginia Tech\\nBlacksburg, VA, USA\\nshoosh@vt.edu\\nABSTRACT\\nWhile many studies have focused on students’ performance in CS1\\ncourses, research related to the performance and persistence of\\nstudents in CS2 classes is not as widely performed. In this work, we\\nwill extend our previous work to examine students’ performance\\nin CS2. We examined a data set that spanned over seven years\\non more than 5300 student records. In addition to typical factors\\nstudied by others (i.e. gender, race, CS1 performance), our work\\nalso took into account the relationship between various CS1 path-\\nways to CS2, student major, and the number of previous college\\nCS courses (including transfer credits) and student performance in\\nCS2. CS1 grade is a good indicator of performance in CS2. Gender\\nwas not a significant factor in determining performance in CS2 and\\nundeclared engineering majors stood out as high performers. CS\\nmajors passed the course at higher rates than other majors. Our\\nlarge data set allowed for more granular analysis according to race\\nand ethnicity and additional access to students’ underserved status.\\nRace and ethnicity had a significant correlation with performance,\\nand so did the underserved status. Our large data set confirmed\\nsome of the findings of our previous work, while providing some\\nnew insight.\\nCCS CONCEPTS\\n•Social and professional topics →Student assessment .\\nKEYWORDS\\nCS2, Student Performance, Data Structures, Prior CS Knowledge,\\nDiversity\\nACM Reference Format:\\nMargaret Ellis and Sara Hooshangi. 2023. Replication and Expansion Study\\non Factors Influencing Student Performance in CS2. In Proceedings of the\\n54th ACM Technical Symposium on Computer Science Education V. 1 (SIGCSE\\n2023), March 15–18, 2023, Toronto, ON, Canada. ACM, New York, NY, USA,\\n7 pages. https://doi.org/10.1145/3545945.3569867\\n1 INTRODUCTION\\nCS Educators are interested in understanding factors that contribute\\nto students’ success in computer science courses. The hope is that\\nby further understanding student performance and persistence in\\ncomputers science courses, the development of curriculum, instruc-\\ntion, and supports can be improved to increase overall performance\\nThis work is licensed under a Creative Commons Attribution-\\nNoDerivs International 4.0 License.\\nSIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\\n©2023 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-9431-4/23/03.\\nhttps://doi.org/10.1145/3545945.3569867and broaden participation in the field. There is a high demand for\\ncomputing professionals and a computer science education can\\nopen doors for many students. Both CS1 and CS2 are gateway\\ncourses that are critical in attracting and preparing students for\\ndegrees in computer science.\\nMany previous studies focus on the factors that impact success\\nin CS1 such as psychometrics, external factors, and previous expe-\\nriences [ 10]. Fewer studies have focused on student performance\\nin CS2. The most commonly studied predictor of success in CS2\\nis performance in CS1. Previous research has also examined prior\\nmath experience, demographics, self-efficacy, and other course re-\\nlated analysis [2, 5, 8, 14, 25, 26]. In our previous CS2 analysis, we\\nanalyzed persistence and performance of 610 students in a single\\nsemester. We introduced two new factors that had not been studied\\nin the past. These factors are 1) pathways to CS2 (AP CS, institution\\nCS1, transfer credits, non-standard pathways) and 2) the number\\nof CS courses that a student has taken before arriving at CS2 [15].\\nWith our larger data set of 5,300 CS2 students over the span of\\nseven years, we revisit analysis of factors such as CS1 performance,\\ngender, and major with this larger data set. We also investigate\\nrace and ethnicity with increased granularity. Additionally, our\\nlarger data set provides indicators for whether students are under-\\nserved, so we consider that as an additional factor related to CS2\\nperformance.\\n2 PREVIOUS RESEARCH\\nA plethora of studies have examined the factors that influence\\nsuccess in introductory CS courses with the ultimate goal of de-\\ntermining interventions to improve student success early in the\\nCS pipeline [ 14,25]. In previous studies, various factors have been\\ninvestigated and several metrics are commonly used to indicate\\nstudent success, the most common being course grade. The most\\ncommonly studied predictors are engagement and performance\\nin previous courses. Demographic and psychometric factors such\\nas self-regulation and self-efficacy have also been studied [ 14,25].\\nBeck et al. recently attempted to determine specific CS1 exam ques-\\ntions that can assess students’ readiness for success in CS2 [2].\\nBergin et al. determined that both mathematics preparation in\\nhigh school and self-regulation correlated positively with success\\nin CS1 [ 3,4,24]. Alavardo et al. have shown that students with AP\\ncredits have higher grades across many CS courses [ 1]. Catanese et\\nal. have shown that transfer students perform well in higher level\\ncomputer science courses as non-transfer students [7].\\nCS1 course performance has been repeatedly show to predict\\nCS2 course performance [ 5,8,19]. Our previous study on a single\\nsemester of data from this course verified these findings and also in-\\ndicated that gender and race were not significantly correlated with\\n896SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada Margaret Ellis & Sara Hooshangi\\nwhether students passed, did not pass, or dropped the course. How-\\never, undeclared engineering majors stood out as high performers\\nand students’ CS pathway leading to CS2 was also significant. Stu-\\ndents with CS1 transfer credits had significantly lower pass rates,\\nwhile students with only one previous CS course credit were less\\nlikely to drop or not pass the course [15].\\nOur larger data set provides the opportunity to analyze success\\nrates for more specific races. The National Center for Education\\nStatistics data indicates that both Black and Hispanic students are\\nless likely to finish a bachelor’s degree within four years relative\\nto their White and Asian peers [ 11]. George et al. paints a more\\ncomplex picture of persistence specific to computing and based\\non student interest according to race. Two years after taking an\\nintroductory computing course, the Asian/Asian American group\\nwas the only race/ethnicity variable that was statistically significant\\nin predicting interest in a computing career [13].\\nOur larger data set provides information about whether or not\\na student is considered underserved. Students who are identified\\nas underserved at our university include students who are first\\ngeneration college students, receive Pell Grant funding, or identify\\nas veterans. These three populations have unique characteristics but\\nall experience an academic performance gap [ 6,13,16,17,30,36].\\nA review of a CS2 course containing 86 students at Humbloldt State\\nUniversity reported a variation in course success rates for specific\\nsub-populations. The first generation students had a 70.6% pass\\nrate, while the not first generation students had a 82.1% success\\nrate. Students in that course receiving financial aide only had a\\n69.8% success rate, compared to the 84.8% success rate for students\\nwho received no financial aide [21].\\nMany studies have demonstrated student success is affected\\nby multiple factors including students’ previous experience, self-\\nefficacy and sense-of-belonging. Prior computing experience such\\nas internships, co-curricular activities, or high school program-\\nming courses can significantly impact a student’s experience in a\\ncomputer science course [23]. In Rountree’s study, expectancy for\\nsuccess was the strongest single indicator of success in a CS1 course,\\nmore so than demographics and background [ 27]. Krauss-Levy et\\nal. found that a lower sense of belonging was negatively correlated\\nwith course performance and pass rates [18].\\nStudents from minoritized groups tend to experience lower self\\nefficacy and sense of belonging. For example, Krauss-Levy et al’s\\nfound that women, first generations students and transfer students\\nhad a lower sense of belonging [ 18]. In a study of over 100 institu-\\ntions, black and female students were more likely to have communal\\ngoal orientation and to have a weaker sense of belonging with com-\\nputing [ 20]. Also, controlling for race and gender, Nguyen and\\nLewis found that competitive enrollment was a negative predictor\\nof sense of belonging and self-efficacy for students without prior\\nexperience [22].\\nOther research has focused on examining the WDF rate and\\nimproving that ratio post-intervention [ 31]. WDF is the percentage\\nof students receiving a grade of “D” or “F”, or withdrawing from the\\ncourse before the course ends. Further research into the factors that\\ninfluence WDF can shed light on effective intervention methods\\nthat could impact students who are in the danger of exiting the CS\\npipeline.3 RESEARCH QUESTIONS\\nThis project aims to examine different factors that are correlated\\nwith student performance and retention in CS2. Our research ques-\\ntions are the following as a replication study with a larger data\\nset:\\n•RQ1: Are demographic factors such as gender, race, student’s\\nmajor, and underserved status correlated with performance\\n(passing or failing) CS2?\\n•RQ2: Is CS1 grade correlated with CS2 performance?\\n•RQ3: Is CS1 pathway to CS2 (standard CS1 at our institution\\nor transfer credit for CS1) correlated with performance in\\nCS2?\\n4 INSTITUTIONAL BACKGROUND\\nOur institution is a large research university located in United\\nStates with more than 30,000 undergraduate students and a rapidly\\ngrowing CS department. Computer Science is one of 13 departments\\nwithin the College of Engineering (COE). Students are admitted to\\nthe College of Engineering and are considered General Engineering\\nmajors (referred to in this paper as undeclared engineering) in their\\nfirst year. They can declare a major within the college after their\\nfirst year. All students in computer science major or minor are\\nrequired to take our standard CS2 course which is taught in Java.\\n4.1 CS2 Pathways\\nThe software and programming sequence of courses for our CS\\nmajors and minors is CS1 (Introduction to Software Design), CS2\\n(Introduction to Data Structures and Software Design), and CS3\\n(Data Structures and Algorithms) all taught in Java with no previous\\nexperience required in CS1. However, many students enter CS2 by\\nsatisfying the prerequisite with a course other than the standard\\nCS1 course offered to CS majors at our institution.\\nStudents may have earned credit for CS1 during high school\\nby passing the APCS A exam, taking a dual enrollment computer\\nscience course, or a CS course at an International Baccalaureate (IB)\\nHigh School. Some students may earn CS1 transfer credits directly\\nfrom another university or a community college. Students in other\\nmajors at our institution can take an introductory course in Java or\\na sequence of Python courses before enrolling in CS2.\\n4.2 Dataset\\nThis study focuses on the examination of students who took CS2\\nbetween Fall 2015 and Spring 2022. The course was heavily updated\\nin the Fall of 2015 and underwent subtle improvements over the\\nyears, with a reorganization in Spring 2020 to address needs to\\ndeliver the course online and to incorporate an increased number of\\nsmaller coding practice opportunities. The objectives of the course,\\nprojects, and tests remained very similar over this seven year period.\\nData was acquired from the University Registrar’s office upon\\nIRB approval. We only focused on students who attempted and\\nreceived a grade for CS2 between Fall 2015 and Spring 2022. Table 1\\nshows the gender, race/ethnicity, and underserved status distribu-\\ntion of students who completed the course. A total of 5316 students\\nwere included in our data set. Gender and race distributions are\\nsimilar to the composition of the CS undergraduate population in\\nour department. For race, we have used similar categories found\\n897Replication and Expansion Study on Factors Influencing Student Performance in CS2 SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\\nin other Computing Research Association (CRA) publications, but\\nour data does not separate non-resident students from domestic\\nstudents. We used the National Science Foundation [ 12] definition\\nof underrepresented minorities in the STEM field, where Hispanic\\nof any race is categorized as Hispanic. Other/mixed category in-\\ncludes racial and ethnic groups combined (American Indians or\\nAlaska Natives, Native Hawaiians or Other Pacific Islanders, and\\nindividuals who report more than one race and are not Hispanic).\\nStudents without a disclosed race are marked as Not Reported in our\\ntables and figures. At our institution, non-international students\\nare reported as underserved based on any one of three factors, 1)\\nneither parent completed a bachelor’s degree, commonly referred\\nto as first-generation students, or 2) qualify for a Pell Grant based\\non the Free Application for Federal Student Aide (FAFSA), or 3)\\nself-identify as veterans and receive GI benefit while enrolled [ 29].\\nTable 1: Gender, race/ethnicity and underserved status\\nTotal\\nGender Male 4250 (80.0%) 5316 (100%)\\nFemale 1058 (19.9%)\\nUnknown 8 (0.1%)\\nRace/ Ethnicity White 2447 (46.0%) 5316 (100%)\\nAsian 1924 (36.2%)\\nHispanic 314 (5.9%)\\nOther/Mixed 273 (5.1%)\\nBlack 206 (3.9%)\\nNot Reported 152 (2.9%)\\nUnderserved No 4298 (80.9 %) 5316 (100%)\\nYes 1012 (19.0%)\\nUnknown 6 (0.1%)\\nFigure 1 shows the trend in our CS2 enrollment over the seven\\nyears of analyzed data. We see a steady increase in the number of\\nstudents who enroll in CS2 each year, consistent with the growth\\nthat has been observed in the number of undergraduate CS students\\nin our department. Each vertical bar represents total enrollment for\\nfall, spring, and summer in an academic year. For 21-22 academic\\nyear, the data for summer enrollment was not available at the time\\nof this analysis. This contributes to the lower number of students\\nshown for that academic year. The percentage of female students\\naverages around 20% over the seven years, with a slight increase\\nover the past three years, from 17% in 2019-2022 academic year to\\n23% in 2021-2022 academic year.\\nFor this data set we also had indicators for whether a student\\nwas underserved as defined earlier. Overall, 19% of students were\\nunderserved across the data set. The percentage of students who\\nhave an underserved status has fluctuated over the years, with its\\npeak at 23% in 2016-2017 academic year, dropping to 16% in 2020-\\n2021, and then slowly increasing to 19% at the end of 2021-2022\\nacademic year. Figure 2 shows the trend in our CS2 enrollment by\\nunderserved status over the seven years of data.\\n5 RESULTS\\nFor our 7-year CS2 data set, we analyzed the following factors: gen-\\nder, race, student major, CS1 pathway, number of CS courses taken\\nFigure 1: CS2 enrollment over the years by gender\\nFigure 2: CS2 enrollment by underserved status\\nbefore CS2, underserved status, and CS1 grade. We investigated\\nthe relationship between these factors and CS2 grades. We also\\nanalyzed the correlation of these factors with whether students\\npassed the course or received a WDF. For this course, students must\\nearn a C or better to proceed to the next course, so C- students\\nwere categorized in the WDF group.\\n5.1 RQ1: Are demographic factors such as\\ngender, race, student’s major, and\\nunderserved status correlated with\\nperformance (passing or failing) CS2?\\nTo examine demographic factors (gender, race, major, and under-\\nserved status) that may be correlated with student performance\\nin CS2, we looked at each of these factors separately and exam-\\nined the performance of students belonging to each group within a\\ngiven factor. A few students had received atypical grades such as\\nincomplete, repeat or no grade. To simplify our analysis, we had\\nremoved these few cases from our data set before any analysis.\\n898SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada Margaret Ellis & Sara Hooshangi\\nTable 2 shows passing rate based on binary gender. The number\\nof female students in our CS2 class is consistent with the overall\\nrate of female students in our department. The result of this table\\nsuggests that male and female students are performing similarly in\\nthe course, and passing at the same rate.\\nTable 2: Pass/fail rate based on gender\\nGender Pass WDF Total\\nMale 3968 (93.4%) 282 (6.6%) 4250 (100.0%)\\nFemale 999 (94.4%) 59 (5.6%) 1058 (100.0%)\\nUnknown 8(100%) 0 (0.0%) 8 (100.0%)\\nTotal 4975 (93.4%) 341 (6.4%) 5316 (100.0%)\\nWe further analyzed the relationship between gender and per-\\nformance by running a Chi-squared analysis based on two possible\\nCS2 outcomes; passing or not passing (WDF). Our analysis reveals\\nthat for gender ( 𝜒2=2.13,𝑝=.34), there is no significant differ-\\nence between CS2 outcome. This is consistent with the result of\\nour previous study, where we saw no correlation between gender\\nand performance [15].\\nWe also looked at race and ethnicity as a factor that may be\\ncorrelated with performance in the class. Figure 3 shows the grade\\ndistribution for various demographic groups as box-plots. Table 3\\nshows passing rate based on race/ethnicity. The initial analysis\\nof these results indicates that Hispanic and Black students are\\npassing at a slightly lower rate than Asian and White students. To\\nfurther examine this observation, we ran a Chi-squared analysis\\nthat indicated a significant difference based on these race/ethnicity\\ncategories (𝜒2=17.65,𝑝=.003).\\nFigure 3: CS2 performance based on race/ethnicity\\nWe further analyzed our result by performing pairwise Chi-\\nsquare analysis between Black/White, Black/Asian, Hispanic/White\\nand Hispanic/Asian students. The pairwise comparison showed\\na significant difference between Black students’ rate of passing\\nwhen compared to White students ( 𝜒2=6.97,𝑝=.008) and Asian\\nstudents (𝜒2=6.65,𝑝=.009). Similarly Hispanic students’ passing\\nrate when compared to White students ( 𝜒2=8.35,𝑝=.003) and\\nAsian students ( 𝜒2=7.85,𝑝=.005) were significant. No significantdifference was observed between Asian and White students, as\\nexpected from the passing rates shown Table 3. These results, which\\nwere possible to obtain with the larger data set, demonstrate that\\nfurther research and interventions are needed to support Black and\\nHispanic students.\\nTable 3: Pass/fail rate based on race/ethnicity\\nRace Pass WDF Total\\nWhite 2305 (94.2%) 142 ( 5.8%) 2447 (100%)\\nAsian 1812 (94.2%) 112 ( 5.8%) 1924 (100.0%)\\nHispanic 282 (89.8%) 32 (10.2%) 314 (100.0%)\\nOther/Mixed 252 (92.3%) 21 ( 7.7%) 273 (100.0%)\\nBlack 184 (89.3%) 22 (10.7%) 206 (100.0%)\\nNot Reported 140 (92.1%) 12 ( 7.9%) 152 (100.0%)\\nTotal 4975 (93.4%) 341 (6.4%) 5316 (100.0%)\\nWe also looked at how students’ major was correlated with per-\\nformance. Figure 4 shows CS2 grade distribution for different ma-\\njors. In this analysis, we separated the majors into four categories:\\n\"CS\" for students majoring in computer science; \"Other STEM\",\\nfor students majoring in a STEM field other than CS’ \"Undeclared\\nEngineering\", for freshmen students in the College of Engineering\\nwho are yet to declare a major, and finally \"Other\" to account for\\nstudents who are not in any STEM fields. The graph indicates that\\nundeclared engineering students have the highest average grade\\nfollowed by CS majors, which is the same order determined in the\\nSpring 2021 data set [15].\\nFigure 4: CS2 performance based on major\\nTable 4 shows passing rate based on major. The CS students\\nare passing at higher rates (98.3%) than other majors, followed by\\nundeclared engineering students. The students coming from non-\\nSTEM fields have the highest rate of failure. To further examine\\nthis result, we ran a Chi-squared analysis which indicated that a\\nsignificant difference is observed based on major ( 𝜒2=150.7,𝑝<\\n.001). We further analyzed our data by performing pairwise Chi-\\nsquare analysis between all major pairs and in all cases the p-\\nvalue was less than 0.001. While the performance of undeclared\\nengineering majors is higher than CS students, overall CS students\\n899Replication and Expansion Study on Factors Influencing Student Performance in CS2 SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\\nare passing at a higher rate than others. The students from non-\\nSTEM majors had the highest rate of failure relative to any other\\ndemographic group analyzed (14%) and more interventions should\\nbe considered to increase their success in CS2.\\nTable 4: Pass/fail rate based on student’s major\\nMajor Pass WDF Total\\nCS 1270 (98.3%) 22 ( 1.7%) 1292 (100.0%)\\nUndeclared Eng 1706 (95.8%) 75 ( 4.2%) 1781 (100.0%)\\nOther STEM 1458 (90.3%) 156 ( 9.7%) 1614 (100.0%)\\nOther 541 (86.0%) 88 (14.0%) 629 (100.0%)\\nFinally, we looked at the relationship between underserved sta-\\ntus and performance in the CS2 class. As shown in Table 1, 19% of\\nstudents are considered underserved. The average grade for these\\nstudents is similar to other groups, and we see no difference in the\\ndistribution of their CS2 grades. However Table 5 shows that the\\npassing rate is slightly less for those in the underserved category.\\nWe further analyzed the relationship between underserved status\\nand performance by running a Chi-squared analysis. Our analy-\\nsis reveals that for the underserved status, there is a significant\\ndifference between CS2 passing outcome ( 𝑐ℎ𝑖2=11.23,𝑝<.001).\\nThese results, which were possible to obtain with the underserved\\nindicator in this data set, demonstrate that further research and\\ninterventions are needed to support students who are considered\\nunderserved.\\nTable 5: Pass/fail rate based based on underserved status\\nUnderserved Pass WDF Total\\nN 4046 (94.1%) 252 (5.9%) 4298 (100.0%)\\nY 923 (91.2%) 89 (8.8%) 1012 (100.0%)\\nUnknown 6 (100.0%) 0(0.0%) 6 (100.0%)\\n5.2 RQ2: Is CS1 grade correlated with CS2\\nperformance?\\nFor our data set, CS1 grade reasonably predicts CS2 grade. Figure 5\\nshows the relationship between CS1 grade and CS2 grade. Students\\nneed a grade of C or better in CS1 before they can complete CS2,\\nhence the grade distribution for CS1 only includes the range of C\\nto A grades. As shown in Figure 5, the grades of students who took\\nCS1 at our institution are correlated with their CS2 grades. This\\ncorrelation is statistically significant with a p< .001 and a correlation\\ncoefficient of 0.42. The mean grade for CS1 is 3.06 and 3.14 for CS2.\\nAs shown in the box-plot, there is more variation in performance\\nfor students who received a lower CS1 grade, but overall CS1 grade\\nis a significant factor in CS2 grade. These findings align well with\\nmany previous studies that have correlated CS1 and CS2 grade as\\ndiscussed in Section 2.\\nFigure 5: CS2 performance based on CS1 grades\\n5.3 RQ3: Is CS1 pathway to CS2 (standard CS1 at\\nour institution or transfer credit for CS1)\\ncorrelated with performance in CS2?\\nStudents reach our CS2 classes from a variety of different academic\\npathways. Based on information provided in our data set, we created\\nthree categories to indicate student pathway to CS2 as follows:\\n\"CS1\" for our traditional CS1 path; \"CC Transfer\" for community\\ncollege transfer credit; \"Other Transfer\" for transfer credit from all\\nother sources. \"Other Transfer\" encompasses AP CS, IB, four year\\ninstitution transfers, and other pathways at our institution, but\\nthis granularity was not available at the time of analysis. Table 6\\nsummarizes students’ path to CS2 for these groups. About two-\\nthirds of our students take our CS1 course, and the rest transfer the\\ncredit into our program.\\nWe also looked into the number of prior CS college-level courses\\nthat a student had credit for before enrolling in CS2. In the 7-year\\ndata set, 90% of students came to CS2 from a single college course,\\nwhich in all likelihood is CS1 or an equivalent transfer credit. Mean-\\nwhile, 7% came with 2 previous courses, and only 3% had taken 3\\nor more courses before enrolling in CS2. There was less variation\\nin the number of previous CS courses the students had taken in\\nthe larger historical data set than in the recent single semester\\ndata set where 31% of the students had credit for 2 or 3 previous\\ncollege-level CS courses.\\nTable 6: Pass/fail rate based on CS1 pathway\\nCS1 Path Pass WDF Total\\nCS1 2863 (93.6%) 195 (6.4%) 3058 (100.0%)\\nCC* Transfer 192 (94.1%) 12 (5.9%) 204 (100.0%)\\nOther Transfer 1920 (93.5%) 134 (6.5%) 2054 (100.0%)\\nTotal 4975 (93.6%) 341 (6.4%) 5316 (100.0%)\\n*CC: Community College\\nWe further analyzed the relationship between groups in each\\nfactor by running a Chi-squared analysis on each factor. Our Chi-\\nsquared analysis reveals that for neither CS1 pathway ( 𝜒2=0.44,𝑝=\\n.93), nor number of prior CS courses ( 𝜒2=0.02,𝑝=1.0), was sig-\\nnificantly correlated with CS2 grade. This is in contrast to what we\\n900SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada Margaret Ellis & Sara Hooshangi\\nobserved in our previous study on a single recent semester [ 15],\\nwhere we saw a significance difference. We partially attribute this\\nto the fact that there is now more variation in students’ CS1 course\\nand number of prior CS courses which is thus not reflected in the\\nlarger, more historic data set. Recognizing this changing landscape\\nis a reminder to continue researching how to support students with\\nvarious CS college course pathways to CS2.\\n6 DISCUSSION\\nIn this follow-up paper, we revisited several factors that may be\\ncorrelated with student performance in a CS2 class on a large data\\nset that spanned over seven years and contained more than 5300\\nstudent records. Our analysis corroborates previous research on\\nsome of the predictors of CS2 performance and also provides some\\nnew and specific insights that can be further investigated. CS1\\ngrades are significant factors in our students’ CS2 performance. Our\\nresults confirm findings in previous studies [ 5,8,10] and also our\\nown previous one-semester study [ 15] in this regard. Furthermore,\\nwe saw no evidence of gender as being a significant factor in student\\nperformance or passing rate for this CS2 course. Prior works have\\nalso shown no correlation between gender and performance in\\nintroductory CS courses [33, 34].\\nOn the other hand, we found race and ethnicity to be correlated\\nwith student performance in this CS2 course over the 7 year time\\nframe. The lower passing rate of these students is also indicative\\nthat Black and Hispanic students are at a disadvantage when com-\\npared to their White and Asian peers, and risk dropping out of the\\nCS pipeline at a higher rates. Further work needs to be done to\\nsupport Black and Hispanic students in our CS2 course. It is possible\\nthese students’ performance is affected by their level of previous\\nexperience, self-efficacy, and sense-of-belonging, all of which can\\nbe interrelated and affect academic performance [ 18,23,27,28].\\nMore research and interventions need to be undertaken. For ex-\\nample, if these students have communal goal orientation and a\\nweaker sense of belonging similar to students in [ 20] then possibly\\nvolunteering opportunities, mentoring, and group projects may\\nimprove sense of belonging and thus performance. A psychological\\nintervention that is designed to instill that hardship and doubt are\\ncommon to all CS2 students regardless of race may also be able to\\nimprove Black students’ sense of belonging and self-efficacy and\\nthus performance [32].\\nIn addition, student’s major was also a significant factor in pre-\\ndicting CS2 grade. While undeclared engineering students, who\\nare College of Engineering students who have not yet declared\\na specific major, had a higher average grade in CS2, overall CS\\nmajors had the higher rate of passing the course. The difference\\nin passing rate between various major categories in this study (CS,\\nundeclared engineering, other STEM, and other) was significant.\\nThis indicates that CS students are passing CS2 at a higher rate. In\\nour previous study [ 15], we saw that the undeclared engineering\\ngroup outperformed others and passed at a higher rate. There is\\nnow a larger and higher performing percentage of undeclared engi-\\nneering students who are taking CS2. These students are 34% of the\\n7-year population and 52% of the Spring 2021 population. Placing\\nCS2 students in cohorts may help improve the performance of the\\nother STEM majors and other majors similarly to how Decker sawa decreased WDF rate in CS1 when students were placed in cohorts\\nbased on previous experience [9].\\nUnderserved status was also significantly correlated with the\\npassing rate of a student. Students who were considered under-\\nserved passed at a lower rate, and this indicates that intervention\\nor support mechanisms should be put in place to provide support\\nto these groups. Transparent teaching strategies as outlined by\\nWinkelmes et al. could improve confidence, sense of belonging, and\\nmastery of skills for low-income and underrepresented CS2 stu-\\ndents [ 35]. For example, ensuring that problem-based assignments\\nclearly state knowledge gained and skills practiced that are relevant\\nto students 5 years in the future.\\nWe saw no significant difference between students who trans-\\nferred CS1 credit from a community college. In our previous study\\nstudents who transferred CS1 credit had a lower passing rate. Our\\nanalysis on the larger historic data set is more aligned with work\\nby Catanese et al. that showed transfer students perform as well as\\nnon-transfer students in computing courses [ 7]. It is possible that\\nhistorically transfer students transfer CS1 credit, but more recently\\nan increased number of non-transfer students also obtain credit\\noutside our institution. Unfortunately, our data set did not indicate\\ntransfer student status and did not have enough granularity to\\nexamine the specific source of all possible CS1 pathways.\\n6.1 Threats to Validity\\nBecause there are no defined experimental and control groups in\\nthis study, a direct causality between process measurements and\\nour outcomes cannot be inferred. Differential experience could be\\na potential threat to this study, as students come from diverse aca-\\ndemic backgrounds. Our findings are based on data for students\\nat a larger research institution. As such our results might not be\\ngeneralizable to all undergraduate students. However, the charac-\\nteristics of our students are representative of typical undergraduate\\nstudents who enroll in CS2 offered at large research institutions in\\nthe United States.\\n7 CONCLUSION\\nThis is a replication study using a large data set over many years\\nof student records. We verified that CS1 grade is a good indicator\\nof CS2 performance, and that gender is not correlated with perfor-\\nmance. We found that Black and Hispanic students pass CS2 at a\\nlower rate and more attention must given to their success in CS2\\nclasses. Students who have already declared a CS major pass the\\ncourse at higher rates than other majors. Further study is needed\\nto understand current students’ prior pathways and computing\\nexperience before enrolling in CS2. Further study is also needed to\\ninvestigate these factors, not just in isolation, but in conjunction\\nwith one another and previous computing experience. Student self-\\nefficacy and sense of belonging should also be considered in order\\nto better understand how to support students for success in CS2.\\nREFERENCES\\n[1]Christine Alvarado, Gustavo Umbelino, and Mia Minnes. 2018. The Persistent\\nEffect of Pre-College Computing Experience on College CS Course Grades. In\\nProceedings of the 49th ACM Technical Symposium on Computer Science Education\\n(Baltimore, Maryland, USA) (SIGCSE ’18). Association for Computing Machinery,\\nNew York, NY, USA, 876–881. https://doi.org/10.1145/3159450.3159508\\n901Replication and Expansion Study on Factors Influencing Student Performance in CS2 SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\\n[2]Leland Beck, Patty Kraft, and Alexander W. Chizhik. 2022. Predicting Student\\nSuccess in CS2: A Study of CS1 Exam Questions. In Proceedings of the 53rd\\nACM Technical Symposium on Computer Science Education V. 1 (Providence, RI,\\nUSA) (SIGCSE 2022) . Association for Computing Machinery, New York, NY, USA,\\n140–146. https://doi.org/10.1145/3478431.3499276\\n[3]Susan Bergin and Ronan Reilly. 2005. Programming: Factors That Influence\\nSuccess. SIGCSE Bull. 37, 1 (Feb. 2005), 411–415. https://doi.org/10.1145/1047124.\\n1047480\\n[4]Susan Bergin, Ronan Reilly, and Desmond Traynor. 2005. Examining the Role of\\nSelf-Regulated Learning on Introductory Programming Performance. In Proceed-\\nings of the First International Workshop on Computing Education Research (Seattle,\\nWA, USA) (ICER ’05). Association for Computing Machinery, New York, NY, USA,\\n81–86. https://doi.org/10.1145/1089786.1089794\\n[5]Halil Bisgin, Murali Mani, and Suleyman Uludag. 2018. Delineating Factors that\\nInfluence Student Performance in a Data Structures Course. In 2018 IEEE Frontiers\\nin Education Conference (FIE). IEEE, 1–9. https://doi.org/10.1109/FIE.2018.8659300\\n[6]Emily Forrest Cataldi. 2018. First-generation students: College access, persistence,\\nand postbachelor’s outcomes. https://nces.ed.gov/pubsearch/pubsinfo.asp?\\npubid=2018421\\n[7]Helen Catanese, Carl Hauser, and Assefaw H. Gebremedhin. 2018. Evaluation\\nof Native and Transfer Students’ Success in a Computer Science Course. ACM\\nInroads 9, 2 (April 2018), 53–57. https://doi.org/10.1145/3204471\\n[8]Holger Danielsiek and Jan Vahrenhold. 2016. Stay on These Roads: Potential\\nFactors Indicating Students’ Performance in a CS2 Course. In Proceedings of\\nthe 47th ACM Technical Symposium on Computing Science Education (Memphis,\\nTennessee, USA) (SIGCSE ’16). Association for Computing Machinery, New York,\\nNY, USA, 12–17. https://doi.org/10.1145/2839509.2844591\\n[9]Adrienne Decker, Christopher Egert, and Erin Cascioli. 2020. Cohorting Incoming\\nStudents in a CS1 Course: Experiences and Reflections from the First Year of\\nImplementation. J. Comput. Sci. Coll. 35, 8 (aug 2020), 186–197.\\n[10] Richard J. Enbody, William F. Punch, and Mark McCullen. 2009. Python CS1\\nas Preparation for C++ CS2. SIGCSE Bull. 41, 1 (mar 2009), 116–120. https:\\n//doi.org/10.1145/1539024.1508907\\n[11] National Center for Education Statistics (NCES). 2021. Digest of Education\\nStatistics. Retrieved August 19, 2022 from https://nces.ed.gov/programs/digest/d21/\\ntables/dt21_326.10.asp, (2021).\\n[12] National Science Foundation. 2022. Women, Minorities, and Persons with\\nDisabilities in Science and Engineering. Retrieved August 19, 2022 from\\nhttps://ncses.nsf.gov/pubs/nsf21321/ (2022).\\n[13] Kari L. George, Linda J. Sax, Annie M. Wofford, and Sarayu Sundar. 2022. The\\ntech trajectory: Examining the role of college environments in shaping students’\\ninterest in Computing Careers. Research in Higher Education 63, 5 (2022), 871–898.\\nhttps://doi.org/10.1007/s11162-021-09671-7\\n[14] Arto Hellas, Petri Ihantola, Andrew Petersen, Vangel V. Ajanovski, Mirela Gutica,\\nTimo Hynninen, Antti Knutas, Juho Leinonen, Chris Messom, and Soohyun Nam\\nLiao. 2018. Predicting Academic Performance: A Systematic Literature Review.\\nAssociation for Computing Machinery, New York, NY, USA, 175–199. https:\\n//doi.org/10.1145/3293881.3295783\\n[15] Sara Hooshangi, Margaret Ellis, and Stephen H. Edwards. 2022. Factors Influ-\\nencing Student Performance and Persistence in CS2. In Proceedings of the 53rd\\nACM Technical Symposium on Computer Science Education V. 1 (Providence, RI,\\nUSA) (SIGCSE 2022) . Association for Computing Machinery, New York, NY, USA,\\n286–292. https://doi.org/10.1145/3478431.3499272\\n[16] Terry T. Ishitani. 2006. Studying attrition and degree completion behavior among\\nfirst-generation college students in the United States. The Journal of Higher\\nEducation 77, 5 (2006), 861–885. https://doi.org/10.1353/jhe.2006.0042\\n[17] Brandy M. Jenner. 2017. Student veterans and the transition to Higher Education:\\nintegrating existing literatures. Journal of Veterans Studies 2, 2 (2017), 26. https:\\n//doi.org/10.21061/jvs.14\\n[18] Sophia Krause-Levy, William G. Griswold, Leo Porter, and Christine Alvarado.\\n2021. The Relationship Between Sense of Belonging and Student Outcomes in CS1\\nand Beyond. In Proceedings of the 17th ACM Conference on International Computing\\nEducation Research (Virtual Event, USA) (ICER 2021). Association for Computing\\nMachinery, New York, NY, USA, 29–41. https://doi.org/10.1145/3446871.3469748[19] Lucas Layman, Yang Song, and Curry Guinn. 2020. Toward Predicting Success and\\nFailure in CS2: A Mixed-Method Analysis. In Proceedings of the 2020 ACM South-\\neast Conference (Tampa, FL, USA) (ACM SE ’20). Association for Computing Ma-\\nchinery, New York, NY, USA, 218–225. https://doi.org/10.1145/3374135.3385277\\n[20] Colleen Lewis, Paul Bruno, Jonathan Raygoza, and Julia Wang. 2019. Alignment\\nof Goals and Perceptions of Computing Predicts Students’ Sense of Belonging in\\nComputing. In Proceedings of the 2019 ACM Conference on International Computing\\nEducation Research (Toronto ON, Canada) (ICER ’19). Association for Computing\\nMachinery, New York, NY, USA, 11–19. https://doi.org/10.1145/3291279.3339426\\n[21] Adamou Fode Made and Abeer Hasan. 2020. Creating a More Equitable CS\\nCourse through Peer-Tutoring. J. Comput. Sci. Coll. 35, 10 (apr 2020), 33–38.\\n[22] An Nguyen and Colleen M. Lewis. 2020. Competitive Enrollment Policies in\\nComputing Departments Negatively Predict First-Year Students’ Sense of Be-\\nlonging, Self-Efficacy, and Perception of Department. In Proceedings of the 51st\\nACM Technical Symposium on Computer Science Education (Portland, OR, USA)\\n(SIGCSE ’20). Association for Computing Machinery, New York, NY, USA, 685–691.\\nhttps://doi.org/10.1145/3328778.3366805\\n[23] Anne-Kathrin Peters, Anders Berglund, Anna Eckerdal, and Arnold Pears. 2014.\\nFirst Year Computer Science and IT Students’ Experience of Participation in\\nthe Discipline. In 2014 International Conference on Teaching and Learning in\\nComputing and Engineering. 1–8. https://doi.org/10.1109/LaTiCE.2014.9\\n[24] Keith Quille and Susan Bergin. 2018. Programming: Predicting Student Success\\nEarly in CS1. a Re-Validation and Replication Study. In Proceedings of the 23rd\\nAnnual ACM Conference on Innovation and Technology in Computer Science Edu-\\ncation (Larnaca, Cyprus) (ITiCSE 2018). Association for Computing Machinery,\\nNew York, NY, USA, 15–20. https://doi.org/10.1145/3197091.3197101\\n[25] Keith Quille and Susan Bergin. 2019. CS1: how will they do? How can\\nwe help? A decade of research and practice. Computer Science Educa-\\ntion 29, 2-3 (2019), 254–282. https://doi.org/10.1080/08993408.2019.1612679\\narXiv:https://doi.org/10.1080/08993408.2019.1612679\\n[26] Farzana Rahman and Jaya Tyagi. 2021. Investigating the Role of Different Prep\\nPathways on CS2 Performance Across Three Different Majors. In Proceedings of\\nthe 22st Annual Conference on Information Technology Education (SnowBird, UT,\\nUSA) (SIGITE ’21). Association for Computing Machinery, New York, NY, USA,\\n141–146. https://doi.org/10.1145/3450329.3476851\\n[27] Nathan Rountree, Janet Rountree, and Anthony Robins. 2002. Predictors of\\nSuccess and Failure in a CS1 Course. SIGCSE Bull. 34, 4 (dec 2002), 121–124.\\nhttps://doi.org/10.1145/820127.820182\\n[28] Anya Tafliovich, Jennifer Campbell, and Andrew Petersen. 2013. A Student\\nPerspective on Prior Experience in CS1. In Proceeding of the 44th ACM Technical\\nSymposium on Computer Science Education (Denver, Colorado, USA) (SIGCSE ’13).\\nAssociation for Computing Machinery, New York, NY, USA, 239–244. https:\\n//doi.org/10.1145/2445196.2445270\\n[29] Virginia Tech. 2022. University DataCommons. Retrieved August 19, 2022 from\\nhttps://udc.vt.edu/irdata/dictionary (2022).\\n[30] Paul Tough. 2014. “Who Gets to Graduate?”. The New York Times Magazine, May\\n15. New York: New York Times Company. (2014).\\n[31] Arto Vihavainen, Jonne Airaksinen, and Christopher Watson. 2014. A System-\\natic Review of Approaches for Teaching Introductory Programming and Their\\nInfluence on Success. In Proceedings of the Tenth Annual Conference on Inter-\\nnational Computing Education Research (Glasgow, Scotland, United Kingdom)\\n(ICER ’14). Association for Computing Machinery, New York, NY, USA, 19–26.\\nhttps://doi.org/10.1145/2632320.2632349\\n[32] Gregory M. Walton and Geoffrey L. Cohen. 2007. A Question of Belonging: Race,\\nSocial Fit, and Achievement. Journal of Personality and Social Psychology 92, 1\\n(2007), 82–96.\\n[33] Brenda Cantwell Wilson. 2002. A Study of Factors Promoting Success in Computer\\nScience Including Gender Differences. Computer Science Education 12, 1-2 (2002),\\n141–164. https://doi.org/10.1076/csed.12.1.141.8211\\n[34] Brenda Cantwell Wilson and Sharon Shrock. 2001. Contributing to success in\\nan introductory computer science course: a study of twelve factors. Acm sigcse\\nbulletin 33, 1 (2001), 184–188. https://doi.org/10.1145/364447.364581\\n[35] Mary-Ann Winkelmes, Matthew L. Bernacki, Jeffrey Butler, Michelle Zochowski,\\nJennifer Dawn Golanics, and Kathryn Harriss Weavil. 2016. A Teaching Inter-\\nvention That Increases Underserved College Students’ Success.\\n[36] Sherry A. Woosley and Dustin K. Shepler. 2011. Understanding the Early Integra-\\ntion Experiences of First-Generation College Students. College student journal 45\\n(2011), 700.\\n902',\n",
       "  ['student assessment', 'computing education programs']),\n",
       " ('Working at the Intersection of Race, Disability, and Accessibility \\nChristina N. Harrington Aashaka Desai Aaleyah Lewis \\nCarnegie Mellon University University of Washington University of Washington \\nPittsburgh, PA, USA Seattle, WA, USA Seattle, WA, USA \\ncharring@andrew.cmu.edu aashakad@cs.washington.edu alewis9@cs.washington.edu \\nSanika Moharana Anne Spencer Ross Jennifer Mankof \\nCarnegie Mellon University Bucknell University University of Washington \\nPittsburgh, PA, USA Lewisburg, PA, USA Seattle, WA, USA \\nsmoharan@andrew.cmu.edu a.ross@bucknell.edu jmankof@cs.washington.edu \\nABSTRACT \\nExaminations of intersectionality and identity dimensions in ac-\\ncessibility research have primarily considered disability separately \\nfrom a person’s race and ethnicity. Accessibility work often does \\nnot include considerations of race as a construct, or treats race as a \\nshallow demographic variable, if race is mentioned at all. The lack \\nof attention to race as a construct in accessibility research presents \\nan oversight in our feld, often systematically eliminating whole \\nareas of need and vital perspectives from the work we do. Further, \\nthere has been little focus on the intersection of race and disability \\nwithin accessibility research, and the relevance of their interplay. \\nWhen research in race or disability does not mention the other, this \\nwork overlooks the potential to better understand the full nuance \\nof marginalized and “otherized” groups. To address this gap, we \\npresent a series of case studies exploring the potential for research \\nthat lies at the intersection of race and disability. We provide exam-\\nples of how to integrate racial equity perspectives into accessibility \\nresearch, through positive examples found in these case studies \\nand refect on teaching at the intersection of race, disability, and \\ntechnology. This paper highlights the value of considering how \\nconstructs of race and disability work alongside each other within \\naccessibility research studies, designs of socio-technical systems, \\nand education. Our analysis provides recommendations towards \\nestablishing this research direction. \\nCCS CONCEPTS \\n• Human-centered computing → Accessibility; • Social and \\nprofessional topics → Race and ethnicity. \\nKEYWORDS \\nRace, Disability, Accessibility, Intersectionality, Inclusion \\nACM Reference Format: \\nChristina N. Harrington, Aashaka Desai, Aaleyah Lewis, Sanika Moharana, \\nAnne Spencer Ross, and Jennifer Mankof. 2023. Working at the Intersec-\\ntion of Race, Disability, and Accessibility. In The 25th International ACM \\nThis work is licensed under a Creative Commons Attribution International \\n4.0 License. \\nASSETS ’23, October 22–25, 2023, New York, NY, USA \\n© 2023 Copyright held by the owner/author(s). \\nACM ISBN 979-8-4007-0220-4/23/10. \\nhttps://doi.org/10.1145/3597638.3608389 SIGACCESS Conference on Computers and Accessibility (ASSETS ’23), Octo-\\nber 22–25, 2023, New York, NY, USA. ACM, New York, NY, USA, 18 pages. \\nhttps://doi.org/10.1145/3597638.3608389 \\n1 INTRODUCTION \\n“If Disability Studies took up Black Studies and Critical \\nRace Theory in ways that displaced the white disabled \\nbody as the norm, we might gain a stronger, more fex-\\nible, and globally relevant framework. . . Too often we \\nengage race and its impact as an additive or compar-\\native category of diference rather than a constitutive \\naspect of notions of disability in the West.” (Bailey and \\nMobley, [5]) \\nIn her 1991 Stanford Law Review article, Kimberlé Crenshaw \\nspeaks of the importance of intersectionality as a analytic lens say-\\ning “although racism and sexism readily intersect in the lives of real \\npeople, they seldom do in feminist and anti-racist practices” [35]. \\nSociologists Patricia Hill Collins and Sirma Bilge note that the the-\\noretical lens of intersectionality stands to address the gap within \\nsocial problems experienced by women of color that persists within \\na single-focused lens on social inequalities [31]. Since the coining \\nof this term and emergence of its use among scholars in law and the \\nhumanities, we have seen researchers apply the theoretical frame-\\nwork of intersectionality to various areas of study to understand \\nthe many axes of social division that work together and infuence \\npeople’s lives and their relationship to power. Scholars who take a \\ncritical look at technology research have applied intersectionality \\nas a framework to examine race and social class in regards to so-\\nciotechnical systems, arguing that this lens and research approach \\nhas the potential to “bring about solidarity within the HCI com-\\nmunity” [127]. Similarly, we argue that, while ableism and racism \\noften collide in everyday life [109], the intersection of these con-\\nstructs is mostly absent from both accessibility research and race \\nand HCI research. As a result, implicitly, the bulk of accessibility \\nresearch contributes to a limited understanding of the experiences \\nof people with disabilities, primarily focusing on those who are \\nWhite [52, 118, 134], or well of and have access to higher education \\n(to even be diagnosed with some types of disability is a privilege of \\nthe white middle class [17]). \\nImportant advances have been established at the nexus of race \\nand disability outside the technology space. Various scholars have \\nbegun to advocate for an intersectional lens when studying disabil-\\nity [53, 68], primarily in areas of education and law. For example, \\nHarris [68] asserts that some of the more salient socio-politicalASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nissues can no longer stand to look at racialization and disability as \\ndistinct. In his recent article, Harris states that centering those at \\nthe intersection of the racial justice and disability rights movements \\npresent a “unifying lens to understand the roots of both race and \\ndisability discrimination, the nature of the harms experienced by \\nthose with intersectional identities” and presents a construction of \\nremedies to address inequality. Outside of the ivory tower, disability \\nactivists have led eforts to advocate for more intersectional move-\\nments where these constructs and dismantling the axes of power \\nthat they succomb to are acknowledged as critical to liberation \\n[77], citing not only the erasure but the regression of progress that \\nhappens when neglecting one or the other. Technology research \\nhas additionally demonstrated the importance of looking at race \\nand disability, (although in isolation from each other) for certain \\ntopics (e.g., biased algorithms [34, 62, 108, 125]). Preliminary in-\\nvestigations of an intersectional lens to disability and accessibility \\nresearch point to the advantages for digital inclusion [54, 105], and \\nhighlight the potential of neglecting such perspectives. Given this \\ncontext, by ignoring the intersection of race and disability in ac-\\ncessibility research, we may fail to study topics that arise only at \\ntheir intersection (such as the need for captioning systems that \\nsupport the rapid code switching common in multilingual families \\n[140]). The potential for investigating the intersections experienced \\nby racially minoritized groups that live with disbilities is vast, and \\ncalls attention to how we might make technology and our research \\npractice more inclusive. \\nWe might also ask what new domains of study accessibility \\nmight engage with, were it to consider the intersection of race and \\ndisability. For example, disaster response and safety is an important \\narea of vulnerability for both Black and Brown people and people \\nwith disabilities [39], and of interest to HCI researchers (e.g., [110, \\n121, 137]), disability scholars (e.g., [2, 75, 114, 124]) and race scholars \\n(e.g., [15]). HCI researchers have also begun to engage with groups \\nsuch as unhoused people [84], institutionalized or incarcerated \\npeople [120], and unemployed individuals [42, 43]. All of these \\nare domains where people with disabilities, and Black and Brown \\ncommunities, are either over- or underrepresented and as a result \\nmust be considered through an intersectional lens to accessibility \\nresearch. \\nA few accessibility papers have begun to emerge in this landscape \\nto address this gap (e.g., [11, 79, 98, 141]). For example, Bennett \\net al. [11] explore the role of race in image descriptions through \\ninterviews with a variety of people with diverse identities. Their \\npaper represents a valuable foray into what we believe is a rich \\nand under-examined space for research in accessibility. Building \\nupon the recent emergence of this area, we echo that a broader \\nunderstanding and awareness of these constructs is foundational to \\nmeaningfully engage the intersection of race, disability, and tech-\\nnology. Thus, our exploration of this topic was spurred initially \\nby an efort by the authors to educate themselves in this space. \\nIn the Fall of 2020, two members of our research team led a re-\\nsearch seminar on Race, Disability, and Technology. We detail this \\nexperience in Section 3. Developing the reading list and seminar \\ndiscussions revealed the scarcity of work in this space. The sem-\\ninar also helped highlight the many impactful topics that can be \\nunderstood only through the intersection of race, disability, and \\naccessibility research. This paper provides an overview of our eforts towards develop-\\ning a graduate seminar focused on the intersection of race, disability \\nand technology. We refect on our experience teaching this research \\nseminar as the catalyst to an analysis of existing work and research \\nin our feld. Refections of this seminar led to the formation of this \\nresearch team, and our work together to learn about the intersec-\\ntion of race and disability. From our realization that we did not \\nhave a framework for thinking about work at this intersection, \\nwe present a review of considerations for engaging with race and \\ndisability in the research and education process and highlight four \\nopportunities for intersectional engagement with disability and \\nrace in accessibility research. We then analyze three exemplary \\npapers which lie at the intersection of race, disability and technol-\\nogy, identifying research recommendations to rethink our eforts \\ntowards accessibility research. \\nPositionality. We ofer the analyses of the case studies in this \\npaper as one of several possible interpretations which are infuenced \\nby our identity and positionality as scholars. Situated knowledge \\nis essential to evaluating technological systems in sociocultural \\ncontexts [155], and as a result there is an active discourse around \\naccessibility research that includes the perspectives of disabled \\npeople and those that technology aims to serve [12, 60]. At the same \\ntime, it has rightly been pointed out that disabled academics do not \\nrepresent all people with disabilities [86]. With these considerations \\nin mind, we worked with the intention to form an author group \\nthat could draw from both personal and professional knowledge \\nin doing this work. We note this is of particular importance as \\nresearch scholars examining the intersection of race, disability, and \\naccessibility. Thus, our research team is composed of both scholars \\nwith and without disability, graduate students and senior academic \\nfaculty, and individuals who identify as White American, Black \\nAmerican, South Asian-American, and South Asian immigrant. All \\nauthors have extensive experience studying accessibility. \\nWe also acknowledge that our scope of race is contextualized \\nto the United States based on our collective positionality and ex-\\nperiences as living across the United States. Following work by To \\net al. [143], we situate our work in the United States as a frst step. \\nOur discussion of what it means to engage race and disability in ac-\\ncessibility research throughout this paper is therefore informed by \\nthe perspective of the history and current state of racial dynamics \\nwithin the United States. \\n2 BACKGROUND ON DISABILITY, \\nACCESSIBILITY & RACE \\nMuch of the research on technology-related accessibility has fo-\\ncused primarily on a very few categories of disability [99] such as \\nblind and low vision technology (over 40%); while research on the \\nintersection of accessibility and other identity categories is rare. For \\nexample, in the summer of 2020, we conducted a search of the ACM \\ndigital library for papers that used words like “race”, “disability” and \\n“Black” in preparation for the teaching seminar. Our search turned \\nup extremely few results that were not relevant to the constructs of \\nrace and disability (such as about black lists and race conditions in \\nthe security sphere)∗. In the rare case when a paper talks about both \\n∗We acknowledge that since the submission and acceptance of this paper there \\nhave been additional papers published in this area that were not included in this searchWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\ndisability and race [98], they are often treated separately, providing \\ninformation on what percentage of participants are in various cat-\\negories without considering how those identities interact or how \\nthey impact lived experiences of participants (e.g., [67, 156]). We \\nnote that literature on the intersection of disability and race has \\narisen from both academic contexts such as DisCrit [32] and com-\\nmunity contexts such as Disability Justice [77]; however much of \\nthis literature is not yet connected to most accessibility research. \\nTo that end, we focus this review on important adjacent domains \\nwhere these constructs have been studied. First, we defne what we \\nmean by the terms ‘disability’ and ‘race’, and contextualize what it \\nmeans to engage with these constructs in research. We next review \\nsome of the ways in which race and disability interact to amplify \\ninequity and highlight key precedent work at this intersection. \\nFinally, we identify technology-related work at the intersection of \\ndisability and race including those that have amplifed disability or \\nracial bias. \\n2.1 Defning Constructs of ‘Disability’ and \\n‘Race’ \\nScholars have acknowledged that both disability and race are social \\nconstructs with no objective reality, and thus people come to make \\ndecisions on them based on their social identities [93, 122]. For ex-\\nample, according to Leonardo and Broderick, ‘Whiteness’ “reserves \\nthe right to exclude any person or group for the purposes of racial \\ndomination. ‘White’ is whatever Whites make it to be, using what-\\never ideological reasoning happens to be available at the time” [93]. \\nExplaining the historical musings of historians James McPherson \\nand Ralph Waldo Emerson, Ta-Nehisi Coates positions that race \\nis an American social construct that is a product of social context \\n[30]. He explains that there are no physiological measures which \\nseparates the races, instead they were adapted to explain social \\ndiference and inferiority. \\nSimilarly, scholars have positioned disability as one that is sub-\\njective with no one particular experience. The specifc language \\nused by people with disabilities to describe themselves and the \\nconcept of a disability identity are inconsistent [ 101, 129, 136]. As \\nnoted by Susan Jones, the experiences of disability vary across time, \\nculture, and environments [80]. There is no singular experience \\nof disability, rather humans defne what it means to be disabled. \\nFor example, while the medical model situates disability within \\nthe individual (as impairment), the social model of disability posits \\nthat disability is caused by barriers in the environment and soci-\\nety [117], and the political/relational model further recognizes the \\nideological systems and discriminatory attitudes that contribute \\nto disability [82]. Within the context of HCI and design research, \\nscholars posit that exploring and engaging constructs of ‘disability’ \\nand ‘race’ must also move past solution-problem framing and ori-\\nentation [37, 64, 141], and instead employed in ways that leverage \\nthe social complexities of these constructs. Engaging constructs \\nof race and disability in new research eforts will require such an \\norientation that can be used to navigate discussions and analyze \\nresearch fndings, outcomes, and what this means for accessibility \\nas a result. \\nSeveral research-related challenges may exist due to the nature of \\nthese variables being defned by social consensus. One consequence of the complexities underlying understanding how to engage these \\nsocial constructs in research assessments is that they are hard to \\nmeasure. Oftentimes, within a collection of demographics, there are \\nseveral compounding factors that can act as proxies for engaging \\nin race and disability. Common adjacent data and identifers that \\nare reported as ways to defne or operationalize these constructs \\ninclude health status and services utilized, socioeconomic status, \\nand locality [115]. For example, evaluating what healthcare services \\nare utilized by an individual and the extent of their use are used as \\ndeterminants of access, even though disability, type of impairment, \\nand level of severity can be multi-faceted and require consideration \\nof social factors [117]. Research eforts can be further complicated \\nby the fact that identifying along each of the constructs of race or \\ndisability can have undesirable efects. Mannerisms and interactions \\nof how folks are treated upon revealing they belong to a certain \\nidentity, community, and group can have negative perceptions and \\nconsequences. This is common within employment and other pro-\\nfessional settings [70]. The disclosure of these facets also subject \\npeople at this intersection to more criticism and harm. Finally, data \\ncollection (and the labeling associated with it) are complicated by \\nthe fact that these social constructs are a moving target [93]. People \\nmay interpret one’s race based on social positioning or other sub-\\njective factors (e.g. perceptions of skin tone, regional background). \\nIn accessibility research, this is potentially exarcerbated by the fact \\nthat little emphasis is placed on reporting or recording information \\nabout race, or engaging with it as a construct. \\n2.2 The Intersection of RacexDisability \\nAs a feld, HCI broadly continues to refne what it means to ap-\\npropriately enact intersectionality [127, 135]: Scholars Rankin and \\nThomas assert that drawing on intersectionality requires far more \\nthan checking boxes of race, class or gender but instead an under-\\nstanding of the politics of identity [127]. When enacted through \\nan approach of categorizing identities, the structural dynamics of \\nresearch that is intended to center the voices of participants “still \\noften perpetuates the exploitation of marginalized folks” [22]. Al-\\nthough the intersection of disability and race is understudied in the \\naccessibility technology literature, it is very much a topic of inquiry \\nin the broader disability literature [32, 40, 77, 158]. This body of \\nliterature touches on topics such as religion, fashion, innovation, \\nscience inclusion, communication access, climate and disaster re-\\nsponse, mutual aid, interdependence, joy, class, ability, opportunity \\ngaps, over and under representation, the school-to-prison pipeline \\nand school reform, and race, disability and the law. \\nEngaging both race and disability within research also means \\nrecognizing that the intersection of these social constructs might \\nnot be explicit. On the one hand, those that sit at the crossover of \\nthese identities often face a “double burden” [44]. This overlapping \\nmarker within social stratifcation dicates additional inequities, dis-\\nparities, and lack of access. On the other hand, Rankin and others \\nacknowledge that observing the lens of such intersectional identi-\\nties may in fact lead to a more unifed research practice through \\nunique experiences [126, 127]. For example, a study of STEM work-\\ners found that people of color with disabilities were among the most \\ndisadvantaged compared to white able bodied heterosexual men \\nand 31 other intersectional groups on measures ranging from socialASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\ninclusion to salary [26]. More generally, race, disability and poverty \\nare deeply intertwined in the United States [56, 76], with disabled \\npeople, especially those of color, facing inequities in education, \\npoverty status, employment, medical debt, food security, and more \\n[56]. Given this context, it is not surprising that students with dis-\\nabilities, particularly people of color with disabilities, are less than \\nhalf as likely to complete a bachelor’s degree as their non-disabled \\ncounterparts [56]. Similarly, entire books have been written about \\nthe interlocking factors that contribute to the over-representation \\nof both people of color (particularly Black people) and people with \\ndisabilities in the North American carceral system, refugee and \\nasylum systems, and systems that feed into these [28, 40]. These \\nproblems have deep historical roots, in the context of eugenics and \\nmedical experimentation and immigration, as highlighted in stud-\\nies re-examining historical events from a lens that considers the \\nintersection of race and disability (e.g., [ 78, 112, 147]). For example, \\n(supposed) disability was used to justify slavery and to deny immi-\\ngration to unwanted groups [108, 112]: “...discrimination against \\npeople of color, women, and other historically marginalized groups \\nhas often been justifed by representing these groups as disabled. . . . \\nThus disability is entwined with, and serves to justify, practices of \\nmarginalization.” ([108], p. 11). \\nThere is also an active and informative dialogue taking place \\noutside of academia at the intersection of race and disability, includ-\\ning blogs and podcasts such as the ‘Chicas Talk Disability YouTube \\nChannel’† and the ‘Black Disabled Men Talk’ podcast‡. Disability \\nactivists have also championed the #BlackDisabledLivesMatter§ \\nand #DisabilityTooWhite [109] movements to acknowledge this \\nintersection. Many of those driving this broader conversation have \\nhistroically been actively excluded from academia in the past, but \\nhave provided a strong foundation for any discourse around the \\nintersection of race and disability. \\nOut of these and other settings, the concept of ‘Disability Justice’ \\nhas arisen [14, 77]. Disability Justice, which was founded by a \\ngroup of marginalized, disabled community activists and artists, is \\nbased on the observation that “all bodies are caught in bindings of \\nability, race, class, gender, sexuality and citizenship ...[and that] \\nonly universal, collective access can lead to universal, collective \\nliberation” [77]. Rather than focusing on one issue, disability justice \\nrecognizes the power in complex identities, and situates power in \\nan intersectional, anti-capitalist collective framework of caring, \\ninterdependence, solidarity and liberation. At the same time, it \\nprovides a critical lens for examining oppression [141]. \\n2.3 Race and Disability in the Technology Space \\nThe idea that technology can reproduce racism is discussed in works \\nsuch as Hankerson et al. [61], which found that at the time only \\nsix articles in the ACM digital library mentioned the term “racism.” \\nThe authors ask why racism is not refected in our literature and \\nhighlight how technology can perpetuate racial bias, a premise \\nthat is further investigated in Ruha Benjamin’s book ‘Race After \\nTechnology’ [9]. It does not require digging far beneath the surface \\nto uncover a multitude of examples of such technologies. Multiple \\n†https://www.youtube.com/@chicastalkdisabilityyoutub7482/about \\n‡See https://blackdisabledmentalk.com/\\n§See https://blacklivesmatter.com/disabled-black-lives-matter/ articles have taken up the important topic of how technology and \\nrace intersect (e.g., [66, 116]) as well as the inclusiveness of the \\nfeld to scholars of color [47]; our methods (e.g., [21, 41, 95, 145]); \\nand the potential for technology to directly address the implicit \\nand explicit racism in app and algorithm design (e.g., [14]) and the \\nimpacts of racism (e.g., [144]). \\nSimilarly, technology can reproduce ableism. Not only are many \\nwebsites and apps simply not accessible to people who do not use \\nthe traditional combination of keyboard, mouse, and monitor in \\ntraditional confgurations (e.g., [132, 149]); but many apps, data sets, \\nand algorithms may all encode ableist biases [104]. Data-related \\nbiases can be made worse by the heterogeneity of the disability \\nexperience, making it hard to ensure that data about any one per-\\nson can be collected at scale [125]. As with racism and technology, \\nmany examples exist of ableist biases impacting a multitude of \\ndomains. The risks associated with ableist technology include se-\\nrious concerns such as disclosure (of disability), surveillance, and \\ntechnology’s role in denying services to people or failing to even \\nrecognize that a sensed signal is a person as well as exacerbating \\nor causing disability [104]. Ableism is so prevalent, even within \\nour own accessibility research community, that multiple papers \\nhave taken a critical perspective on how we as a community co-\\nconstruct our domains of inquiry (e.g., [ 97]’s discourse analysis of \\ninstitutional logics in discourses of housing and [162]’s analysis of \\nepistemic violence in disability-related technology research). \\nDespite the parallels in these two domains of inquiry, few tech-\\nnology focused articles have considered the intersectional aspects \\nof race, disability and technology. There is a robust literature study-\\ning race and culture in the context of assistive technology adoption \\n[74, 131, 146], which illustrates the importance of intersectional \\nanalyses that illuminate how racism and ableism intertwine and \\ninteract to generate unique forms of inequality and resistance (e.g., \\n[74]). Such literature includes the examination of disability and \\nlabor of data workers in China [159], the study of image descrip-\\ntions with and about intersectional identities [11], post-COVID \\nmutual aid networks among communities of color and people with \\ndisabilities [139], and the intersectional experiences of refugees \\nwith disabilities [59]. Some of this literature even acknowledges the \\ncomplex nuance of the intersectional lens, such as Edwards et al. \\n[46] highlighting tensions in intersectional persona development \\nwith regard to concerns such as oversimplifcation and stereotyping. \\nFinally, Sum et al. [141] organized a workshop on Disability Justice \\nin HCI, which included 34 submissions touching on topics from \\ndisability justice in the Global South to body autonomy to smart \\ncities to virtual and hybrid conferences as a form of intersectional \\nequity. Although these workshop papers represent early thinking \\nin this space, they demonstrate the breadth and depth of research \\nthat our community is inspired to do at the intersection of race and \\ndisability. \\n3 TEACHING & LEARNING AT THE \\nINTERSECTION \\nThis project started with a graduate-level reading seminar that fo-\\ncused on disability, race, and technology held in the Fall of 2020 led \\nby two members of our research team. This course was in response \\nto the national activism for racial equity, forefronted by the BlackWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\nLives Matter movement in 2020, which led some of our research \\nteam to acknowledge their lack of experience with the intersection \\nof race, disability, and accessibility. Instructors chose a seminar \\nformat to structure self-education, and open the opportunity to \\nothers. This seminar was neither exhaustive nor perfected, but it \\nprovides context for this paper and experience for others to draw \\nfrom, especially those who do not feel they have the background to \\nbegin exploring the intersection of race, disability, and technology. \\n3.1 Structure \\nThe Fall 2020 seminar was a specialized version of a graduate-level \\naccessibility research seminar that had been ofered in a computer \\nscience department for years. The class included about 20 people \\nwho self-identifed with a range of cultural backgrounds with many \\nstudents identifying as disabled. However, the group was predomi-\\nnantly white and female graduate students from computer science, \\nengineering, and human-centered design. Almost all attendees had \\nresearch focused on accessibility. Few had professional experience \\nengaging with race in their research. Overall, the class, including the \\nmembers of the research team that served as instructors, were more \\ncomfortable discussing accessibility research, disability, and access \\nneeds. This created opportunities for learning and unlearning as \\nwell as challenges in extending our discussions to meaningfully \\nengage with race. \\nTo prepare for the seminar, two members of our research team \\nthat instructed the seminar spent the summer of 2020 searching for \\ntechnical academic publications in the ACM digital library using \\nphrases such as “race”, “disability”, “accessibility”, and “Black”, but \\nfound very few relevant papers, a driving factor for continuing the \\nproject presented in this paper. The instructors also began reading \\nbooks that discussed the intersection of race and disability, such as \\nDisCrit[ 32]; Blackness and disability: Critical examinations and cul-\\ntural interventions [78]; and Disability Visibality [158]. The scarcity \\nof literature at the intersection of race, disability and technology \\nled us to bring a variety new of perspectives into our seminar in-\\ncluding press, blogs, panels, and other media. When searching for \\nnon-academic papers, our team of instructors tried to prioritize \\nsources from people with frst-hand experience. Table 1 presents \\nthe fnal reading schedule, which included areas with known large \\nimpacts at the intersection of race and disability (e.g., policing, \\nhealthcare, algorithmic bias); common topics from previous itera-\\ntions of the accessibility seminar that instructors wanted to revisit \\nwith a race-disability intersectional lens (e.g., speech technology, \\nfabrication); and topics that emerged from seminar discussions (e.g., \\nself description & disclosure, activism in academia). The Activism \\nin Academia topic emerged during the seminar as the group bal-\\nanced self-education (i.e., through the reading and discussions) and \\ncalls-to-action. Within the seminar attendees presented current \\nevents and opportunities for taking action (e.g., protests and peti-\\ntions to local governance around police brutality in the wake of \\nBLM protests, petitions and open letters on discrimination in our \\nacademic spaces). Instructors tried to balance using the seminar for \\nbuilding capacity for collective action with not wanting to misuse \\nour power as facilitators. \\nAdditionally, the team of instructors sought guest presenters \\nwho worked in relevant topic spaces outside of accessibility or had done accessibility work that featured Black participants. Scholars \\nincluded researchers whose work supported Black communities \\nof elders, and a researcher who had done work in bias in artifcial \\nintelligence systems refecting on race and disability independently. \\nOur guests were all academics from our close professional networks \\n(see Table 1). While the broader (non-academic) use of blog posts \\nand podcasts was benefcial to the seminar, instructors did not con-\\ntact those authors. On refection, this may have undermined these \\nauthors’ ability to get value from our use of their material. Moving \\nforward, we encourage proactively contacting authors about the \\nuse of their publications and creating paid opportunities for those \\nauthors to join the discussions. \\n3.2 Lessons from Facilitating: Access Needs, \\nIdentity, and Power Dynamics \\nThe seminar was intended to be inclusive for people with a range \\nof backgrounds, accessibility needs, and communication styles. The \\nprocess of co-creating access practices within the identity-focused \\ntopics of the seminar, surfaced how access interacts with race, \\ncultural background, and other identity dimensions. This led to \\ndiscussion topics, opportunities for learning, and points of refection \\nfor us as facilitators. \\nTo support access, people were encouraged to speak their name \\nwhen commenting verbally, voicing text-based contributions, and \\nverbally describing our visual appearance and setting for people \\nwho were blind, low-vision, did not have video access, or otherwise \\nbeneftted from that information. Driven by the seminar’s focus and \\nthe expectations set by the course instructors’ introductions, these \\nself-descriptions often included race/skin-tone and disability infor-\\nmation. Some participants shared identity information as an access \\npractice, others to contextualize their contributions. Discussions \\narose around balancing access with disclosure. \\nAdditionally, there was uncertainty around the language of self-\\ndescription (e.g., describing skin-tone versus racial or cultural back-\\nground), discomfort in drawing additional attention to people with \\nminority identities, and creating implicit pressures for people to \\nshare more parts of their identity than they wanted (e.g., pronoun \\nsharing for someone questioning their gender identity). One exam-\\nple of an initial misstep during the seminar was the understanding \\nand misuse of the term “white-presenting” as a descirptive phrase. \\nProviding historical context to the term provided space to under-\\nstand its misuse for the correct term, “white-passing”. Students \\nresponded (including through more private channels) that others \\nhad recognized this mis-use as well but not felt comfortable speak-\\ning up. While this was an opportunity to demonstrate learning \\namong instructors, it was also indicative of the power dynamics \\nthat other people in the discussion group did not feel comfortable \\nraising this as a concern before instructors did. This examples indi-\\ncates both the importance of engaging with scholars from diverse \\nbackgrounds and viewpoints in discussing disability and accessibil-\\nity but also the need to create safe and open learning environments \\nthat facilitate refection, open discussion, and correction.ASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nTable 1: List of topics and readings assigned each week. \\nIntroduction How to center disability in the tech response to COVID-19 [24]; \\nOn Being Black and ‘Disabled but Not Really’ [7] \\nAI & Fairness [Guest visitor: Dr. Shari \\nTrewin] Algorithmic de-biasing [125]; Artifcial Intelligence’s White Guy Problem [34]; \\nCan you make an AI that isn’t Ableist? [62]; Optional: [108] \\nSpeech & Speech Technologies On How Deaf People Might Use Speech to Control Devices [16]; \\nWhy racial bias still haunts speech-recognition AI [17, 96]; \\nBlack AAC User Perspectives on Racism and Disability [153] Optional: [20]; [88];[55] \\nGovernmentality & Algorithms \\nExposing Error in Poverty Management Technology: A Method for \\nAuditing Government Benefts Screening Tools [49]; \\nWhat Happens when an Algorithm Cuts your Healthcare [92] \\nFinancial Inequality: Disability, Race and Poverty in America [56] \\n(Introduction) \\nOptional: [84]; [156] \\nHealth and Healthcare Provision Engaging low-income African American older adults in health discus-\\n[Guest visitor: Dr. Christina Harrington] sions through community-based design workshop [65]; \\nChallenging Invisibility, Making Connections: Illness, Survival, and \\nBlack Struggles in Audre Lorde’s Work [18]; \\nCompounded Disparities: Health Equity at the Intersection of Disabil-\\nity, Race, and Ethnicity. [161] (especially Section 4a) Optional: [163]; [85] \\nSelf-Description and Disclosure \\n[Guest visitor: Dr. Cynthia Bennett] “Recovering our Stories”: A small act of resistance. [33]; \\nHow to write an image description. [27] \\nOptional: [71]; Audio Description example [Kartemquin Films]; [50]; \\n[25] [142] \\nPolicing, Prison & the School to \\nPrison Pipeline [Guest visitor: Dr. Karin \\nMartin] Surveillance and Confnement: Explaining and Understanding The \\nExperience of Electronically Monitored Curfews [111]; \\nMigrant surveillance: How the federal government monitors asylum \\nseekers [1]; \\nWhen They Call You a Terrorist. Chapter 4. [36]; \\nCrippin’ Jim Crow: Disability, dis-location, and the school-to-prison \\npipeline [48]; Optional: [4](1 hr listen); [72] (preface and frst chapter); [8] \\nActivism in Academia & the Work-\\nplace Addressing institutional racism within initiatives for SIGCHI’s diver-\\nsity and inclusion. [57]; \\nA challenging response. [102]; \\nMayor Durkan and Seattle Police: Release the Public Records of Her-\\nbert Hightower Jr.’s ’04 Police Killing NOW! (policing and disability in \\nSeattle petition); \\nDemand for the University of Washington Administration to Meet the \\nNeeds of Black Students (petition on policing and education at the \\nUniversity of Washington) \\nFabrication & Cyborgs 3D printing prosthetics for amputees in Haiti [29]; \\nShifting Expectations: Understanding Youth Employees’ Handofs in a \\n3D Print Shop [45]; \\nA very kind conversation between a cyborg and some biohackers [150] \\n3.3 Refections on Teaching Race and \\nAccessibility material in topics around race, disability, and technology was use-\\nful. However, due to the skew in experience of seminar attendees \\ntoward accessibility research, it was an ongoing challenge to have \\nrich discussions around the intersection of race in the accessibility \\nand technology spaces. One solution could be integrating more Students appreciated the breadth of topics and sources covered. Sem-\\ninar participants had a range of backgrounds, so having groundingWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\nfoundational readings into the course and continuing to build our \\nown understanding to be better able to facilitate those discussions. \\nIn the time since leading the seminar, instructors have contin-\\nued to explore literature on race in computing and critical studies. \\nFuture iterations or continuations of this seminar could include \\nrecent additions to the literature in the HCI and Accessibility com-\\nmunities, as well as more historic readings to contextualize the \\nsystemic history of the intersection of race and disability and the \\nsystemic history of race and disability-based oppression through \\ntechnical systems. Further, scholars could draw from a larger range \\nof disciplines. We explore this approach as a part of this paper. \\n4 FRAMEWORKS FOR ENGAGING THE \\nINTERSECTION OF RACE & DISABILITY \\nLearning from the experiences of the seminar, our larger research \\ngroup formed to begin exploring next steps that would address \\nthe gap observed within intersectional work in the feld of accessi-\\nbility. Here we present a frst important step: Identifying existing \\nframeworks for engaging with race (e.g., [3]) and disability (e.g., \\n[81, 100]). We are not aware of a single framework within HCI that \\nengages with the intersection of both race and disability, however \\nthe frameworks we review here share a common focus of engaging \\nacross the research process. Andrews et al. provide one founda-\\ntion of appropriately engaging with race and ethnicity in research \\n[3]. Here, scholars discuss four stages, and related substages, for \\nincorporating a racial and ethnic equity perspective throughout \\nthe research process as critical to work is both inclusive and re-\\nspectful. According to this framework, researchers who work with \\nor study phenomena related to racially minoritized communities \\nhave a responsibility not to “perpetuate disparities, inequalities, \\nand stereotypes” [3]. As such, they recommend that such research \\nshould start with a “landscape assessment”, or understanding the \\nhistory and values of a community; collecting the contextual infor-\\nmation necessary to properly defne the research problem without \\nbias and identify root causes. Next, study design requires develop-\\ning equitable research questions, designing the research process \\nwith community input, and considering who should collect data \\nand how to share and prioritize information. Data analysis must \\nguard against implicit bias and support community investment in \\nthe results. Finally, dissemination should consider audience, mes-\\nsaging, medium and sustainability through ongoing engagement \\nwith the community of focus. \\nWithin the feld of computing, Mack et al. [100] identify a staged \\nprocess for inclusive methods to consider disability starting with \\n“doing your homework” (similar to a landscape assessment), and \\nthen integrating the study design with accessibility considerations \\nfor method selection, recruitment, access checks, transportation to \\nthe study, and accessibility of physical space if relevant. They also \\ndiscuss data analysis and dissemination, where they highlight the \\nimportance of member checking at the end of the study [100]. This \\nprocess demonstrates the potential for people with disabilities to \\nparticipate in the research process and ideal research parameters \\nthrough activities such as member checking, which ensures that \\nparticipants’ voices are not only heard, but clearly valued. Kabir [81] \\nadditionally introduces participant health concerns as being central \\nto any framework of engaging with disability; listing a similar set of stages. Both Mack et al. [100] and Kabir [81] also argue for the \\nimportance of both anticipating potential barriers to access, and \\nadjusting in the moment. This is especially true when needs may \\nchange dynamically [101]. \\nIn addition to framework-level contributions, we draw from lit-\\nerature that highlights new methodological structure and goals \\nthat arise in working at these intersectional spaces [145, 152]. For \\nexample, Leal et al. and Ymous et al. [91, 162] highlight how this \\nwork can perpetuate epistemic violence and oppression. Oftentimes, \\ndisabilty or even racialized identity “demarcate a type of knowing \\nand lived experience that is systematically subverted” and marginal-\\nized away from what is considered “real” research. There is also \\na risk of invalidation in work that stems from people that might \\nnot carry institutional power [63, 91, 162]. Such work may be seen \\nas threatening [86], or be categorized as service or advocacy and \\nnot considered legitimate in the perspectives of the research com-\\nmunity at large. Reactions such as recent eforts to silence and ban \\nconcepts such as “equity” and “critical race theory” threaten our \\nability to engage in the full body of work relevant to the intersection \\nof race and disability [ 123]. Finally, Williams et al. [ 154] highlight \\nthe importance of critique in their paper on counterventions, “a \\ncritical approach to research design that engages with community-\\ninformed counterargument in the production of empirical studies \\nthat imagine alternatives to normative intervention.” They illustrate \\nexamples of studies and critique research aims using two sets of \\ncriteria. The frst relates to landscape assessment and asks whether \\nthe researcher discusses “the political/historical tensions between \\nliterature and participant experiences and describe how they hope \\nto address it with the project”. The second relates to community \\ninput, and asks whether the project has been successfully oriented \\n(or re-oriented) to critically examine whether an intervention is \\n“devoted to participants’ desires without ulterior motive.” \\nBy learning from the frameworks described, researchers can \\nbuild a strong foundation for doing access work at the intersection \\nof race and disability. These frameworks can help support refection \\non where ableism and racial bias might infuence research ques-\\ntions, study design questions, analysis questions, and dissemination \\nconcerns. Our discussions around what it means to engage with \\nthe intersection of race and disability in HCI have been shaped \\nby reviewing this literature along with our own experiences as \\nresearchers. However, there are some unique considerations at the \\nintersection of race and disability. We highlight this with respect \\nto four research considerations. The frst two, formalization and \\nframing and scoping , relate to landscape assessment but also re-\\nquire an understanding of the literature and theory relevant to race, \\ndisability and its intersection. The next, methods , relates directly \\nto study design. Finally, writing and analysis relates to analysis \\nand parts of dissemination. \\n4.1 Stage 1: Formalization \\nIn this stage of research, researchers may defne what is meant \\nby race, disability, and their intersection, and use that to guide \\nwhat phenomena is studied. This is informed by theories that draw \\nseparately from race and disability scholarship (e.g., Critical Race \\nTheory [116] and [52] or Disability Studies [103]) as well as in-\\ntersectional frameworks and theories (e.g., DisCrit [32, 93, 128]).ASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nEngaging with disability in HCI has prominently looked at how \\nwe can use technology to make the world accessible for people \\nwith disabilities, or highlighting the exclusionary nature of existing \\ntechnologies and corresponding failures to meet access needs of \\npeople with disabilities (e.g., [ 10, 73, 99, 103]). Engaging with race \\nin HCI similarly explores the impact and adoption of technologies \\nby diferent racial groups, experiences with certain systems, and \\nhow these technologies may perpetuate harm against those with \\nmarginalized racial identities (e.g., [ 3, 61, 64, 78, 116, 138]). Engag-\\ning with the intersection of race and disability may involve framing \\nthe research more broadly than either construct individually, and \\nhighlight new avenues for research. \\nFor example, a study of disability in the school system might \\nfocus on a math tutoring system that helps to address negative \\nemotional behaviors [9]. A study of race in the school system might \\nfocus on cultural and language relevance in technologies that im-\\nprove learning [51]. A study that engages with the intersection of \\nrace and disability might start by identifying relevant theory in \\nboth domains and identify the inequities faced by students of color \\nwith disabilities. These may include 1– being more likely to be \\nlabeled as learning disabled, and less likely to receive an actual dis-\\nability diagnosis [17], and 2– facing school suspensions and other \\ndisciplinary actions for students of color with disabilities, that in \\nturn lead to students being held back, placed in the juvenile justice \\nsystem, or other outcomes [40, 94]. This intersectional analysis thus \\nsuggests a widening set of stakeholders involved in the study as \\nwell as a need to bring together systems that address how negative \\nemotions are read and understood with cultural relevance and an \\nunderstanding of the inadequacy of student labeling. \\n4.2 Stage 2: Framing and Scoping \\nThis is the stage of research where we decide what research ques-\\ntions are worth answering. To do this, the dynamics of knowledge \\nproduction need to be interrogated and reimagined, including who \\nis included in the process [3, 162]. In HCI, that has been realized \\nthrough a shift towards participatory research methods for accessi-\\nbility and action research [69]. The opportunity to engage in the \\ndefnition of research questions is a particularly powerful part of \\nthe research process where engaging diverse voices is important. \\nAs an example, an accessibility focused research project may \\nfocus on automated speech recognition and captioning technolo-\\ngies potential to support d/Deaf and hard-of-hearing individuals \\n(e.g., [ 13, 83, 89, 106]). A similar project in this space that engages \\nwith racial identity may explore the use of speech recognition tech-\\nnologies (such as Alexa) by speakers with diferent dialects (e.g., \\n[38, 66, 148, 151]). Engaging with the intersection of race and dis-\\nability brings up new research questions – how well do current \\ncaptioning systems capture diferent dialects? How well do speech \\nrecognition systems work for speakers of these dialects who also \\nhave deaf accents, stutters, or non-normative speech patterns? Dis-\\nability and race each ofer new dimensions to explore. How does the \\npotential for surveillance through captioning tech impact adoption \\nby those with multiple marginalized identities? How does inter-\\nnalized ableism shape how folks adapt/hack speech recognition \\ntechnologies for use? 4.3 Stage 3: Methods \\nThe next step in the research process is to identify the epistemo-\\nlogical foundations of the project and select and execute methods. \\nAs an example, research that engages with the construct of race \\nmay need to address insider/outsider dynamics, cultural relevance, \\nand power structures [58, 107]. An accessibility focused method-\\nological plan must additionally consider how research addresses \\naccessibility needs such as breaks, access to interpreters, and mul-\\ntiple modalities [100]. Researchers might also ask, do the selected \\nmethods provide room for the identifcation of root causes and the \\ndevelopment of insights based on lived experience [73]. Further, \\nthis is a continual, iterative process: When we venture into sensitive \\nspaces, we must repeatedly and consistently interrogate the work \\nwe do as we do it to identify potential unintended consequences and \\nnegative impacts of our interventions and interactions. Whether \\nthose consequences represent unexpected accessibility needs [100], \\ndifculties with recruiting or retaining participants, or deeper ex-\\npressions of bias and ableism, ongoing attention to their possibility \\nis essential to reacting to and addressing them, minimizing harm, \\nand maximizing the power and positive impact of the time gifted \\nby participants to the endeavor. \\nFor example, consider this sequence of studies (all by the same \\nauthors): First, a study of passively sensed behavior correlates of \\ndiscrimination experiences among students might start by talking \\nto a broad sample of students of diferent races and ethnicities, \\nuncovering changes in psychological state, physical activity, phone \\nuse and sleep [136]. However, the representation of people with \\ndisabilities in the study was only 1%, and in later iterations of the \\nsame study despite attempts to recruit, this only went up to an \\naverage of 10% [160], about half of the true representation at uni-\\nversities [90]. A deeper look at the study methods must question \\nthe root cause of these challenges, which might be recruitment, \\nretention due to inaccessible study design, or an unknown factor \\nand adjust the methods accordingly. Alternatively, a new approach \\nmight revisit the same questions in an interview format, as [164] \\ndo. This may in turn require returning to questions of framing and \\nscoping. For example, Zhang et al. [164] mention one example of \\nthe intersection of race and disability where a participant bene-\\nfted from ofce hours not only due to the ease with which they \\ncould zoom in on the whiteboard (they were low vision); but also \\nbecause they felt unsafe walking to in-person ofce hours due to \\ntheir race and gender. We present this as an example of a sequence \\nof articles that variously touch on race and disability, but never \\nfully engage with their intersection. Even just by touching on demo-\\ngraphics, however, they aptly illustrate the importance of iterating \\non methods to engage with diferent populations. \\n4.4 Stage 4: Analysis and Writing \\nThe fnal stage of a project involves integration and synthesis of the \\nwork, to carry the intersectional research questions and analysis \\ngoals that guided the research through to writing and analysis as \\nwell. Recognizing how researchers’ own identities and biases have \\nshaped the research process is one key aspect of this work. This \\nhas increasingly shown up in HCI literature as explicit positionality \\nstatements regarding ability and racial identities of authors. At \\na deeper level, this may look like interrogating power dynamicsWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\nand relations between researchers and participant communities or \\namongst the research team due to these identities. For example, in a \\nco-design efort with a local Black community group, Tran O’Leary \\net al. [145] found, upon refection that that the very methods being \\nstudied morphed during their analysis, where the researchers were \\n“thinking in terms of lines of ‘perfect’ alignment for the concrete \\nforms [while their collaborators] prioritized specifc people’s en-\\ngagements over an idealized design process.” While that study did \\nnot engage with disability, the importance of critical questioning of \\nassumptions and defnitions extends to intersectional work as well. \\nIn addition, even when research questions explored by a paper \\ndo not center the intersection of race and disability, there is still \\npotential for these constructs to come up in data and fndings (as \\nillustrated in the case above). Through analysis and writing, re-\\nsearchers have a chance to refect on biases and new dimensions of \\ninquiry. This may involve noting instances of ableism and racism \\nin collected data (e.g., participant interviews), as well as identifying \\nhistoric and systemic factors/biases and root causes that may be \\ncontributing/impacting research questions and fndings. While this \\nis often realized through positionality statements, Boveda and An-\\nnama [19] call for researchers to refect on “how they engage with \\nand communicate knowledge about multiply marginalized people” \\nthroughout the research process. Their three-pronged framework \\non the onto-epistemic, sociohistorical and sociocultural outlines \\nquestions for researchers to refect on throughout the research \\nprocess, from design to publication [19]. \\nAlthough our work does not dive into the fnal stage of Andrews’ \\nframework [3], it is worth noting that writing often must consider \\nmultiple audiences, including the communities that ofered their \\ntime for the research to take place. \\n4.5 Final Considerations \\nWhile we have endeavored to ofer some concrete examples for each \\nof the outlined stages, we do not mean to oversimplify the nuances \\nrelated to deep and meaningful engagement with social constructs \\nof race and disability. This engagement looks diferent for each \\nproject. We emphasize that researchers need to be responsive to \\nwhat the wants and needs of multiple marginalized communities \\nand what they say is right and important. Further, work in this space \\nnecessitates iteration based on learning, and constantly adapting \\nwith and to needs of the community. \\n5 CASE STUDIES \\nA prominent way in which HCI progresses in understanding, re-\\nfecting, and advancing our work can be attributed to case stud-\\nies. Researchers have used case studies to study various types of \\nintersectional work [6, 55], construct compelling narratives, and \\ncultivate a critical lens for HCI research. This process of refect-\\ning through a collection of existing work and projects surfaces \\nnew ways of thinking and research futures for the community \\n[12, 65, 103, 154]. Below we present a series of case studies of lit-\\nerature that exemplify engagement with both race and disability \\nthrough the course of research. \\nIn choosing our case studies, we focused on what it means to \\nengage with race and disability, the discussion of these two as social \\nconstructs, and the overlapping oppressions that populations face, amplifed by or experienced through interactions with technology. \\nIn addition to using keywords such as ‘Race’, ‘Disability’ and ‘Inter-\\nsectionality’, we used keyword descriptors of research approaches \\n(e.g., equity, inclusion, diverse) to guide our search across the ACM \\nDigital Library and Google Scholar. We read through articles fol-\\nlowing a scoping search to identify research questions outlined, \\nframeworks used, and engagement with constructs of race and dis-\\nability within the research article to choose our three case studies. \\nThis was not intended to be an exhaustive review, and we note the \\nfeld continues to introduce notable publications since this work \\nhas commenced. Through our selected case studies, we attempt to \\naddress a broad coverage of themes through examples that portray \\nelements of intersectionality based on existing frameworks. \\nThe case studies selected below comprise diferent domains of \\ntechnology and technology use. The choices of papers have no \\nintention of dissecting a particular group or type of disability, or \\nassessment and evaluation of technology’s role in the space, or \\nits usability. Our analysis examines the people, focus of the study, \\nand methods applied while contemplating how each facet of the \\nresearch process engages with constructs of race and disability \\nand undertakes intersectional approaches. Given these components \\nas core to our review, our search goes beyond the ASSETS/HCI \\nresearch sphere and its associations. Instantiation of these themes \\nare strongly represented in other felds and adjacent research com-\\nmunities we draw from. \\n5.1 Case Study 1: Negotiation Accessibility and \\n(Mis)Representation in Image Descriptions \\nBennett et al. [11] provide a landmark example in the discussion of \\nrace, gender, and disability around image descriptions, suggesting \\na need to critically assess tensions across these generally siloed \\ncategories of identity for accessibility research. This paper serves \\nas an imperative case study for HCI research as it expands upon an \\nintersectional lens by interviewing people at the nexus of identities \\n(racial minorities, gender, and disability) and provides analysis \\nacross these multiple dimensions of identity which are arguably \\nand frequently narrowly classifed. \\nThis paper reports on interviews with 25 screen reader users \\n(mix of totally blind, those with visual memories, and other visual \\nimpairments) who also identify as “Black, Indigenous, People of \\nColor, Non-binary, and/or Transgender,” and actively browsed social \\nmedia as their main criteria. The study focuses on how to describe \\nappearance – preferences for self-descriptions of identity; expe-\\nriences and concerns around misrepresentation by others and/or \\ntoo much description; interest in knowing others’ appearance; and \\nguidance for AI generated image descriptions. \\nWhile the goal of the work was to fgure out how to better de-\\nscribe appearance in image-descriptions, it recognizes that explicit \\nexpression of these parts of identities and their manifestations \\nmight not always be appropriate and necessary in all interactions \\nof content. People aren’t always seeking the same level of acknowl-\\nedgement, feedback, and detail in all scenarios and use cases. There-\\nfore, it is crucial to point out that there are certain contexts vs. \\nothers in which the interplay of race and disability might not be of \\nsignifcance or priority. However, if unknown, there are situationsASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nwhere making an assumption and erasing these facets of identity, \\n(e.g., by AI generated prediction) can be more harmful. \\nWhy this paper? Due to the nature of image description as an \\nassistive technology for people with vision impairments and the \\nlevel of access it might aford, populations that are more marginal-\\nized and minoritized typically take the burden of its harms. This \\npaper delves into these tradeofs between access and microaggres-\\nsions that skew these experiences with technology. The authors \\ngo beyond just reporting demographics data of race and ethnicity \\nof participants to actively comparing and contrasting the specifc \\nperspectives that emerged as a result of participants’ particular \\nidentity–how it afected their choices to use the technology, how \\nmuch of the burden of misrepresentation they had to endure, and \\nlevels of harm they had to tolerate. Taking a critical deeper dive \\ninto engaging them through potential benefts and harms, the paper \\ncalled for “intersectional ethical review of accessibility research.” \\nAnother nuance to note is the domain and type of technology \\nbeing discussed in this case study. While the paper immerses in \\nintersectional analysis and discussion across dimensions of race \\nand disability, it is signifcant to observe how race plays a role in the \\nfunctionality of the technology in question. Image descriptions by \\nnature require description of physical features and appearances that \\ncan allude to aspects of race itself. Therefore, even though the case \\nstudy examines what might be considered a more traditional form of \\naccessibility in supporting people with visual disabilities, it engages \\nwith the construct of race by challenging its conceptualization in \\nvisual vs. nonvisual ways of sensemaking, the power dynamics, and \\nthe historical validity associated with both. \\n5.2 Case Study 2: Designing for Intersectional, \\nInterdependent Accessibility \\nGonzales [55] outlines how an interdependent and intersectional \\napproach to translation work can help with creating accessible \\ndigital content. This work is directed towards a technical communi-\\ncation researcher audience who often negotiate making their works \\naccessible to a variety of audiences. By combining insights from \\ndisability studies and translational studies, it highlights how access \\npractices as well as cultural and racial practices infuence every \\nstage of research design, method, and dissemination, in the context \\nof the author’s prior work with communities of translators. Using \\nthese prior works to generate data narratives, along with her own \\nexperience as a bilingual immigrant, technical communicator, and \\ntranslator, the author’s refections showcase meaningful engage-\\nment with intersectional tenets through the course of research. \\nFrom 2014-2017, the author worked with communities of trans-\\nlators to learn how they used a variety of digital and non-digital \\nresources and practices to transform information across languages. \\nWhile her focus was initially on spoken/written language trans-\\nlation, she quickly realized that communication is an embodied \\nexperience wherein translators use any available mode to com-\\nmunicate (drawing fgures, texting, using their bodies, singing, \\ndancing). The embodied nature of this practice depends on issues \\nof access and dis/ability, and cannot be separated from material \\nconditions, histories, and experiences of translators and audiences \\nof translation. The translators she worked with often identifed as \\nimmigrants and with many dis/abilities, and so she noted how their identities infuenced their approach to their work. It was through \\ndisability studies work that she came to understand the intercon-\\nnectedness and interdependence of modalities, communities, and \\nhistories. By “threading disability studies’ ongoing attention to em-\\nbodiment, dexterity, and mobility with translation and language di-\\nversity scholars’ attunement to racial and cultural practices,” we can \\nreimagine access in technical communication research. In creating \\nvideo montages of these translation sites, she wanted to understand \\nhow her decisions regarding accessibility (such as adding captions) \\nimpacted the presentations of translanguaged information. Thus in \\nthis paper, she refects on her experiences designing and publishing \\nthat work, using data narratives to highlight interdependent and \\nintersectional considerations in creating accessible and bilingual \\ndigital content. \\nThe frst narrative, regarding creating bilingual captions, dis-\\ncusses tensions between translations and accessibility for audiences. \\nSubtitles usually ofer language translations for viewers from difer-\\nent languages, and captions ofer transcriptions of both speech and \\nnon-speech audio for d/Deaf and hard of hearing audiences. As the \\nauthor’s work focused on sites of translation, the videos contain a \\nmix of “Spanishes and Englishes and gestures”– the act of adding \\ncaptions and subtitles necessitates making assumptions about abil-\\nity and linguistic background of the audience, and balancing the \\nneed for access with the goal of showcasing linguistic tensions \\nthat arise during translation work. The second narrative, which \\nfocuses on technical skills and language competencies, talks about \\nthe need to acknowledge non-normative communication practices. \\nBy highlighting multimodality in their work, they had to further \\ninterrogate the impact of “dis/ability, power, agency and consent” \\non participants and surrounding communities. The third narrative, \\nregarding rights and representation, discusses her attempts to in-\\nclude translators in writing and analysis (moving from “about” to \\n“with”) and notions of reciprocity and consent and interdependence \\nin the shaping of any intellectual work. It concludes by setting \\ngoals for intersectional interdependent accessible content creation: \\nincluding designing for language fuidity, developing culturally \\nrelevant policies for digital publishing, and recognizing labor of \\nmultilingual content creation. \\nWhy this paper? The author’s refections on how frameworks of \\ninterdependence and intersectionality informed the methods used, \\nas well as decisions made through the research process demonstrate \\nwhat strong engagement with intersectionality can look like. For \\nexample, the choice to use video recordings in sites of translation \\nallowed her to capture multimodal, embodied communication, but \\nbrought up new considerations regarding comfort and consent \\nof participants, and the role of race, class, and gender in shaping \\npower dynamics. The resulting discussion of centering collective \\naccess and goals shaped data collection and which video data was \\ndisseminated. This also came up when she incorporated translators’ \\nperspectives through her work – by working with translators and \\nnot just writing about, she refected on reciprocity and giving back \\nto the community whose time she was using. This led to collectively \\ndeciding with participants to not anonymize data, and thus allow \\nthem to trace their contributions in resulting publications. The use \\nof data narratives through this analysis shows one of many possible \\nchoices to address needs of multiply marginalized – the constantWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\nacknowledgement of many possible ways of addressing these needs \\nand multiplicity is well done. \\nAdditionally, the phenomena studied and case made for dissemi-\\nnating research in a way that is accessible to those from diferent \\nlinguistic backgrounds and a variety of dis/abilities has direct im-\\nplications for the research community and how we move towards \\nengaging with race and disability at multiple scales and beyond \\nthe scope of a single project. The decisions she refected on are not \\nunique to a single project and often embedded in research publica-\\ntion and dissemination pipelines. By explicitly stating these choices, \\nwe can begin to interrogate how we may be reinforcing structures \\nof oppression. If we want to “purposely decenter standardized no-\\ntions of language, culture, and ability simultaneously” as Gonzales \\nstates, we need to recognize the critical considerations involved in \\nworking with multiply marginalized communities, AND the labor \\nof doing so well. This has value to prepare the research community \\nand professionals for this area of work. \\n5.3 Case Study 3: Understanding Socio-cultural \\nAccessibility Barriers for Refugees with \\nDisabilities in the US \\nHamidi et al. [59] examines the amplifed challenges surround-\\ning access to disability and healthcare services that refugees with \\ndisabilities face in host countries. Hamidi and colleagues report \\nfndings from semi-structured interviews with six experts who have \\nexperience serving marginalized refugees in the United States. The \\nauthors note that composed interview protocols were developed \\nbased on preliminary literature review. Their analysis reveals two \\nmajor categorizations of barriers to accessing disability and health-\\ncare resources: cultural factors and language factors. Further, their \\nanalysis examines the use of digital technologies by refugees with \\ndisabilities and raises opportunities for further development of tech-\\nnology solutions towards structural change in order to promote \\nculturally sensitive accessibility and healthcare resources. \\nTo reduce potential burdens that could emerge from the par-\\nticipation of refugee families with a member with disabilities, the \\nauthors capture the perspectives of experts who have experience \\nserving marginalized refugees in the United States. Three of the ex-\\nperts who have lived experiences of arriving in the United States as \\nrefugees themselves, highlight their personal navigation of cultural \\nand language barriers. The authors make note of this limitation, \\nas the presented recruitment strategy limits the comprehensive \\nunderstanding of refugees’ personal perspectives regarding accessi-\\nbility and access to healthcare services. The authors surface future \\ndirections for incorporating such perspectives into their work. \\nThrough the lens of cultural barriers, their fndings identify \\ncultural disconnects in the social interpretations of disability be-\\ntween refugees and their host countries– this is rooted in the difer-\\nent types and levels of stigma. Regarding language barriers, their \\nfndings identify two main contributing factors to the presence of \\ncommunication gaps: limited language support (lack of resources \\nfor refugees to overcome language barriers to accessing health-\\ncare and accessibility resources) and both refugees’ and providers’ \\nlimited uptake of services to overcome language difculties. The \\nauthors found that these highlighted cultural and language barri-\\ners between refugees and disability/health services contribute to misunderstandings, in turn, lending to mistrust, not having suf-\\ncient tools to address accessibility and healthcare concerns at this \\nintersection. The authors argue that this dilemma, “can be under-\\nstood from an intersectional perspective that places the experiences \\nof refugees with disabilities at the intersection of multiple over-\\nlapping categories of power relations, disability and immigration \\nstatus. Viewed from this perspective, it becomes clear that the re-\\nsources and training of experts serving only people with disabilities \\nor refugees will not be adequate to address the needs of people \\nwho are both refugees and have disabilities.” Hamidi et al.’s analysis \\ngoes on to examine current technologies used by refugees with \\ndisabilities and raise recommendations for future assistive technol-\\nogy solutions that build on these existing technologies available to \\nrefugees with disabilities. \\nWhy this paper? This work examines a phenomena that has been \\npreviously overlooked in accessibility scholarship– demonstrating \\nthe value in expanding the domain in which we do accessibility \\nresearch. This phenomena goes beyond traditional avenues to pro-\\nmote structural change in order to make accessibility and healthcare \\nresources more culturally sensitive for refugees with disabilities. \\nTheir usage of post-medical frameworks honors the social con-\\nceptualizations of disability and emphasizes the rights of people \\nwith disabilities, while also ofering room to recognize overlapping \\npower relations that impact the intersectional experiences of people \\nwith disabilities. The authors’ refections of these frameworks aid in \\ninforming their composed methods and analysis. When recruiting \\nthey were mindful of their target population. Thus, to reduce po-\\ntential burdens that could emerge from the participation of refugee \\nfamilies with a member with disabilities, the authors capture the \\nperspectives of experts who are experienced with serving refugees \\nin the United States. The authors also highlight lived experiences \\nof navigating personal cultural and language barriers, as three of \\nthe experts came to the United States as refugees themselves. \\nThrough their identifed cultural and language barriers, the au-\\nthors center the voices of experts who serve refugees in provid-\\ning recommendations of future assistive technology approaches \\nthat build on existing technologies (e.g., social networks, low-cost \\ncell phones) that are available to refugees. Their identifed cul-\\ntural and language barriers between refugees with disabilities and \\nservices directly inform technology recommendations. Examples \\nof recommendations raised by the participants include language \\nsupport/integration, improving the cultural training of healthcare \\nservice providers, training refugees on how to use technologies to \\nidentify resources, among others. This paper can serve to guide \\nfuture work to examine understudied accessibility topics through \\nan intersectional perspective. \\n5.4 Considering a Framework and Agenda for \\nRacexDisability in Accessibility Research \\nOur case studies exhibit the various points in a research endeavor \\nin which accessibility research may beneft from analyzing data \\nfrom the perspective of race and disability. Both disability and \\nrace are positioned as constructs that cause us to refect on equity, \\nequality, access and inclusion in our research practice. Frameworks \\nsuch as Intersectional HCI and Critical Race Theory in HCI have \\nprovided a critical foundation for such an analysis, and we buildASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nupon such frameworks to explicitly incorporate race as a construct \\ninto the consideration of disability work in technology-related \\nresearch. By analyzing these cases we aim to not just suggest this \\nmethodological undertaking as one of labor, but to push beyond the \\ncategorization of demographics and truly refect on and engage with \\nthese constructs. To this end, we discuss the overall representation \\nof these case studies against our proposed framework. \\n1– Formalization: All of the case studies examined include \\nsome form or formalization. Race is treated diferently in each pa-\\nper, in part because each paper looks at diferent social and technical \\ncontexts. For example, Bennett et al. [11] treat race as a “socioma-\\nterial system for categorizing people” based on cultural, behavioral \\nand physical traits because that is most relevant to how it trans-\\nlates to language and description, and draw on critical race theory \\nin their analysis. Hamidi et al [59] focus on and discuss ethnicity, \\nwhich is consistent with their participants, who do not necessarily \\nidentify according to U.S. concepts of race before they arrive in \\nthe US. In contrast, the immigrants in Case Study 2 (Gonzales et \\nal.) [55] have been part of the U.S. context for much longer and \\nrace manifests in the power structures impacting translation work. \\nDisability is also addressed through multiple frames. For exam-\\nple, Bennett et al [11] discuss disability in terms of “margins of \\nnormalcy”, meaning something that does not go along with normal-\\nizing expectations imposed by society. They draw from feminist \\ncrip theory and disability studies in their analysis. Gonzales et al. \\n[55] draw from interdependence frameworks and thus considers \\ndisabilities both in the consumers of transcription and translators’ \\nexperiences. Hamidi et al. [59] surfaces cultural factors that lead to \\nbarriers in accessing disability and healthcare services and refer-\\nences the social model and human rights model of disability. This \\nlatter model ofers room for the experiences of refugees with dis-\\nabilities who are also impacted by other facets of their identity. In \\nterms of topic, Bennet et al. [ 11] and Gonzales et al. [ 55] use their \\napproaches to critique problem spaces that are already central to \\naccessibility research, while Hamidi et al. [59] focus on a domain \\nthat is outside the scope of traditional accessibility research. Both \\nrepresent important benefts of doing intersectional work. \\n2– Framing: The infuence of the research questions being \\nasked is profound. For example, despite many works on image \\ndescription, Bennett et al. [11] is the frst to ask how we should de-\\nscribe people in images who experience the marginalization of their \\nimpairment and their racial identity. It is a refection of the depth \\nof Bennett et al.’s [11] engagement with intersectional approaches \\nthat they specifcally asked people of color who are screen reader \\nusers about their preferences for descriptions of self-identity and \\nappearance nonvisually in various contexts. Hamidi et al. [59] are \\nmotivated to better understand the amplifed cultural and language \\nbarriers that people with disabilities face, and to elicit structural \\nchange that supports their navigation of healthcare services and \\nthe technology that refugees with disabilities use to access these \\nservices. Their work uncovers a disconnect between refugees and \\nservice providers, stigma, language barriers and lack of resources \\nand uptake of services to address these difculties. Gonzales [55] \\naims to better understand the material conditions, histories, and \\ncultural and bodily experiences of translators with disabilities and \\nrefect on tensions in creating accessible, inclusive digital content for disseminating research to people with diferent linguistic back-\\ngrounds and disabilities. In each of these projects, we can see the \\ndeep impact of the intersectional frame on the questions being \\nasked. \\nEach of these papers also does work to connect intersectional \\nissues to the design of technology. For example Bennett et al.’s \\n[11] work has direct implications for the design of AI based image \\ndescription technologies. Hamidi’s work can impact the creation \\nof accessibility technologies aimed at supporting refugees. Gon-\\nzales’ work helps us to understand tools and practices that can \\nsupport creation and dissemination of accessible digital content by \\nresearchers. \\n3– Method: In refecting on the themes across these works, we \\nnote that the choice to work with populations marginalized along \\nvarious axes of identity directly impacted how the researchers \\nrecruited participants and the methods they chose. Recruitment \\nlimitations were noted in multiple papers. For example, Hamidi et \\nal. [59] discuss how, to reduce potential negative consequences and \\nburdens on participation for refugee families, they worked with \\nexperts who serve refugees, some of whom also had prior experi-\\nence as refugees themselves. The authors note this as a limitation, \\nciting a lack of personal perspectives from some of the experts, and \\ndiscuss the need to include those perspectives in the future. Bennet \\net al. [11] intentionally recruit for identities they know people have \\ndisclosed, take pride in, and are connected to through advocacy and \\ncommunity organizing spheres. The authors describe this as both \\na limitation and a necessary process for sampling. It is also worth \\nnoting that although none of the case studies formally used partici-\\npatory design, some of the specifc choices that these cases made \\nabout participant engagement refect a careful attention to topics \\nlike power, cost to stakeholders, credit, and so on. For example, Gon-\\nzales et al. [55] use video recordings to capture translators work to \\navoid prioritizing only “auralist ways” of knowing and being. In \\ndoing this, they also needed to consider power relations, privacy, \\nand disclosure that come with video recordings. They worked with \\nparticipants to make sure they could represent their work on their \\nown terms, including the impact of prior experiences on their work. \\nThey also discuss the rights and representation, collectively reach-\\ning a decision not to anonymize so that participants receive credit \\nfor the time they put into the paper. Bennett et al. [11] intentionally \\nfocus on stakeholders who are screen reader users themselves, in \\ncontrast to many prior papers who focus on describers. \\n4– Writing/Analysis: We fnd that each of these papers engage \\nthe interplay of race and disability into their conclusions, even \\nthough they difer in how they represent the identities of partic-\\nipants and their own positionality. Bennett et al. [11] go beyond \\nstandard positionality statements to actively expose personal per-\\nspectives where they are relevant, while being careful not to do \\nthis when a topic is not personally relevant. Before diving into the \\nanalysis and discussion of fndings, the authors designate space \\nto carefully discuss the identities of participants beyond reporting \\ndemographics tables, commenting on “shared disabilities” other \\nthan the one in focus, the relationship between gender and those \\nself-reporting as BIPOC, and those who have “white passing priv-\\nilege.” Hamidi et al. [59] surfaces the conceptualizations of over-\\nlapping power relations and imbalances that impact refugees withWorking at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\ndisabilities. They argue that barriers due to cultural and language \\ndisconnects between refugees with disabilities and providers can \\nbe understood from these power relations. Further, their analysis \\ndiscusses root causes of discrimination towards refugees, in turn, \\ninfuencing access on both the refugee and providers part. Gonza-\\nles [55] draws on their own background “as an immigrant, visibly \\nable bodied tech communicator and translator for the work”. The \\nauthors frequently talk about how visible and invisible dis/abilities \\nof participants infuenced their translation practices. Their work \\nhighlights tensions between linguistic/cultural access and disability \\naccess that has implications for how we might approach later stages \\nof race and disability informed research such as member checking \\nand dissemination [3, 81, 100]. \\n6 DISCUSSION \\nOur work on this project has taken almost three years to reach a \\npoint where we were ready to submit it. In the meantime, we have \\nseen a rapid increase in the attention this intersection is receiving, \\nwith over 40 of our references published in 2021 (the year we began \\nthe analysis of the included case studies) or later, including two of \\nour three case studies [11, 59]. In doing this work, we have learned \\nand grown together. \\nOur three case studies, while notably distinct in their domains \\nof inquiry and methodology, highlight the breadth of work that \\ncan be done when inspired to engage with the intersection of race \\nand disability, from new perspectives on technology work (such \\nas image annotation [11]) to domains entirely new to the feld \\nof accessibility (such as technology use among refugees [59], or \\nmutual aid [139]). The feld of accessibility has already begun to \\nmove past a focus on GUI and web accessibility, past access to next \\ngeneration technology such as AR/VR and intelligent agents, and \\ninto all of the spaces where people interact with technology. In \\nall of these spaces, there are people marginalized by both race or \\ndisability, and people at the intersection of both who deserve our \\nattention and will in turn lead us to new innovations and insights. \\nHCI research that has engaged intersectional considerations into \\nresearch practices [22, 54, 127], whether through project scope or \\nframing of analysis has paved a path for solidarity across HCI and \\naddress power relations in technology needs and access. While \\nprior work has highlighted the value of critique as one way to forge \\nalternatives to normative research practices [154], we see equal \\nvalue in highlighting the promise of this area through exemplary \\ncase studies. \\nFraming research at the intersection of race and disability can \\nalso lead to a broader set of meaningful perspectives being included \\n(i.e., stakeholder expansion) in research. As seen through [59], this \\nbrings up questions of how one might balance the inclusion of \\nfrst-person perspectives with burden to community, and whether \\none should work with other stakeholders instead. In doing this \\nwork, there are crucial considerations around continued engage-\\nment and reciprocity with the community in question (as seen in \\n[55]), without imposing technocentric biases and destabilizing as-\\nsumptions before intervening as highlighted by [11]. Defning race \\nand disability is central to engagement, since they shift in meaning \\nwith respect to culture, community, and social positioning. As dy-\\nnamic dimensions of identity, race and disability vary in how they determine technology use. While each of our case studies defne \\nrace and disability, there is variation in detail provided. It is impor-\\ntant to acknowledge that this may be for various reasons including \\nparticipant privacy or disclosure. Lastly, the constant refection \\nand interrogation of power relations cannot be understated. These \\nsurface across authors, within stakeholder and participant groups, \\nbetween technologies and communities, and through access and \\ncultural practices. \\nWith the lack of data, research, and reporting on intersectional \\ndisability work related to technology [23, 119, 130, 133], there needs \\nto be an increased amount of credibility and trust in situated knowl-\\nedge. This can be attributed to the dynamics of knowledge produc-\\ntion in disability, its embodied nature, epistemological bases, and \\nhow this information is bound to be translated to formal knowledge \\n[113]. What is traditionally viewed as empirical research and knowl-\\nedge sidelines and excludes much of the rich insight and meaning \\ndisability groups construct based on their lived experiences. This \\nhas been called to attention in the ASSETS community by Hofmann \\net. al [73], where they highlight three core observations: ableism, \\noversimplifcation of disability, and centering human connections \\nand relationships around disability. \\nOf course, we acknowledge that race does not have to be squarely \\nin the focus of every accessibility project or vice versa. Further, iden-\\ntity is often a fuid context, and a community of people may not \\nall share the same identity. For example, it would be a mistake to \\nassume that all older people are disabled [67, 87], just as it is a \\nmistake to assume all disabled people are white, or that all racial \\ngroups view disability the same way. We must go beyond implicit as-\\nsumption about who is present in our work to meaningfully engage \\nwith this intersection. It is also important to note the challenges \\nwith intersectional research design that must be considered. For \\nexample, moving away from additive thinking and incorporating in-\\ntersectional concepts into data collection may pose as a challenge to \\nsome in the accessibility space [157]. Based on our analysis we pro-\\nvide guiding principles to help establish and support this research \\narea as one that looks to amplify the experiences of individuals \\nwho sit at this intersection of disability and race as opposed to a \\nproblem-solution orientation: \\n1– Looking Beyond Academia. We learned during our seminar \\nthat much of the conversation taking place at the intersection of \\nrace and disability is notably happening outside of the ivory tower \\nof academia. Disability advocates and racial justice activists alike \\nhave taken to media platforms, blogs, and artistic expression to \\ncall attention to the particular experiences of racially minoritized \\nindividuals who have a disability. Engaging with this ongoing con-\\nversation means that citational justice, and noting what kinds of \\nknowledge production are valid and valued are ways to take action. \\nResearchers should look to activist-led projects and integrate these \\nworks into syllabi and invitations for course lectures. Teaching and \\nproject collaboration are spaces where we can look beyond the \\nacademy to expand our understanding of this intersection. \\n2– Reduce Assumptions of Participant Defaults. It has been shown \\nthrough various meta reviews that very few papers within the HCI \\naccessibility-related space report on the construct of race let alone \\nthe interplay of this construct and technology [67]. We must, as \\na feld, move beyond the default of leaving the reader to assume \\nthat the sample is white (or able bodied) without explicitly sayingASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\nso. Such defaults hinder the inclusiveness of the whole research \\nprocess. As such, we as a feld must work to educate ourselves about \\nrace and disability, because even if a project (or body of work) isn’t \\nengaged with race, it is imperative to recognize when intersectional \\nissues arise in a study or they will end up either on the cutting \\nboard or as a small side note. Researchers should debunk a lot of \\nthe standing defualts and defnitions that ignore the nuance and \\ncomplexitities of experiences of those that sit at the intersection of \\nracial identity and disability. We urge for more reporting of race as \\na demographic factor and the engagement with it as a construct in \\nterms of research design and analysis of fndings where appropriate. \\n3– Going Beyond Basics. While reporting (and collecting) data \\nis essential, we should beware of categorizing for categorization \\nsake and work to engage with race and disability meaningfully. \\nWhat is it about the race of participants in the sample, how does \\nthis social positioning impact perspective, how do their culturally \\nembedded experiences infuence their accessibility needs? Going \\nbeyond the basics pushes beyond the additive nature of listing \\nmutliplte dimensions of identity [126, 157]. Researchers should \\navoid skating around the social context of race and instead lean into \\nwhat this may tell us about phenomena of sociotechnical systems \\nand technology needs. Simply confating race to being synonymous \\nwith marginalization or underserved does a disjustice to the ways \\nracial identity may be at play. This also disproportionately paints \\nthe picture of certain groups as less than in our research feld. Often, \\nwhen we think of low socioeconomic status, we think Black/Brown, \\nthis is a false assumption which neglects the perspective of White \\npeople in this category. Further, the presumption that poor=Black \\nor Brown is itself inherently racist, but also skewed due to the \\ngeographic regions we are working in. We urge scholars to critically \\nconsider social dimensions of these dimensions and their relation \\nto accessibility. \\n4– Making Mistakes. While a part of our research practice is to \\nensure careful intention in our research questions, study design and \\nanalysis, as learners in this feld it is probably inevitable that we \\nwill make mistakes. Fear of these mistakes should not stop us from \\ndoing this work. Sometimes we need to just try and possibly fail fast, \\nas with the seminar. Other times, as with the long path to this paper \\nbeing complete, failure might look like slow progress. We chose, \\nstudied, and discarded two of our own papers and multiple other \\npapers before we settled on those presented here. Along the way we \\nfound our writing at times too critical, at other times we struggled \\nto connect our work to our stated focus. Making mistakes may be \\na vital part of developing a research agenda at this intersection, as \\na way to refne methods and ingenuitive research approaches. \\nConsiderations Going Forward. Existing frameworks call for \\ncontinued engagement with communities (e.g., [3]). This is critical, \\nbut continued engagement with representation in our community \\nis critical. Multiple of us have been engaged in varying ways with \\naddressing social justice within our own research community [91]. \\nHow we run our research groups, the volunteer work we do in \\nthe community, and how the research community operates as a \\nwhole all set the stage for an environment where engaging with \\nrace, disability, and intersectionality is a norm. It is through parts \\nof research (that maybe do not ft into the scope of a single project) \\nlike community building, dissemination, teaching, advocacy, and activism that we can dismantle structures of power and oppression, \\nand have a positive lasting impact on the communities we work \\nwith. Just as disability work done without the input of people with \\nfrst person experience of disability can be problematic, it has been \\nvaluable for our team to have representation from multiple people \\nwith varying experiences in disability, race, and their intersection. In \\ndoing this work, there is an obligation to make space for people who \\ncan speak for the community directly i.e. people with disabilities, \\nracially minoritized individuals, and people at the intersection. \\nIt is no surprise that some of the leading researchers who have \\nbegun to enter these spaces are also people who have personal \\nexperiences that make it impossible for them to ignore how our \\nexisting approaches silence and ignore common and consequential \\nexperiences. We must, as a feld, work to ensure that we create safe \\nspaces to increase representation of people with disabilities, Black \\nand Hispanic and other minoritized researchers, and people that \\nhave been historically disenfranchised. As Brewer [22] argues, “rad-\\nically centering intersectional voices that break binary boundaries \\nand decolonize racial hierarchies“ is one important path towards \\nmore meaningful engagement with intersectional work. \\n7 CONCLUSION \\nWe present case studies and accompanying framework to sug-\\ngest why examining the intersection of race and disability may \\nstrengthen accessibility research. There is great promise for accessi-\\nbility research to engage with this nexus in the research questions, \\nmethodology, analysis, and documentation of the work that we do. \\nIn this paper, we discuss a strong precedence of work that has led \\nto defning this research agenda for the ASSETS community, con-\\ntextualize race and disability in this area, and highlight work that \\nexemplifes this area. We encourage the accessibility community to \\nsee this as a starting point for future research engagements. \\nACKNOWLEDGMENTS \\nNSF EDA 2009977, the Center for Research and Education on Ac-\\ncessible Technology and Experiences (CREATE) and the Paul G. \\nAllen School of Computer Science and Engineering and Population \\nHealth. \\nREFERENCES \\n[1] Roshan Abraham. 2016. Migrant Surveillance: How the Federal Government \\nMonitors Asylum Seekers. Master’s thesis. The City University of New York. \\n[2] David Alexander, JC Gaillard, and Ben Wisner. 2012. Disability and disaster. \\nIn The Routledge handbook of hazards and disaster risk reduction. Routledge, \\nOxfordshire, England, UK, 413–423. \\n[3] Kristine Andrews, Jenita Parekh, and Shantai Peckoo. 2019. A guide to in-\\ncorporating a racial and ethnic equity perspective throughout the research \\nprocess. https://www.childtrends.org/publications/a-guide-to-incorporating-\\na-racial-and-ethnic-equity-perspective-throughout-the-research-process \\n[4] Ramtin Arablouei and Rund Abdelfatah. 2020. American Police. \\n[5] Moya Bailey and Izetta Autumn Mobley. 2019. Work in the intersections: A \\nblack feminist disability framework. Gender & Society 33, 1 (2019), 19–40. \\n[6] Joy Banks. 2014. Barriers and supports to postsecondary transition: Case studies \\nof African American students with disabilities. Remedial and Special Education \\n35, 1 (2014), 28–39. https://doi.org/10.1177/0741932513512209 \\n[7] Imani Barbarin. 2019. On Being Black and ‘Disabled But Not Re-\\nally.’. https://rewirenewsgroup.com/2019/07/26/on-being-black-and-disabled-\\nbut-not-really/ \\n[8] Samantha Bee. 2019. Stop Doing That! With Nyle DiMarco Full Frontal on TBS. \\nhttps://www.youtube.com/watch?v=hMr831Ro2-k \\n[9] Ruha Benjamin. 2020. Race after technology: Abolitionist tools for the new Jim \\ncode.Working at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\n[10] Cynthia L. Bennett, Erin Brady, and Stacy M. Branham. 2018. Interdependence \\nas a Frame for Assistive Technology Research and Design. In Proceedings of the \\n20th International ACM SIGACCESS Conference on Computers and Accessibility, \\nASSETS 2018, Galway, Ireland, October 22-24, 2018, Faustina Hwang, Joanna \\nMcGrenere, and David R. Flatla (Eds.). ACM, New York, NY, USA, 161–173. \\nhttps://doi.org/10.1145/3234695.3236348 \\n[11] Cynthia L Bennett, Cole Gleason, Morgan Klaus Scheuerman, Jefrey P Bigham, \\nAnhong Guo, and Alexandra To. 2021. “It’s Complicated”: Negotiating Accessibil-\\nity and (Mis) Representation in Image Descriptions of Race, Gender, and Disabil-\\nity. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Sys-\\ntems. ACM, New York, NY, USA, 1–19. https://doi.org/10.1145/3411764.3445498 \\n[12] Cynthia L Bennett and Daniela K Rosner. 2019. The promise of empathy: \\nDesign, disability, and knowing the\" other\". In Proceedings of the 2019 CHI \\nconference on human factors in computing systems. ACM, New York, NY, USA, \\n1–13. https://doi.org/10.1145/3290605.3300528 \\n[13] Larwan Berke, Christopher Caulfeld, and Matt Huenerfauth. 2017. Deaf and \\nHard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for \\nCaptioning One-on-One Meetings. In Proceedings of the 19th International ACM \\nSIGACCESS Conference on Computers and Accessibility, ASSETS 2017, Baltimore, \\nMD, USA, October 29 - November 01, 2017, Amy Hurst, Leah Findlater, and \\nMeredith Ringel Morris (Eds.). ACM, New York, NY, USA, 155–164. https: \\n//doi.org/10.1145/3132525.3132541 \\n[14] Patricia Berne, Aurora Levins Morales, David Langstaf, and Sins Invalid. 2018. \\nTen principles of disability justice. WSQ: Women’s Studies Quarterly 46, 1 (2018), \\n227–230. https://doi.org/10.1353/wsq.2018.0003 \\n[15] Jefrey W Bethel, Sloane C Burke, and Amber F Britt. 2013. Disparity in disaster \\npreparedness between racial/ethnic groups. Disaster Health 1, 2 (2013), 110–116. \\nhttps://doi.org/10.4161/dish.27085 \\n[16] Jefrey P Bigham, Raja Kushalnagar, Ting-Hao Kenneth Huang, Juan Pablo \\nFlores, and Saiph Savage. 2017. On how deaf people might use speech to control \\ndevices. In Proceedings of the 19th International ACM SIGACCESS Conference \\non Computers and Accessibility. ACM, New York, NY, USA, 383–384. https: \\n//doi.org/10.1145/3132525.3134821 \\n[17] Wanda J Blanchett. 2010. Telling it like it is: The role of race, class, & culture in \\nthe perpetuation of learning disability as a privileged category for the white \\nmiddle class. Disability Studies Quarterly 30, 2 (2010). https://doi.org/10.18061/ \\ndsq.v30i2.1233 \\n[18] Stella Bolaki. 2011. . Michigan State University Press, East Lansing, MI, Chap-\\nter Challenging invisibility, making connections: Illness, survival, and Black \\nstruggles in Audre Lorde’s work, 47–74. \\n[19] Mildred Boveda and Subini Ancy Annamma. 2023. Beyond making a state-\\nment: An intersectional framing of the power and possibilities of position-\\ning. Educational Researcher 52 (2023), 0013189X231167149. Issue 5. https: \\n//doi.org/10.3102/0013189X231167149 \\n[20] Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick Boudreault, \\nAnnelies Brafort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorri, Tessa \\nVerhoef, et al. 2019. Sign language recognition, generation, and translation: \\nAn interdisciplinary perspective. In Proceedings of the 21st International ACM \\nSIGACCESS Conference on Computers and Accessibility. ACM, New York, NY, \\nUSA, 16–31. https://doi.org/10.1145/3308561.3353774 \\n[21] Kirsten Bray and Christina Harrington. 2021. Speculative blackness: considering \\nAfrofuturism in the creation of inclusive speculative design probes. In Designing \\nInteractive Systems Conference 2021. Association for Computing Machinery, New \\nYork, NY, USA, 1793–1806. https://doi.org/10.1145/3461778.3462002 \\n[22] Johanna Brewer. 2022. Playing Unbound: Towards a Radically Intersectional \\nHCI. In Extended Abstracts of the 2022 Annual Symposium on Computer-Human \\nInteraction in Play. ACM, New York, NY, USA, 270–272. https://doi.org/10.1145/ \\n3505270.3558362 \\n[23] Aurora H Brinkman, Gianna Rea-Sandin, Emily M Lund, Olivia M Fitzpatrick, \\nMichaela S Gusman, and Cassandra L Boness. 2022. Shifting the discourse on \\ndisability: Moving to an inclusive, intersectional focus. American Journal of \\nOrthopsychiatry 93 (2022), 50–62. Issue 1. \\n[24] Lydia X. Y. Brown. 2020. How to centre disability in the tech response to \\nCovid-19. \\n[25] Brenda Jo Brueggemann and Debra Moddelmog. 2002. Coming-out pedagogy: \\nRisking identity in language and literature classrooms. Pedagogy 2, 3 (2002), \\n311–335. \\n[26] Erin A Cech. 2022. The intersectional privilege of white able-bodied heterosexual \\nmen in STEM. Science Advances 8, 24 (2022), eabo1558. \\n[27] Alex Chen. 2020. How to write an image description. https://www.youtube. \\ncom/watch?v=hMr831Ro2-k \\n[28] N. Jamila Chisholm. 2020. To Be BIPOC, Disabled and Fighting for Justice. \\nhttps://colorlines.com/article/be-bipoc-disabled-and-fghting-justice/ \\n[29] Corey Clarke. 2017. 3D Printing Prostethics for Amputees in Haiti. \\nhttps://3dprintingindustry.com/news/3d-printing-prosthetics-amputees-\\nhaiti-102132/ \\n[30] Ta-Nehisi Coates. 2013. What we mean when we say ‘race is a social construct.’. \\nThe Atlantic 15 (2013). [31] Patricia Hill Collins and Sirma Bilge. 2016. Intersectionality (Key Concepts). \\nPolity Press, Cambridge, United Kingdom. \\n[32] DJ Connor, BA Ferri, and S Annamma. 2016. DisCrit: Critical conversations \\nacross race, class, & dis/ability. \\n[33] Lucy Costa, Jijian Voronka, Danielle Landry, Jenna Reid, Becky Mcfarlane, David \\nReville, and Kathryn Church. 2012. “Recovering our stories”: A small act of \\nresistance. Studies in Social Justice 6, 1 (2012), 85–101. \\n[34] Kate Crawford. 2016. Artifcial intelligence’s white guy problem. The New York \\nTimes 25, 06 (2016), 5. \\n[35] Kimberle Crenshaw. 1990. Mapping the margins: Intersectionality, identity \\npolitics, and violence against women of color. Stan. L. Rev. 43 (1990), 1241. \\n[36] Patrisse Cullors et al. 2018. When they call you a terrorist: A black lives matter \\nmemoir. St. Martin’s Press, New York, NY, USA. \\n[37] Jay Cunningham, Gabrielle Benabdallah, Daniela Rosner, and Alex Taylor. \\n2023. On the grounds of solutionism: Ontologies of blackness and HCI. \\nACM Transactions on Computer-Human Interaction 30, 2 (2023), 1–17. https: \\n//doi.org/10.1145/3557890 \\n[38] Jay L. Cunningham. 2023. Collaboratively Mitigating Racial Disparities in Auto-\\nmated Speech Recognition and Language Technologies with African American \\nEnglish Speakers: Community-Collaborative and Equity-Centered Approaches \\nToward Designing Inclusive Natural Language Systems. In Extended Abstracts \\nof the 2023 CHI Conference on Human Factors in Computing Systems, CHI EA \\n2023, Hamburg, Germany, April 23-28, 2023, Albrecht Schmidt, Kaisa Väänänen, \\nTesh Goyal, Per Ola Kristensson, and Anicia Peters (Eds.). ACM, New York, NY, \\nUSA, 484:1–484:5. https://doi.org/10.1145/3544549.3577057 \\n[39] Jennifer S Dargin, Chao Fan, and Ali Mostafavi. 2021. Vulnerable populations \\nand social media use in disasters: Uncovering the digital divide in three major \\nUS hurricanes. International Journal of Disaster Risk Reduction 54 (2021), 102043. \\n[40] Angela Y Davis. 2014. Deepening the debate over mass incarceration. Socialism \\nand Democracy 28, 3 (2014), 15–23. \\n[41] Tawanna R. Dillahunt, Sheena Lewis Erete, Roxana Galusca, Aarti Israni, \\nDenise C. Nacu, and Phoebe Sengers. 2017. Refections on Design Methods \\nfor Underserved Communities. In Proceedings of the 2017 ACM Conference on \\nComputer Supported Cooperative Work and Social Computing, CSCW 2017, Port-\\nland, OR, USA, February 25 - March 1, 2017, Companion Volume, Charlotte P. Lee, \\nSteven E. Poltrock, Louise Barkhuus, Marcos Borges, and Wendy A. Kellogg \\n(Eds.). ACM, New York, NY, USA, 409–413. https://doi.org/10.1145/3022198. \\n3022664 \\n[42] Tawanna R. Dillahunt, Matthew Garvin, Marcy Held, and Julie Hui. 2021. Impli-\\ncations for Supporting Marginalized Job Seekers: Lessons from Employment \\nCenters. Proceedings of the ACM on Human-Computer Interaction 5, CSCW2 \\n(2021), 1–24. \\n[43] Tawanna R. Dillahunt, Aarti Israni, Alex Jiahong Lu, Mingzhi Cai, and Joey \\nChiao-Yin Hsiao. 2021. Examining the Use of Online Platforms for Employment: \\nA Survey of US Job Seekers. In Proceedings of the 2021 CHI Conference on Human \\nFactors in Computing Systems. ACM, New York, NY, USA, 1–23. \\n[44] Kirsten Donato. 2018. National Minority Health Month: The Double Burden for \\nMinorities with Disabilities. https://www.naccho.org/blog/articles/national-\\nminority-health-month-the-double-burden-for-minorities-with-disabilities \\n[45] William Easley, Foad Hamidi, Wayne G Lutters, and Amy Hurst. 2018. Shifting \\nexpectations: Understanding youth employees’ handofs in a 3D print shop. \\nProceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–23. \\n[46] Emory James Edwards, Cella Monet Sum, and Stacy M Branham. 2020. Three \\ntensions between personas and complex disability identities. In Extended ab-\\nstracts of the 2020 CHI conference on human factors in computing systems. ACM, \\nNew York, NY, USA, 1–9. \\n[47] Sheena Erete, Yolanda A. Rankin, and Jakita O Thomas. 2021. I can’t breathe: \\nRefections from Black women in CSCW and HCI. Proceedings of the ACM on \\nHuman-Computer Interaction 4, CSCW3 (2021), 1–23. \\n[48] Nirmala Erevelles. 2014. Disability Incarcerated: Imprisonment and disability in \\nthe United States and Canada. Springer, Palgrave Macmillan, New York, Chapter \\nCrippin’Jim Crow: Disability, dis-location, and the school-to-prison pipeline, \\n81–99. \\n[49] Nel Escher and Nikola Banovic. 2020. Exposing Error in Poverty Management \\nTechnology: A Method for Auditing Government Benefts Screening Tools. \\nProceedings of the ACM on Human-Computer Interaction 4, CSCW1 (2020), 1–20. \\n[50] Heather D Evans. 2019. ‘Trial by fre’: forms of impairment disclosure and \\nimplications for disability identity. Disability & Society 34, 5 (2019), 726–746. \\n[51] Samantha L. Finkelstein, Evelyn Yarzebinski, Callie Vaughn, Amy Ogan, and \\nJustine Cassell. 2013. The Efects of Culturally Congruent Educational Tech-\\nnologies on Student Achievement. In Artifcial Intelligence in Education - 16th \\nInternational Conference, AIED 2013, Memphis, TN, USA, July 9-13, 2013. Pro-\\nceedings (Lecture Notes in Computer Science, Vol. 7926), H. Chad Lane, Kalina \\nYacef, Jack Mostow, and Philip I. Pavlik (Eds.). Springer, New York, 493–502. \\nhttps://doi.org/10.1007/978-3-642-39112-5_50 \\n[52] David Gillborn. 2015. Intersectionality, Critical Race Theory, and the Primacy \\nof Racism: Race, Class, Gender, and Disability in Education. Qualitative Inquiry \\n21, 3 (2015), 277–287. https://doi.org/10.1177/1077800414557827ASSETS ’23, October 22–25, 2023, New York, NY, USA Harrington, et al. \\n[53] Tina Goethals, Elisabeth De Schauwer, and Geert Van Hove. 2015. Weaving \\nIntersectionality into Disability Studies Research: Inclusion, Refexivity and \\nAnti-Essentialism. DiGeSt. Journal of Diversity and Gender Studies 2, 1-2 (2015), \\npp. 75–94. https://www.jstor.org/stable/10.11116/jdivegendstud.2.1-2.0075 \\n[54] Gerard Goggin and Karen Soldatić. 2022. Automated decision-making, digital \\ninclusion and intersectional disabilities. New Media & Society 24, 2 (2022), \\n384–400. https://doi.org/10.1177/14614448211063173 \\n[55] Laura Gonzales. 2019. Designing for intersectional, interdependent accessibility: \\nA case study of multilingual technical content creation. Communication Design \\nQuarterly Review 6, 4 (2019), 35–45. \\n[56] Nanette Goodman, Michael Morris, and Kelvin Boston. 2017. Financial inequality: \\ndisability, race and poverty in America. Technical Report. National Disability \\nInstitute. \\n[57] Siobahn Day Grady, Pamela Wisniewski, Ron Metoyer, Pamela Gibbs, Karla \\nBadillo-Urquiola, Salma Elsayed-Ali, and Eiad Yaf. 2020. Addressing institu-\\ntional racism within initiatives for SIGCHI’s diversity and inclusion. \\n[58] Yasmin Gunaratnam. 2003. Researching ’race’ and ethnicity: Methods, knowledge \\nand power. Sage, London. \\n[59] Foad Hamidi and Zulekha Karachiwalla. 2022. \"I’m ok because I’m alive\": \\nunderstanding socio-cultural accessibility barriers for refugees with disabilities \\nin the US. In W4A’22: 19th Web for All Conference, Lyon, France, April 25 - 26, \\n2022, Dragan Ahmetovic and Victoria Yaneva (Eds.). ACM, New York, NY, USA, \\n26:1–26:11. https://doi.org/10.1145/3493612.3520446 \\n[60] Aimi Hamraie and Kelly Fritsch. 2019. Crip technoscience manifesto. Catalyst: \\nFeminism, Theory, Technoscience 5, 1 (2019), 1–33. \\n[61] David Hankerson, Andrea R. Marshall, Jennifer Booker, Houda el Mimouni, \\nImani Walker, and Jennifer A. Rode. 2016. Does Technology Have Race?. In \\nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems, \\nSan Jose, CA, USA, May 7-12, 2016, Extended Abstracts, Jofsh Kaye, Allison Druin, \\nClif Lampe, Dan Morris, and Juan Pablo Hourcade (Eds.). ACM, New York, NY, \\nUSA, 473–486. https://doi.org/10.1145/2851581.2892578 \\n[62] Karen Hao. 2018. Can You Make an AI That Isn’t Ableist? https: \\n//www.technologyreview.com/2018/11/28/1797/can-you-make-an-ai-that-\\nisnt-ableist/ \\n[63] Donna Haraway. 1988. Situated knowledges: The science question in feminism \\nand the privilege of partial perspective. Feminist studies 14, 3 (1988), 575–599. \\n[64] Christina Harrington, Daniela Rosner, Alex Taylor, and Mikael Wiberg. 2021. \\nEngaging race in HCI. Interactions 28, 5 (2021), 5–5. \\n[65] Christina N Harrington, Katya Borgos-Rodriguez, and Anne Marie Piper. 2019. \\nEngaging low-income African American older adults in health discussions \\nthrough community-based design workshops. In Proceedings of the 2019 chi \\nconference on human factors in computing systems. ACM, New York, NY, USA, \\n1–15. \\n[66] Christina N. Harrington, Radhika Garg, Amanda Woodward, and Dimitri \\nWilliams. 2022. “It’s Kind of Like Code-Switching”: Black Older Adults’ Experi-\\nences with a Voice Assistant for Health Information Seeking. In Proceedings of \\nthe 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, \\nLA, USA) (CHI ’22). Association for Computing Machinery, New York, NY, USA, \\nArticle 604, 15 pages. https://doi.org/10.1145/3491102.3501995 \\n[67] Christina N. Harrington, Aqueasha Martin-Hammond, and Kirsten E. Bray. \\n2022. Examining Identity as a Variable of Health Technology Research for Older \\nAdults: A Systematic Review. In CHI ’22: CHI Conference on Human Factors in \\nComputing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Simone \\nD. J. Barbosa, Clif Lampe, Caroline Appert, David A. Shamma, Steven Mark \\nDrucker, Julie R. Williamson, and Koji Yatani (Eds.). ACM, New York, NY, USA, \\n265:1–265:24. https://doi.org/10.1145/3491102.3517621 \\n[68] Jasmine E Harris. 2021. Reckoning with race and disability. Yale Law Journal \\nForum+ H2332 130 (2021), 916–958. \\n[69] Gillian R Hayes. 2011. The relationship of action research to human-computer \\ninteraction. ACM Transactions on Computer-Human Interaction (TOCHI) 18, 3 \\n(2011), 1–20. \\n[70] Laurie Henneborn. 2021. Make It Safe for Employees to Disclose Their Disabil-\\nities. https://hbr.org/2021/06/make-it-safe-for-employees-to-disclose-their-\\ndisabilities \\n[71] Shawn Lawton Henry. 2022. How to write an image description. \\n[72] Marc Lamont Hill. 2016. Nobody: Casualties of America’s war on the vulnerable, \\nfrom Ferguson to Flint and beyond. Simon and Schuster, New York, NY. \\n[73] Megan Hofmann, Devva Kasnitz, Jennifer Mankof, and Cynthia L Bennett. \\n2020. Living disability theory: Refections on access, research, and design. In \\nProceedings of the 22nd International ACM SIGACCESS Conference on Computers \\nand Accessibility. ACM, New York, NY, 1–13. \\n[74] Daudet Ilunga Tshiswaka, Shondra Loggins Clay, Chung-Yi Chiu, Reginald \\nAlston, and Allen Lewis. 2016. Assistive technology use by disability type \\nand race: Exploration of a population-based health survey. Disability and \\nRehabilitation: Assistive Technology 11, 2 (2016), 124–132. \\n[75] Rooted in Rights. 2016. The Right to be Rescued (Film). https://rootedinrights. \\norg/video/therighttoberescued/ [76] National Disability Institute. 2020. Race, ethnicity and disability: The fnancial \\nimpact of systemic inequality and intersectionality. Technical Report. National \\nDisability Institute. \\n[77] Sins Invalid. 2020. What is disability justice? Sins Invalid. June 16 (2020), 2020. \\n[78] Michelle Jarman. 2012. . Michigan State University Press, Lansing, MI, Chapter \\nComing Up from Underground: Uneasy Dialogues at the Intersection of Race, \\nMental Illness, and Disability Studies, 9–29. \\n[79] Jazette Johnson, Vitica Arnold, Anne Marie Piper, and Gillian R. Hayes. 2022. \"It’s \\na lonely disease\": Cultivating Online Spaces for Social Support among People \\nLiving with Dementia and Dementia Caregivers. Proc. ACM Hum. Comput. \\nInteract. 6, CSCW2 (2022), 1–27. https://doi.org/10.1145/3555133 \\n[80] Susan R Jones. 1996. Toward inclusive theory: Disability as social construction. \\nNASPA journal 33, 4 (1996), 347–354. \\n[81] Kazi Sinthia Kabir, Ahmad Alsaleem, and Jason Wiese. 2021. The Impact of \\nSpinal Cord Injury on Participation in Human-Centered Research. In Designing \\nInteractive Systems Conference 2021. Association for Computing Machinery, New \\nYork, NY, USA, 1902–1914. \\n[82] Alison Kafer. 2013. Feminist, Queer, Crip. Indiana University Press, IN. http: \\n//www.jstor.org/stable/j.ctt16gz79x \\n[83] Sushant Kafe and Matt Huenerfauth. 2017. Evaluating the usability of auto-\\nmatically generated captions for people who are deaf or hard of hearing. In \\nProceedings of the 19th International ACM SIGACCESS Conference on Computers \\nand Accessibility. Association for Computing Machinery, New York, NY, USA, \\n165–174. \\n[84] Naveena Karusala, Jennifer Wilson, Phebe Vayanos, and Eric Rice. 2019. Street-\\nlevel realities of data practices in homeless services provision. Proceedings of \\nthe ACM on Human-Computer Interaction 3, CSCW (2019), 1–23. \\n[85] Elizabeth Kaziunas, Mark S Ackerman, and Tifany CE Veinot. 2013. Localizing \\nchronic disease management: Information work and health translations. Pro-\\nceedings of the American Society for Information Science and Technology 50, 1 \\n(2013), 1–10. \\n[86] Reuben Kirkham. 2021. Why Disability Identity Politics in Assistive Technolo-\\ngies Research Is Unethical. In International Conference on the Ethical and Social \\nImpact of ICT. Universidad de La Rioja, Universidad de La Rioja, Logroño, Spain, \\n475–487. \\n[87] Bran Knowles, Vicki L Hanson, Yvonne Rogers, Anne Marie Piper, Jenny Way-\\ncott, Nigel Davies, Aloha Hufana Ambe, Robin N Brewer, Debaleena Chattopad-\\nhyay, Marianne Dee, et al. 2021. The harm in confating aging with accessibility. \\nCommun. ACM 64, 7 (2021), 66–71. \\n[88] Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion \\nMengesha, Connor Toups, John R Rickford, Dan Jurafsky, and Sharad Goel. 2020. \\nRacial disparities in automated speech recognition. Proceedings of the National \\nAcademy of Sciences 117, 14 (2020), 7684–7689. \\n[89] Raja S Kushalnagar, Walter S Lasecki, and Jefrey P Bigham. 2014. Accessibility \\nevaluation of classroom captions. ACM Transactions on Accessible Computing \\n(TACCESS) 5, 3 (2014), 1–24. \\n[90] David Leake. 2015. Problematic Data on How Many Students in Postsecondary \\nEducation Have a Disability. Journal of Postsecondary Education and Disability \\n28, 1 (2015), 73–87. \\n[91] Debora de Castro Leal, Angelika Strohmayer, and Max Krüger. 2021. On ac-\\ntivism and academia: Refecting together and sharing experiences among critical \\nfriends. In Proceedings of the 2021 CHI Conference on Human Factors in Comput-\\ning Systems. Association for Computing Machinery, New York, NY, USA, 1–18. \\nhttps://doi.org/10.1145/3411764.3445263 \\n[92] Colin Lecher. 2018. What Happens When An Algorithm Cuts Your Health Care. \\nThe Verge (Mar 2018). \\n[93] Zeus Leonardo and Alicia A Broderick. 2011. Smartness as property: A critical \\nexploration of intersections between whiteness and disability studies. Teachers \\nCollege Record 113, 10 (2011), 2206–2232. \\n[94] CE Lhamon, P Timmons-Goodson, DP Abegbile, GL Herriot, PN Kirsanow, D \\nKladney, K Narasaki, and M Yaki. 2019. Beyond suspensions: Examining school \\ndiscipline policies and connections to the school-to-prison pipeline for students \\nof color with disabilities. \\n[95] Calvin A Liang, Sean A Munson, and Julie A Kientz. 2021. Embracing four \\ntensions in human-computer interaction research with marginalized people. \\nACM Transactions on Computer-Human Interaction (TOCHI) 28, 2 (2021), 1–47. \\n[96] Jef Link. 2020. Why Racial Bias Still Haunts Speech-Recognition AI. https: \\n//builtin.com/artifcial-intelligence/racial-bias-speech-recognition-systems \\n[97] Kevin M. Storer and Stacy M. Branham. 2021. Deinstitutionalizing Independence: \\nDiscourses of disability and housing in accessible computing. In Proceedings of \\nthe 23rd International ACM SIGACCESS Conference on Computers and Accessibil-\\nity. ACM, New York, NY, USA, 1–14. \\n[98] Kelly Mack, Rai Ching Ling Hsu, Andrés Monroy-Hernández, Brian A. Smith, \\nand Fannie Liu. 2023. Towards Inclusive Avatars: Disability Representation in \\nAvatar Platforms. In Proceedings of the 2023 CHI Conference on Human Factors in \\nComputing Systems (Hamburg, Germany) (CHI ’23). Association for Computing \\nMachinery, New York, NY, USA, Article 607, 13 pages. https://doi.org/10.1145/ \\n3544548.3581481Working at the Intersection of Race, Disability, and Accessibility ASSETS ’23, October 22–25, 2023, New York, NY, USA \\n[99] Kelly Mack, Emma McDonnell, Dhruv Jain, Lucy Lu Wang, Jon E. Froehlich, \\nand Leah Findlater. 2021. What Do We Mean by \"Accessibility Research\"?: A \\nLiterature Survey of Accessibility Papers in CHI and ASSETS from 1994 to 2019. \\nIn CHI ’21: CHI Conference on Human Factors in Computing Systems, Virtual \\nEvent / Yokohama, Japan, May 8-13, 2021, Yoshifumi Kitamura, Aaron Quigley, \\nKatherine Isbister, Takeo Igarashi, Pernille Bjørn, and Steven Mark Drucker \\n(Eds.). ACM, New York, NY, USA, 371:1–371:18. https://doi.org/10.1145/3411764. \\n3445412 \\n[100] Kelly Mack, Emma McDonnell, Venkatesh Potluri, Maggie Xu, Jailyn Zabala, \\nJefrey Bigham, Jennifer Mankof, and Cynthia L. Bennett. 2022. Anticipate \\nand Adjust: Cultivating Access in Human-Centered Methods. In CHI ’22: CHI \\nConference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 \\nApril 2022 - 5 May 2022, Simone D. J. Barbosa andClif Lampe, Caroline Appert, \\nDavid A. Shamma, Steven Mark Drucker, Julie R. Williamson, and Koji Yatani \\n(Eds.). ACM, New York, NY, USA, 603:1–603:18. https://doi.org/10.1145/3491102. \\n3501882 \\n[101] Kelly Mack, Emma J. McDonnell, Leah Findlater, and Heather D. Evans. 2022. \\nChronically Under-Addressed: Considerations for HCI Accessibility Practice \\nwith Chronically Ill People. In Proceedings of the 24th International ACM SIGAC-\\nCESS Conference on Computers and Accessibility, ASSETS 2022, Athens, Greece, \\nOctober 23-26, 2022, Jon Froehlich, Kristen Shinohara, and Stephanie Ludi (Eds.). \\nACM, New York, NY, USA, 9:1–9:15. https://doi.org/10.1145/3517428.3544803 \\n[102] Jennifer Mankof. 2020. A Challenging Response. https://interactions.acm.org/ \\nblog/view/a-challenging-response \\n[103] Jennifer Mankof, Gillian R. Hayes, and Devva Kasnitz. 2010. Disability stud-\\nies as a source of critical inquiry for the feld of assistive technology. In Pro-\\nceedings of the 12th International ACM SIGACCESS Conference on Computers \\nand Accessibility, ASSETS 2010, Orlando, FL, USA, October 25 - 27, 2010, Ar-\\nmando Barreto and Vicki L. Hanson (Eds.). ACM, New York, NY, USA, 3–10. \\nhttps://doi.org/10.1145/1878803.1878807 \\n[104] Jennifer Mankof, Devva Kasnitz, Disability Studies, L. Jean Camp, Jonathan \\nLazar, and Harry Hochheiser. 2022. Areas of Strategic Visibility: Disability Bias \\nin Biometrics. (2022), 10 pages. arXiv:2208.04712 https://doi.org/10.48550/arXiv. \\n2208.04712 RFI Response to the Science and Technology Policy Ofce’s request \\nfor “Information on Public and Private Sector Uses of Biometric Technologies”. \\n[105] Nina Markl. 2022. Language variation and algorithmic bias: understanding \\nalgorithmic bias in British English automatic speech recognition. In FAccT \\n’22: 2022 ACM Conference on Fairness, Accountability, and Transparency, Seoul, \\nRepublic of Korea, June 21 - 24, 2022. ACM, New York, NY, USA, 521–534. https: \\n//doi.org/10.1145/3531146.3533117 \\n[106] Emma J McDonnell, Ping Liu, Steven M Goodman, Raja Kushalnagar, Jon E \\nFroehlich, and Leah Findlater. 2021. Social, environmental, and technical: Fac-\\ntors at play in the current use and future design of small-group captioning. \\nProceedings of the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1–25. \\n[107] Sharan B Merriam, Juanita Johnson-Bailey, Ming-Yeh Lee, Youngwha Kee, Gabo \\nNtseane, and Mazanah Muhamad. 2001. Power and positionality: Negotiating \\ninsider/outsider status within and across cultures. International journal of \\nlifelong education 20, 5 (2001), 405–416. \\n[108] Mara Mills and Meredith Whittaker. 2019. Disability, Bias, and AI. Technical \\nReport. NYU Center for Disability Studies. \\n[109] Carrie Elizabeth Mulderink. 2020. The emergence, importance of# Disability-\\nTooWhite hashtag. Disability Studies Quarterly 40, 2 (2020). \\n[110] Michael J. Muller and Sacha Chua. 2012. Brainstorming for Japan: rapid dis-\\ntributed global collaboration for disaster response. In CHI Conference on Human \\nFactors in Computing Systems, CHI ’12, Austin, TX, USA - May 05 - 10, 2012, \\nJoseph A. Konstan, Ed H. Chi, and Kristina Höök (Eds.). ACM, New York, NY, \\nUSA, 2727–2730. https://doi.org/10.1145/2207676.2208668 \\n[111] Mike Nellis. 2009. Surveillance and confnement: Explaining and understand-\\ning the experience of electronically monitored curfews. European Journal of \\nProbation 1, 1 (2009), 41–65. \\n[112] Kim E Nielsen. 2012. A disability history of the United States. Vol. 2. Beacon \\nPress, Boston. \\n[113] Greg Nijs and Ann Heylighen. 2015. Turning disability experience into exper-\\ntise in assessing building accessibility: A contribution to articulating disability \\nepistemology. Alter 9, 2 (2015), 144–156. \\n[114] Janet Njelesani, Shaun Cleaver, Myroslava Tataryn, and Stephanie Nixon. 2012. \\n. IntechOpen, United Kingdom, Chapter Using a human rights-based approach \\nto disability in disaster management initiatives, 21–46. \\n[115] National Academies of Sciences Engineering, Medicine, et al. 2018. Health-care \\nutilization as a proxy in disability determination. Technical Report. National \\nAcademies of Science Engineering and Medicine. \\n[116] Ihudiya Finda Ogbonnaya-Ogburu, Angela D. R. Smith, Alexandra To, and \\nKentaro Toyama. 2020. Critical Race Theory for HCI. In CHI ’20: CHI Confer-\\nence on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, \\n2020, Regina Bernhaupt, Florian ’Floyd’ Mueller, David Verweij, Josh Andres, \\nJoanna McGrenere, Andy Cockburn, Ignacio Avellino, Alix Goguey, Pernille \\nBjøn, Shengdong Zhao, Briane Paul Samson, and Rafal Kocielnik (Eds.). ACM, \\nNew York, NY, USA, 1–16. https://doi.org/10.1145/3313831.3376392 [117] Mike Oliver. 2013. The social model of disability: Thirty years on. Disability & \\nsociety 28, 7 (2013), 1024–1026. \\n[118] Rhoda Olkin, H’Sien Hayward, Melody Schaf Abbene, and Goldie VanHeel. \\n2019. The experiences of microaggressions against women with visible and \\ninvisible disabilities. Journal of Social Issues 75, 3 (2019), 757–785. \\n[119] Ruth Owen. 2022. How will data help us break the cycle of in-\\ntersectional discrimination for girls and women with disabilities? \\nhttps://www.inclusive-education-initiative.org/blog/how-will-data-help-us-\\nbreak-cycle-discrimination-and-intersectional-disadvantages-girls-and \\n[120] Kentrell Owens, Camille Cobb, and Lorrie Faith Cranor. 2021. \"You Gotta Watch \\nWhat You Say\": Surveillance of Communication with Incarcerated People. In \\nCHI ’21: CHI Conference on Human Factors in Computing Systems, Virtual Event / \\nYokohama, Japan, May 8-13, 2021, Yoshifumi Kitamura, Aaron Quigley, Katherine \\nIsbister, Takeo Igarashi, Pernille Bjørn, and Steven Mark Drucker (Eds.). ACM, \\nNew York, NY, USA, 62:1–62:18. https://doi.org/10.1145/3411764.3445055 \\n[121] Leysia Palen and Sophia B. Liu. 2007. Citizen communications in crisis: antici-\\npating a future of ICT-supported public participation. In Proceedings of the 2007 \\nConference on Human Factors in Computing Systems, CHI 2007, San Jose, Califor-\\nnia, USA, April 28 - May 3, 2007, Mary Beth Rosson and David J. Gilmore (Eds.). \\nACM, New York, NY, USA, 727–736. https://doi.org/10.1145/1240624.1240736 \\n[122] Elizabeth B. Pearce. 2013. The Social Construction of Diference. \\n[123] Eesha Pendharkar. 2022. Eforts to ban critical race theory could restrict teach-\\ning for a third of America’s kids. https://www.edweek.org/leadership/eforts-\\nto-ban-critical-race-theory-now-restrict-teaching-for-a-third-of-americas-\\nkids/2022/01 \\n[124] Mark Priestley and Laura Hemingway. 2007. Disability and disaster recovery: a \\ntale of two cities? Journal of Social Work in Disability & Rehabilitation 5, 3-4 \\n(2007), 23–42. \\n[125] Manish Raghavan, Solon Barocas, Jon M. Kleinberg, and Karen Levy. 2020. \\nMitigating bias in algorithmic hiring: evaluating claims and practices. In FAT* \\n’20: Conference on Fairness, Accountability, and Transparency, Barcelona, Spain, \\nJanuary 27-30, 2020, Mireille Hildebrandt, Carlos Castillo, L. Elisa Celis, Salvatore \\nRuggieri, Linnet Taylor, and Gabriela Zanfr-Fortuna (Eds.). ACM, New York, \\nNY, USA, 469–481. https://doi.org/10.1145/3351095.3372828 \\n[126] Yolanda A. Rankin and Jakita O. Thomas. 2019. Straighten up and Fly Right: \\nRethinking Intersectionality in HCI Research. Interactions 26, 6 (oct 2019), 64–68. \\nhttps://doi.org/10.1145/3363033 \\n[127] Yolanda A Rankin, Jakita O Thomas, and Nicole M Joseph. 2020. Intersectionality \\nin HCI: Lost in translation. Interactions 27, 5 (2020), 68–71. \\n[128] Joel Michael Reynolds. 2022. Disability and white supremacy. Critical Philosophy \\nof Race 10, 1 (2022), 48–70. \\n[129] Kathryn E Ringland, Jennifer Nicholas, Rachel Kornfeld, Emily G Lattie, David C \\nMohr, and Madhu Reddy. 2019. Understanding mental ill-health as psychoso-\\ncial disability: Implications for assistive technology. In Proceedings of the 21st \\nInternational ACM SIGACCESS Conference on Computers and Accessibility. ACM, \\nNew York, NY, USA, 156–170. \\n[130] Dianne Rios, Susan Magasi, Catherine Novak, and Mark Harniss. 2016. Con-\\nducting accessible research: including people with disabilities in public health, \\nepidemiological, and outcomes studies. American journal of public health 106, \\n12 (2016), 2137–2144. \\n[131] Jacquie Ripat and Roberta Woodgate. 2011. The intersection of culture, disability \\nand assistive technology. Disability and Rehabilitation: Assistive Technology 6, 2 \\n(2011), 87–96. \\n[132] Anne Spencer Ross, Xiaoyi Zhang, James Fogarty, and Jacob O Wobbrock. 2020. \\nAn epidemiology-inspired large-scale analysis of android app accessibility. ACM \\nTransactions on Accessible Computing (TACCESS) 13, 1 (2020), 1–36. \\n[133] Carla Sabariego, Carolina Fellinghauer, Lindsay Lee, Kaloyan Kamenov, Alek-\\nsandra Posarac, Jerome Bickenbach, Nenad Kostanjsek, Somnath Chatterji, and \\nAlarcos Cieza. 2022. Generating comprehensive functioning and disability data \\nworldwide: development process, data analyses strategy and reliability of the \\nWHO and World Bank Model Disability Survey. Archives of Public Health 80, 1 \\n(2022), 6. \\n[134] Sami Schalk. 2022. Black Disability Politics. https://doi.org/10.1215/ \\n9781478027003 \\n[135] Ari Schlesinger, W Keith Edwards, and Rebecca E Grinter. 2017. Intersectional \\nHCI: Engaging identity through gender, race, and class. In Proceedings of the \\n2017 CHI conference on human factors in computing systems. ACM, New York, \\nNY, USA, 5412–5427. \\n[136] Yasaman S Sefdgar, Woosuk Seo, Kevin S Kuehn, Tim Althof, Anne Brown-\\ning, Eve Riskin, Paula S Nurius, Anind K Dey, and Jennifer Mankof. 2019. \\nPassively-sensed behavioral correlates of discrimination events in college stu-\\ndents. Proceedings of the ACM on Human-computer Interaction 3, CSCW (2019), \\n1–29. \\n[137] Irina Shklovski, Leysia Palen, and Jeannette Sutton. 2008. Finding community \\nthrough information and communication technology in disaster response. In \\nProceedings of the 2008 ACM conference on Computer supported cooperative work. \\nACM, New York, NY, USA, 127–136.ASSETS ’23, October 22–25, 2023, New York, NY, USA \\n[138] Angela DR Smith, Alex A Ahmed, Adriana Alvarado Garcia, Bryan Dosono, \\nIhudiya Ogbonnaya-Ogburu, Yolanda Rankin, Alexandra To, and Kentaro \\nToyama. 2020. What’s race got to do with it? Engaging in race in HCI. In \\nExtended abstracts of the 2020 CHI conference on human factors in computing \\nsystems. ACM, New York, NY, USA, 1–8. \\n[139] Robert Soden and Embry Owen. 2021. Dilemmas in mutual aid: Lessons for \\ncrisis informatics from an emergent community response to the pandemic. \\nProceedings of the ACM on Human-Computer Interaction 5, CSCW2 (2021), 1–19. \\n[140] Anat Stavans and Ronit Porat. 2019. Multidisciplinary Perspectives on Multi-\\nlingualism: The Fundamentals. De Gruyter Mouton, Berlin, Boston, Chapter \\nCode-switching in multilingual communities., 123–148. \\n[141] Cella M Sum, Rahaf Alharbi, Franchesca Spektor, Cynthia L Bennett, Christina N \\nHarrington, Katta Spiel, and Rua Mae Williams. 2022. Dreaming Disability \\nJustice in HCI. In CHI Conference on Human Factors in Computing Systems \\nExtended Abstracts. ACM, New York, NY, USA, 1–5. \\n[142] Carly Thomsen. 2015. The Post-raciality and Post-spatiality of Calls for LGBTQ \\nand Disability Visibility. Hypatia 30, 1 (2015), 149–166. \\n[143] Alexandra To, Angela D. R. Smith, Dilruba Showkat, Adinawa Adjagbodjou, \\nand Christina Harrington. 2023. Flourishing in the Everyday: Moving Beyond \\nDamage-Centered Design in HCI for BIPOC Communities. In Proceedings of \\nthe 2023 ACM Designing Interactive Systems Conference (Pittsburgh, PA, USA) \\n(DIS ’23). Association for Computing Machinery, New York, NY, USA, 917–933. \\nhttps://doi.org/10.1145/3563657.3596057 \\n[144] Alexandra To, Wenxia Sweeney, Jessica Hammer, and Geof Kaufman. 2020. \\n“They Just Don’t Get It”: Towards Social Technologies for Coping with Inter-\\npersonal Racism. Proceedings of the ACM on Human-Computer Interaction 4, \\nCSCW1 (2020), 1–29. \\n[145] Jasper Tran O’Leary, Sara Zewde, Jennifer Mankof, and Daniela K Rosner. 2019. \\nWho gets to future? Race, representation, and design methods in Africatown. In \\nProceedings of the 2019 CHI Conference on Human Factors in Computing Systems. \\nACM, New York, NY, USA, 1–13. \\n[146] Courtney Ward-Sutton, Natalie F Williams, Corey L Moore, and Edward O \\nManyibe. 2020. Assistive technology access and usage barriers among african \\namericans with disabilities: A review of the literature and policy. Journal of \\napplied rehabilitation counseling 51, 2 (2020), 115–133. \\n[147] Harriet A Washington. 2006. Medical apartheid: The dark history of medical \\nexperimentation on Black Americans from colonial times to the present. Doubleday \\nBooks, New York, NY. \\n[148] Alicia Beckford Wassink, Cady Gansen, and Isabel Bartholomew. 2022. Uneven \\nsuccess: automatic speech recognition and ethnicity-related dialects. Speech \\nCommunication 140 (2022), 50–70. \\n[149] WebAim. 2021. The WebAIM Million. https://webaim.org/projects/million/ \\n[150] Jillian Weise, Berkelly Gonzalez, Cobalt Barnett, Hylyx Hyx, Jacob Boss, and \\nRyan O’Shea. 2020. A Very Kind Conversation Between a Cyborg and Some \\nBiohackers. https://disabilityvisibilityproject.com/2020/10/06/a-very-kind-\\nconversation-between-a-cyborg-and-some-biohackers/ \\n[151] Kimi Wenzel, Nitya Devireddy, Cam Davison, and Geof Kaufman. 2023. Can \\nVoice Assistants Be Microaggressors? Cross-Race Psychological Responses to \\nFailures of Automatic Speech Recognition. In Proceedings of the 2023 CHI Con-\\nference on Human Factors in Computing Systems. ACM, New York, NY, USA, \\n1–14. \\n[152] Jen White-Johnson. Visited July, 2023. Zine Workshops for Community En-\\ngagement. https://jenwhitejohnson.com/Zine-Workshops-for-community-\\nengagement \\n[153] Kevin Williams and Lateef McLeod. 2020. Black AAC User Perspectives on \\nRacism and Disability. https://www.youtube.com/watch?v=iTSAK4yRf5A \\n[154] Rua Mae Williams, Louanne Boyd, and Juan E Gilbert. 2023. Counterventions: \\na reparative refection on interventionist HCI. In Proceedings of the 2023 CHI \\nConference on Human Factors in Computing Systems. ACM, New York, NY, USA, \\n1–11. \\n[155] Rua M Williams, Kathryn Ringland, Amelia Gibson, Mahender Mandala, Arne \\nMaibaum, and Tiago Guerreiro. 2021. Articulations toward a crip HCI. Interac-\\ntions 28, 3 (2021), 28–37. \\n[156] Susan Copeland Wilson. 2012. e-government legislation meets the poverty \\nthreshold: issues for the economically disadvantaged. In Proceedings of the 13th \\nAnnual International Conference on Digital Government Research. Association \\nfor Computing Machinery, New York, NY, USA, 74–83. \\n[157] Elena Ariel Windsong. 2018. Incorporating intersectionality into research \\ndesign: an example using qualitative interviews. International Journal of Social \\nResearch Methodology 21, 2 (2018), 135–147. https://doi.org/10.1080/13645579. \\n2016.1268361 \\n[158] Alice Wong. 2020. Disability visibility: First-person stories from the twenty-frst \\ncentury. Vintage Books, New York, NY. \\n[159] Di Wu. 2023. Good for tech: Disability expertise and labor in China’s artifcial \\nintelligence sector. First Monday 28 (2023), 1–20. Issue 1. \\n[160] Xuhai Xu, Han Zhang, Yasaman Sefdgar, Yiyi Ren, Xin Liu, Woosuk Seo, Jennifer \\nBrown, Kevin Kuehn, Mike Merrill, Paula Nurius, et al. 2022. GLOBEM Dataset: \\nMulti-Year Datasets for Longitudinal Human Behavior Modeling Generalization. Harrington, et al. \\nA\\ndvances in Neural Itnformation Processing Systems 35 (2022), 24655–24692. \\n[161] Silvia Yee, Mary Lou Breslin, Tawara D. Goode, Susan M. Havercamp, Willi \\nHorner-Johnson, Lisa I. Iezzoni, and Gloria Krahn. 2016. Compounded Dispari-\\nties: Health Equity at the Intersection of Disability, Race, and Ethnicity. \\n[162] Anon Ymous, Katta Spiel, Os Keyes, Rua M Williams, Judith Good, Eva Hor-\\nnecker, and Cynthia L Bennett. 2020. \"I am just terrifed of my future\"—Epistemic \\nViolence in Disability Related Technology Research. In Extended Abstracts of \\nthe 2020 CHI Conference on Human Factors in Computing Systems. ACM, New \\nYork, NY, USA, 1–16. \\n[163] Tae-Jung Yun, Hee Young Jeong, Tanisha D Hill, Burt Lesnick, Randall Brown, \\nGregory D Abowd, and Rosa I Arriaga. 2012. Using SMS to provide continuous \\nassessment and improve health outcomes for children with asthma. In Proceed-\\nings of the 2nd ACM SIGHIT International Health Informatics Symposium. ACM, \\nNew York, NY, USA, 621–630. \\n[164] Han Zhang, Margaret Morris, Paula Nurius, Kelly Mack, Jennifer Brown, Kevin \\nKuehn, Yasaman Sefdgar, Xuhai Xu, Eve Riskin, Anind Dey, et al. 2022. Impact \\nof Online Learning in the Context of COVID-19 on Undergraduates with Disabil-\\nities and Mental Health Concerns. ACM Transactions on Accessible Computing \\n15, 4 (2022), 1–27.',\n",
       "  ['accessibility', 'race and ethnicity'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_documents\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
