Exploring User-SpeciÔ¨Åc Information in Music Retrieval
Zhiyong Cheng
National University of Singapore
jason.zy.cheng@gmail.comJialie Shen
Northumbria University, UK
jerry.shen@northumbria.ac.ukLiqiang Nie
Shandong University, China
nieliqiang@gmail.com
Tat-Seng Chua
National University of Singapore
chuats@comp.nus.edu.sgMohan Kankanhalli
National University of Singapore
mohan@comp.nus.edu.sg
ABSTRACT
With the advancement of mobile computing technology and
cloud-based streaming music service, user-centered music
retrieval has become increasingly important. User-speciÔ¨Åc
information has a fundamental impact on personal music
preferences and interests. However, existing research pays lit-
tle attention to the modeling and integration of user-speciÔ¨Åc
information in music retrieval algorithms/models to facili-
tate music search. In this paper, we propose a novel model,
named User-Information-Aware Music Interest Topic (UIA-
MIT) model. The model is able to eÔ¨Äectively capture the
inÔ¨Çuence of user-speciÔ¨Åc information on music preferences,
and further associate users‚Äô music preferences and search
terms under the same latent space. Based on this model, a
user information aware retrieval system is developed, which
can search and re-rank the results based on age- and/or
gender-speciÔ¨Åc music preferences. A comprehensive experi-
mental study demonstrates that our methods can signiÔ¨Åcantly
improve the search accuracy over existing text-based music
retrieval methods.
CCS CONCEPTS
‚Ä¢Information systems !Information retrieval; Mu-
sic retrieval; Retrieval models and ranking;
KEYWORDS
Semantic music retrieval, user demographic information, re-
ranking, topic model
ACM Reference format:
Zhiyong Cheng, Jialie Shen, Liqiang Nie, Tat-Seng Chua, and Mo-
han Kankanhalli. 2017. Exploring User-SpeciÔ¨Åc Information in
Music Retrieval. In Proceedings of SIGIR‚Äô17, August 7‚Äì11, 2017,
Shinjuku, Tokyo, Japan, , 10 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080772
Jialie Shen is corresponding author
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for proÔ¨Åt or commercial advantage
and that copies bear this notice and the full citation on the Ô¨Årst page.
Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior speciÔ¨Åc permission and/or a fee. Request permissions
from permissions@acm.org.
SIGIR‚Äô17, August 7‚Äì11, 2017, Shinjuku, Tokyo, Japan
¬©2017 ACM. 978-1-4503-5022-8/17/08...$15.00
DOI: http://dx.doi.org/10.1145/3077136.30807721 INTRODUCTION
With the rapid development of mobile computing technology
and cloud-based streaming music service, smart devices have
become the most popular platforms to consume music daily.
Based on Nelsen‚Äôs Music 360 2015 report1, 44% of US music
listeners use smartphones to listen to music on a daily basis.
Smartphones are typically designed for personal use and it is
easy to obtain personal information from smartphones, which
can be used in personalized applications to achieve better
user experiences when developing personalized applications.
With the rapidly growing trend in music consumption with
smartphones, therehasbeenanincreasinginterestinstudying
user-centered music retrieval [ 11,25,33,34,36]. Techniques
to support eÔ¨Äective user-centered music search are gaining its
importance due to its wide range of potential applications [ 7,
25,36]. Based on this technology, intelligent music search or
recommendation systems can be developed to automatically
cater to users‚Äô personal music needs.
Text-based music retrieval, as one of the most popular
music search paradigms, typically requires users to provide
a few text keywords as queries to describe their music in-
formation needs [ 20,24,27,41]. Most previous research
eÔ¨Äorts in music retrieval have been devoted to the develop-
ment of retrieval/recommendation algorithms [ 5,11,21,27],
eÔ¨Äective music representations [ 23,24,31,42], similarity
measurement [ 6,12,38,45,48], and automatic music anno-
tation [1,3,13,26,28,37,40,41,46]. However, the eÔ¨Äects
of user-speciÔ¨Åc information in music retrieval has not been
well studied [ 25,33,34]. User-speciÔ¨Åc information or user
background, such as age, gender, social status, growing-up
environment, culture background, etc., has great impact on
users‚Äô long-term music interests. This hypothesis has received
strong support in prior research studies [ 8,22,43]. Due to
its great impact, given the same query, users with diÔ¨Äerent
backgrounds might expect diÔ¨Äerent search results [ 11]. For
example, given a query ‚Äú pop, sad", the retrieved songs ex-
pected by a 40-year-old male could be very diÔ¨Äerent from
those favored by a 20-year-old female.
In this study, we develop a text-based music retrieval sys-
tem, which can eÔ¨Äectively leverage user-speciÔ¨Åc information
to improve the search performance. The key challenges on
eÔ¨Äectively integrating user-speciÔ¨Åc information in music re-
trieval include: (1) how to model the inÔ¨Çuence of user-speciÔ¨Åc
1http://www.nielsen.com/us/en/insights/reports/2015/music-360-
2015-highlights.html
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
655information on music preferences; and (2) how to associate
the inÔ¨Çuence with search queries and songs. To tackle these
two challenges, a novel topic model named User Information
Aware Music Interest Topic (UIA-MIT) is proposed. UIA-
MIT can explicitly model the music preferences of diÔ¨Äerent
types of user-speciÔ¨Åc information. In this model, the music
preferences aÔ¨Äected by diÔ¨Äerent factors (i.e., age and gen-
der) are represented by probabilistic distributions on a set
of latent topics. These latent topics are in turn probabilistic
distributions of songs and terms (song‚Äôs annotations or tags).
Therefore, songs, terms, and music preferences (inÔ¨Çuenced by
age and gender) can be associated by latent topics. Based on
the UIA-MIT model, we develop a probabilistic text-based
music retrieval method, which can eÔ¨Äectively exploit user
information to improve the search results. In our context,
user-speciÔ¨Åc information refers to the user related informa-
tionoruser backgrounds which have been proven to have
great impact on a user‚Äôs long-term music interests, such as
age,gender, and country[8, 22, 43].
In order to evaluate the performance of our proposed
method, extensive experiments and comprehensive compar-
isons over diÔ¨Äerent methods have been conducted on two
retrieval tasks: ad-hoc search and re-ranking. The experi-
mental results demonstrate that users‚Äô age and/or gender
information play an important role on search performance
improvement, which indicates the importance of utilizing
user-speciÔ¨Åc information in real music retrieval system devel-
opment. To the best of our knowledge, our work is the Ô¨Årst
attempt on designing advanced music retrieval methods to
leverage user-speciÔ¨Åc information in retrieval. In summary,
the main contributions of this work include:
This is the Ô¨Årst attempt to explore the incorporation of
user-speciÔ¨Åc information in music retrieval algorithms for
search accuracy improvement.
WeproposeaUIA-MITmodel, whichcanexplicitlymodel
the music preferences aÔ¨Äected by diÔ¨Äerent types of user
information. Furthermore, based on the model, a text-
based music retrieval method is developed to eÔ¨Äectively
utilize user-speciÔ¨Åc information for improving the search
accuracy.
We construct two test collections and empirically evaluate
the performance of the proposed retrieval method and
compare it with a set of baselines. The experimental
results demonstrate signiÔ¨Åcant performance improvement.
Theremainderofthispaperisorganizedasfollows: Section
2 gives a brief overview of related work; Section 3 detailedly
describes the proposed UIA-MIT model and retrieval method;
Section 4 introduces the experimental datasets and conÔ¨Ågu-
rations; and Section 5 reports the experimental results and
main Ô¨Åndings. Finally, Section 6 concludes the paper.
2 RELATED WORKS
In this following, we review the research directions which
are closely related to this paper, including text-based music
retrieval, the use of user information in music retrieval and
recommender systems, and related topic models.2.1 Text-based Music Retrieval
As one of the most popular music search paradigms, text-
based retrieval system is built on the top of mature text
retrieval techniques and typically requires users to provide
a few text keywords as queries to describe their music infor-
mation needs [ 20,24,27,41]. Its search performance heavily
relies on the meta-information (e.g., artist and title) and well-
deÔ¨Åned categorized information (e.g., genre and instrument).
In many cases, users would also like to describe their current
contexts, such as emotions andoccasions, with the expec-
tation that the music search engine return a playlist with
suitable songs [ 19]. To support the search of such semantic
queries, one needs to annotate songs with a rich vocabulary of
music terms. However, human annotation is very expensive
in terms of both time and labor, and thus it is unlikely to
scale with the growth in the amount of recorded songs. To
deal with this problem, many automated methods have been
proposed to automatically annotate songs with music related
concepts by learning the correlation between music acoustic
contents and the semantic terms based on a well-annotated
music collection. Most automated systems generate a vec-
tor of tag weights when annotating a new song for music
retrieval [ 26]. An early work in this direction is described
in [41]. They formalized the audio annotation and retrieval as
a supervised multi-class labeling task. The dataset CAL500
they created in this study became a standard test collection
for subsequent works [1, 13, 26, 28, 42, 44].
With the rapid development of social music websites, songs
are annotated with user-contributed social tags, which pro-
vides an alternative way to navigate and search songs (e.g.,
Last.fm). Social tags have no constraints on the use of text
and provide a rich vocabulary, which covers most terms
used to describe songs. Extensive research eÔ¨Äorts have been
devoted to developing tag-based music search systems [ 24].
However, theuser-providedtagsareknowntobenoisy, incom-
plete and subjective [ 24], which limits the search performance
of tag-based methods. Consequently, many works consider
the combination of tags and acoustic similarity to improve
the search performance [21, 24, 27].
2.2 User Information in Music Retrieval
In recent years, researchers have realized and advocated the
importance of incorporating information about user into mu-
sic information search and discovery [ 25,33,34,36]. Previous
work has shown that user information can be used to improve
music recommendation [ 8,43]. In [29], user demographic in-
formation was taken into a fuzzy system for context inference
using Bayesian network. In [ 15], user information was used to
infer user‚Äôs mood, which is then used to match with the mood
of songs predicted based on music content. [ 35] studied the
inÔ¨Çuence of user information (e.g., age, gender, and country)
on the task of artist recommendation. Although the use of
user information in recommender systems is widespread, little
eÔ¨Äort has been devoted to the exploration of user information
in music retrieval. Furthermore, previous studies have not
explicitly modeled the music preferences of diÔ¨Äerent ages
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
656and genders in recommendation. In this work, we propose
a user-information aware music interests discovery model to
capture the music preferences of diÔ¨Äerent ages and genders,
and develop a music retrieval framework to use age and/or
gender information in music retrieval.
2.3 Topic Models in Music Retrieval
Topic models, such as pLSA [ 17] and LDA [ 4], were originally
proposed to discover the underlying themesorlatent topics
for a large scale collection of text documents. In these models,
latent topics are discovered by mining co-occurrence patterns
of words in documents that exhibit similar patterns. By treat-
ingusersasdocuments anditemsaswords, topic models
have been applied to discover user‚Äôs latent interests [ 18,30].
In the domain of music information retrieval and recommen-
dation, several previous studies adapted the topic models to
capture user‚Äôs music interests [ 9,11,16,47]. [47] used the
three-way aspect model to discover user music interests in rec-
ommendation. [ 9] extended the three-away aspect model to
incorporate both location context and global music trends for
location-aware personalized music recommendation. User‚Äôs
local music preference is captured by the co-occurrence of
songs in users‚Äô music proÔ¨Åles and the co-occurrence of music
contents among songs. In [ 10], a location-aware topic model
was proposed to discover the music preferences of diÔ¨Äerent
venue types. [ 16] proposed a variant of LDA to discover
user‚Äôs interests via the combination of co-occurrence of songs
in the same user‚Äôs playlist and co-occurrence of tags in the
same song. In [ 11], a dual-layer music preference topic model
was developed to characterize the correlation among users,
songs, and search terms for personalized music retrieval.
3 OUR APPROACH
In general, users with similar demographics have more similar
music interests than users with diÔ¨Äerent demographics. For
example, users in the same age or gender have more similar
music interests [ 2,22]. To model the inÔ¨Çuence of such user-
speciÔ¨Åc factors, we propose a User Information Aware Music
Interest Topic (UIA-MIT) model. In this model, a set of
latent topics (i.e., Ktopics) are discovered based on the
records of users‚Äô favorite tracks. Each latent topic represents
one type/style of music or a music interest dimension . As
users‚Äô music interests are inÔ¨Çuenced by diÔ¨Äerent factors. UIA-
MIT is designed to capture the inÔ¨Çuence of diÔ¨Äerent factors
on music interests. For example, what are the general music
interests of users in a certain age range or gender ; or in
other words, the likelihood of each type of songs preferred
by the users with regard to their ages and genders. In
this model, a user‚Äôs latent music interest is expressed as a
mixture of multiple latent topic distributions. Each latent
topic distribution represents a certain music interest which
is dependent on a user-speciÔ¨Åc factor (e.g., age). Therefore,
the mixture of multiple latent topic distributions in this
model represents the music interests of a user, as the result
of collective eÔ¨Äects of diÔ¨Äerent factors.Table 1: Notations and their deÔ¨Ånitions
Notation DeÔ¨Ånition
u;s user, and song, respectively
c;a;g country, age, and gender category
sv;sw song, audio word, and text word
D corpus, (u;a;g;s;sw;sv)2D
jDuj number of songs in the u‚Äôs music proÔ¨Åle
U,S user set and song set in the corpus
U,S total number of users and songs in the corpus
W,V text and audio word vocabulary, respectively
W,V text and audio vocabulary size, respecitvely
A,G age category set and gender category set
A,G number of age and gender groups, respectively
K total number of latent topics
y indicator variable: decide zgenerated from u,
a, org
 mixing weight vector
;; Dirichlet priors
u;a;gmusic preference of user u, age group a, and
gender group g, respectively
s,v,wmultinomial distributions over songs S, audio
wordsVand text wordsW
3.1 Preliminaries
For ease of understanding and presentation, we Ô¨Årst introduce
some key concepts and notations. Table 1 lists the notations
used in this paper.
Dataset The dataset Din our model consists of a set of
records, with each record comprising a user, user information
(i.e., age and gender), song, and song‚Äôs content (i.e., tags and
audio words), that is, (u;a;g;s;sw;sv)2D, whereu2U,
s2S,a2A,g2G,sw2W, andsv2V.2One piece
of record in the dataset is a user uof ageaand gender
gwho loves a song swith tagsswand audio content sv:
Dusc=f(u;a;g;s;w;v ) :w2sw;v2svg.
Audio Word An audio word is a representative short
frame (e.g., 0.5s in our implementation) of audio stream in
a music corpus [ 11]. Audio words are used to represent the
audio content of a song as a ‚Äúbag-of-audio-words" document.
Latent Topic A latent topic z, ortopicfor short, in a
song collectionSis a probabilistic distribution over songs,
i.e.,fP(sjs) :s2Sg. Similarly, a topic in a text word
corpusWis a probabilistic distribution over text words, i.e.,
fP(wjw) :w2Wg. A topic in an audio word corpus Vis
a probabilistic distribution over audio words, i.e., fP(vjv) :
v2Vg.
User‚Äôs Music Interest UIA-MIT models users‚Äô music
interest as the mixture of three latent topic distributions (see
Eq. 1): (1)u: the music preferences as a collective result
based on the inÔ¨Çuence of all other factors (e.g., personality)
besides age and gender , (2) a: age-based music preference,
denoting the music preferences of a certain age range aorthe
general music preferences of users in age range a, and (3)g:
2In the paper, unless otherwise speciÔ¨Åed, notations in bold style denote
matrices or vectors, and the ones in normal style denote scalars.
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
657z
w ◊ãw»ôus ◊ãs
»ôgU
GK
Ky
»ú»ôa
Av ◊ãv
Kuag
U
|sv|
|sw| |Du|
U»ïv
»ïw»ïs
ƒÆa
ƒÆgƒÆu
»ñu »ña »ñgy=0
y=1
y=2Figure 1: The graphical representation of the UIA-
MIT model.
gender-based music preference, denoting music preferences
of male or female .
3.2 UIA-MIT Model
3.2.1 Model Description. The graphical representation of
the model is shown in Fig. 1, in which ageandgenderare
considered. UIA-MIT explicitly models the music preferences
of ages (a) and genders ( g). The music preference, as a
result of all other factors (excluding age and gender, such
as user‚Äôs personality and country), is modeled as a single
probabilistic distribution of latent topics, denoted as user‚Äôs
personalmusicinterest( u). NoticethattheUIA-MITmodel
can be easily extended to model the music preference of other
individual factors (e.g., country).
From the generation perspective, the model mimics the
music selection process by considering the user‚Äôs music in-
terest, age-based music preference, and gender-based music
preference in a uniÔ¨Åed manner. Given a user with age aand
genderg, the likelihood the user uselecting a music track is
dependent on the music preferences of user age and gender
as well as his/her personal music interest:
P(sju;a;g;u;a;g;w;v;s) =uP(sju;u;w;v;s)
+aP(sja;a;w;v;s) +gP(sjg;g;w;v;s)
(1)
whereP(sju;u;w;v;s)is the probability that song sis
generated according to the personal music interest of user u,
denoted asu;P(sja;a;w;v;s)andP(sjg;g;w;v;
s)denote the probability that song sis generated according
to the age music preference of aand gender music preference
ofg, denoted as aandgrespectively. =fu;a;g:
u+a+g= 1gis a categorial distribution, which controls
the selection motivation of song s. That is, when selecting
songs, it is possible that user uselects it according to his/her
own music interests uwith probability u, or according to
the age-based music preference awith probability a, or
according to the gender-based music preference gwith prob-
abilityg. Note that is a group-dependent parameter,
as users in diÔ¨Äerent groups have diÔ¨Äerent tendency to se-
lect music from diÔ¨Äerent aspects. For example, from thetraining results, female users are more likely to select music
tracks according to the general music preferences (namely,
mainstreaming music) than male users.
The generation process of UIA-MIT is shown in Algo-
rithm 1. Intuitively, UIA-MIT models user‚Äôs music interests
as the combination of the general music preferences according
to certain user-speciÔ¨Åc information (age and gender here) and
user‚Äôs distinct music interests (aÔ¨Äecting by user‚Äôs personality,
etc.). The general music preferences of certain user-speciÔ¨Åc
information can be applied in music-related service.
Collapsed Gibbs sampling [ 14] is used to estimate the
parameters in the topic model. Due to space limitation, we
omit the description of parameter estimation in the paper.
Algorithm 1: Generation Process of UIA-MIT
1 foreach topick= 1;:::;K do
2Drawk;sDir(js);
3Drawk;wDir(jw);
4Drawk;vDir(jv);
5 foreach useru2Udo
6DrawuDir(ju);
7 foreach age range a2Ado
8DrawaDir(ja);
9 foreach gender g2Gdo
10DrawgDir(jg);
11 foreach useru2Uwith agea2Aand gender g2Gdo
12 for each songs2Dudo
13 Toss a coin according to categorical distribution
ysDir(u;a;g);
14 if ys== 0 then
15 DrawzsMulti( u)according to the music
interest of user u;
16 if ys== 1 then
17 DrawzsMulti( a)according to the music
preference of age a;
18 if ys== 2 then
19 DrawzsMulti( g)according to the music
preference of gender g;
20 After the sampling of the topic zs=k, draw song
sMulti( k;s);
21 for each wordw2swdo
22 DrawwMulti( k;w);
23 for each audio word v2svdo
24 DrawvMulti(k;v);
3.3 Retrieval Method
Given a query q, UIA-MIT can be used to estimate P(sjq)
with the consideration of user‚Äôs age and gender. In music
retrieval, songs are then ranked in the descending order of
P(sjq)and the top results are returned to the user. Specif-
ically, for a query q=fw1;w2::;wngof useruwith agea
and gender g, the conditional probability P(sjq)could be
computed with estimated parameters =fu;a;ggand
=fs;wg. For the simplicity of presentation, we use
P(sjq;)to denoteP(sjq;u;a;g; ;)in the following.
P(sjq;) =nY
i=1P(wijs;u;a;g; ;)P (sju;a;g; ;s)(2)
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
658where query terms in qare assumed to be independent.
P(sju;a;g;;s)is computed as:
P(sju;a;g; ;s) =KX
k=1P(sjz =k;s)P(z=kju;a;g; )
=KX
k=1k;s(uu;k+aa;k+gg;k)
(3)
According to Bayes rule and the graphical representation
of the UIA-MIT model, P(wijs;u;a;g; ;)is estimated as:
P(wijs;) =KX
k=1P(wijz=k;w)P(s;zkju;a;g; ;s)
P(sju;a;g; ;s)
=KX
k=1k;wik;s(uu;k+aa;k+gg;k)PK
k=1k;s(uu;k+aa;k+gg;k)
(4)
Based on Eq. 3 and Eq. 4, Eq. 2 becomes
P(sjq;) =nY
i=1KX
k=1k;wik;s(uu;k+aa;k+gg;k)(5)
Typically, when uof a particular user uis known, Eq. 5
can be used for personalized music search. However, for a new
user to the system, uis unknown, while his/her age/gender
information is relatively easier to be found. In such cases,
we normalize aandgtoa+g= 1, and the following
equation is used for retrieval:
P(sjq;a;g; ;) =nY
i=1KX
k=1k;wik;s(aa;k+gg;k)(6)
If either age or gender information is available, only the
corresponding music preferences will be used (namely, set
a= 1org= 1in the equation). Intuitively, k;wik;s
evaluates the similarity of the song swith respect to query
qin the music dimension kin the music interest space, and
aa;k+gg;kestimates the music preferences with respect
to age range aand gender gin the music dimension k. Thus
it can be seen as that the model re-weights the original query
in diÔ¨Äerent music dimensions based on user‚Äôs age and gender
information. Given a new users with only age and/or gender
information, our system can be used to search songs based on
Eq. 6. Accordingly, the exploitation of age and gender (i.e.,
Eq. 6) can alleviate the cold-start problem in personalized
music retrieval, in which user‚Äôs music preference is unknown.
4 EXPERIMENTAL CONFIGURATION
We conduct a comprehensive experimental study to investi-
gate the performance of our methods on two music retrieval
tasks:ad-hocandre-rank. The experiments mainly answer
the following research questions:
RQ1What is the performance of UIA-MIT based retrieval
methods on ad-hoc search with the use of age and/or
gender information as compared to other text-based
music retrieval methods? (See Sect. 5.1.1)RQ2Whether the use of user-speciÔ¨Åc information (i.e., age
and gender information in our study) for re-ranking
can improve the music search performance? If so, how
much can be improved by using age and/or gender
information? (See Sect. 5.1.2 and Sect. 5.2)
RQ3Whether the UIA-MIT model can be extended to
model the music preference of other user information,
such as country? (See Sect. 5.2)
RQ4What are the impact of diÔ¨Äerent types of user infor-
mation on user‚Äôs music preference, such as age, gender
and country information? (See Sect. 5.3)
To answer these questions, we constructed two test col-
lections to conduct experiments. In our experiments, test
collections are split into a training set and a testing set.
The training set is used to train the UIA-MIT model. User-
speciÔ¨Åc information and users‚Äô favorite songs are used in
the training stage. In the testing stage, only user-speciÔ¨Åc
information is used in retrieval. The information of favorite
songs are only used in the result evaluation. It is similar to
the cold-start problem in personalized search, where user‚Äôs
personal preferences are unknown.
4.1 Datasets
To evaluate the search accuracy of retrieval systems with
respect to query users, a great challenge is how to obtain
the ground truth of the test queries with respect to the
correspondingqueryusers. Inourretrievaltask, givenaquery
qof a user with certain information, a relevant song should
not only be relevant to the query but also loved by the users
with such information. We developed two test collections
by crawling user information from Last.fm. Thousands of
queries and corresponding ground truth are generated for
testing. The dataset will be released for the repeatability of
the experiments and other related studies.3
User ProÔ¨Åle Dataset We construct a dataset with users‚Äô
demographic information and their favorite music tracks from
Last.fm. The dataset is collected in the following procedures.
160 recent active users were randomly selected from Last.fm4.
The friends of these users and the friends of their friends were
also collected with their demographic information, including
age, gender, country. In total, 90,036 users were collected.
Users who provide both age and gender information were
retained. As the number of users with age under 16 or above
54 years old is small, we removed these users and only focused
on studying the inÔ¨Çuence of ages between 16 to 54. Finally,
the dataset remains 45,334 users. Users‚Äô favorite tracks were
collected using Last.fm public API ‚ÄúUser.getLovedTracks",
and a 30-second audio stream of each song was downloaded
from 7digital5. By removing users with less than 10 favorite
songs and songs preferred by less than 10 users, there are
29,412 users (15,826 males and 13,586 females) and 15,323
songs in the Ô¨Ånal dataset. The social tags of these songs were
3Experimental datasets are accessible in:https://www.dropbox.com/
sh/eue9it0lqlpzo7q/AAAe-v2MS0KYln5qSpSgPQQna?dl=0.
4Accessed http://www.last.fm/community/users/active on Mar 3,
2015.
5https://www.7digital.com/
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
659collected from Last.fm using API ‚ÄúTrack.getTopTags". The
social tags of songs from Last.fm are used in the topic model
training and the tag-based music retrieval method (see the
TAG method in Section 4.2). We denote the dataset as D
for ease of presentation.
Retrieval Test Collection 1 (TC1) To judge the rele-
vance of songs with respect to queries, it is necessary to label
the songs in the dataset with query concepts. CAL10K [ 39] is
a labeled song collection. The annotations are used as ground
truth in pervious text-based music retrieval research [ 27].
This dataset contains 10,870 songs from 4,597 diÔ¨Äerent artists.
The label vocabulary is composed of 137 ‚Äúgenre" tags and
416 ‚Äúacoustic" tags. The number of tags of songs varies from
2 to 25 tags. The song tags are mined from the Pandora Web
sites. The annotations in Pandora are contributed by music
experts and are considered highly objective [ 39]. 2,839 songs
inDare contained in the CAL10K. These songs are used as
retrieval dataset in TC1. In experiments, we categorized the
users into 7 age groups, as shown in Table 2. Thus, there are
14 user groups (7 age groups 2 gender groups) in total.
Table 2: Number of users in each age group in Test
Collection 1.
Group 0123456
Age16-2021-2526-3031-3536-4041-4546-54
#Users 9,00312,820 4,9411,482 595324247
Retrieval Test Collection 2 (TC2) Due to the limited
size of the well-labeled songs, TC1 contains only 2,839 songs
in the retrieval stage, which is relatively small. TC1 can
be used to to test the performance of the proposed retrieval
methods by leveraging age and gender information. To ex-
amine the performance of our methods in large datasets and
demonstrate the extendability of UIA-MIT to other user in-
formation (‚Äú country" here), we constructed TC2. TC2 uses
social tags as annotations in relevance judgement. 26,468
users inDhave age, gender, and country information. These
users are from 179 diÔ¨Äerent countries. With the age, gender,
and country information, we categorize users into groups
based on {age, gender, country}, e.g., 16-20_male_US. The
top 30 user groups with the most number of users are used
in experiments to examine the performance of our methods.
These groups are shown in Table 3. By removing users with
less than 10 favorite songs and songs liked by less than 10
users, TC2 has 14,715 users and 1, 0197 songs.
Query Set In experiments, we use a combination of k
distinct terms as queries. Following the methodology in [ 27,
41], queries composed by k=f1;2;3gterms are used. The
method described in [ 27] is used to construct the query set.
In TC1, all the terms in CAL10K dataset are treated as 1-
term query candidates; and for 2-term and 3-term queries, all
the term combinations are considered as candidates. In TC2,
social tags are used to generate the queries. We Ô¨Årst Ô¨Åltered
the tags which appear less than 10 times in the dataset, and
removed the tags which express personal interests in the song,
such as ‚Äúfavorite", ‚Äúgreat", ‚Äúfavor", ‚Äúexcellent", etc. AfterTable 3: Number of users in diÔ¨Äerent groups in Test
Collection 2.
CountryMale Female
16-2021-2526-3016-2021-2526-30
Brazil 16051320 2531665 809156
Poland 179270 83472401 98
Russia 83269109184314 92
UK 186416223198364120
US 5391360 6844571273 533
tokenizing the tags of each song into terms, all the terms
are treated as candidates. For 2-term and 3-term queries, all
the term combinations which appear in a song are treated as
candidates. Next, query candidates with less than 10 relevant
songs in the ground truth of all user groups were removed.
For the 3-term query in TC2, we retain a random sample
of 3,000 queries as [ 27]. Table 4 summarizes the number of
queries in TC1 and TC2. Notice that the queries are the
same for each group. Table 5 shows some query examples.
Table 4: Number of queries in TC1 and TC2.
Test Collection # 1-Term # 2-Term # 3-Term
TC1 33 122 542
TC2 79 1691 3000
Table 5: Few examples for each type of queries.
1-Term Query 2-Term Query 3-Term Query
aggressive aggressive, guitar aggressive, angry, guitar
angry aggressive, rock angry, guitar, rock
breathy bass, tonality drums, angry, guitar
country blues, guitar guitar, aggressive, angry
danceable country, guitar guitar, pop, romantic
Ground Truth As the query is evaluated with respect to
diÔ¨Äerent user groups (i.e., age and gender groups), a relevant
song with respect to a query should (1) contains all the query
terms in the annotations (in the CAL10K dataset for TC1)
or social tags (for TC2); and (2) be loved by at least 10 users
in this user group. The second criterion is to guarantee that
the relevant song is loved by users in a user group (i.e., with
a certain age and gender). Based on the criteria, the relevant
songs in the retrieval datasets of TC1 and TC2 are labeled.
Notice that for each query, the numbers of relevant songs in
diÔ¨Äerent groups are diÔ¨Äerent.
4.2 Experimental Setup
In our experiments, users were split into two sets for two-fold
cross-validation: one set (users with their favorite tracks) is
used for model training, and the other set is used to create
the query set and generate the corresponding ground truth.
The dataset is split in the way to guarantee each set has
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
660approximately equal number of users in the two sets. In
the result presentation, the presented results are the average
performance of the two sets. For the training of UIA-MIT,
the corpus is formulated into three types of documents:
User-Song Document It represents a user‚Äôs music pro-
Ô¨Åle. For each user, a user-song document is generated based
on his/her favorite songs. The document is the concatenation
of all the songs preferred by the users.
Song-Text Word Document This document represents
the semantic content of a song. Songs‚Äô tags from Last.fm are
used to form their text documents. In our implementation,
tags that appeared less than 10 songs are Ô¨Åltered. The
remaining tags of a song are concatenated and tokenized
with a standard stop-list to form its text document.
Song-Audio Word Document This document repre-
sents the audio content of a song, namely, the audio words
used in the UIA-MIT model. The audio contents of one song
are represented by "bag-of-audio-words" document. An au-
dio word is a representative short frame of audio stream (e.g.,
0.5 second) in a music corpus. For each song, a 30-second
audio track downloaded from 7digital is used to generate
its ‚Äúbag-of-audio-word" document. We follow the method
described in [ 11] to generate the song-audio word documents.
4.2.1 Baselines.
TAGIn this method, the social tags of each song in
Last.fm are used as the text description for retrieval. The
standard tf-idf weighting scheme is used to compute the
similarity between query and songs with the standard
cosine distance in the Vector Space Model [32].
WLCThe Ô¨Årst result returned by TAG is used as the
seed for a content-based music retrieval (CBMR) method.
Then the score of the TAG method and CBMR method
are linearly combined together to generate the Ô¨Ånal search
results. This method is implemented as described in [ 11].
PARThis method is proposed in [ 21]. It incorporates
audio similarity into an already existing ranking. In our
experiments, the results of tag-based method (TAG) are
used as the initial ranking list. In the implementation,
we followed the details reported in the referred paper.
GBRThismethodisproposedin[ 27]. Itisagraph-based
ranking method, which combines both tag and acoustic
similarity in a probabilistic graph-based representation
for music retrieval. In our implementation, we followed
the details reported in the referred paper.
Music Popularity Based Re-ranking (MPR) This
method re-ranks the top (e.g., 100) songs returned by
other retrieval methods according to the popularity of
these songs in each user group. The popularity score is
computed as:
POP (s) =N(s;a;g )
N(a;g )(7)
whereN(s;a;g )is the number of users in group (a;g)
favoring song s, andN(a;g)is the total number of the
users in group (a;g ).
Group User Music Representation (GUMR) For
each group, we aggregate the social tags of songs lovedby the users in this group to form a music representation
document for the group. For a given query qof useruin
ageaand gender g, the similarity score is computed as:
Sim(q;s;a;g ) =wTAG (q;s ) + (1 w)Sim(s;a;g )(8)
in whichTAG (q;s)is the cosine similarity between the
songsandqusing TAG method, and Sim (s;a;g )is the
similarity between user group preference and the song s
using cosine distance with the standard tf-idf weighting
scheme (based on song‚Äôs text document and the group‚Äôs
music document). The combination weight is tuned in
experiments.
The following methods are the variants of the proposed
method, which simulate the scenarios when partial user in-
formation is available, and study the improvements of using
such information individually or together in retrieval.
A-MIT: only considering age information in UIA-MIT;
G-MIT: only considering gender information in UIA-
MIT;
C-MIT: only considering country information in UIA-
MIT (only tested in TC2);
AG-MIT : considering both age and gender information
in UIA-MIT.
AGC-MIT: considering age, gender, and country infor-
mation in UIA-MIT (only used in TC 2).
To the best of our knowledge, we have not found any music
search methods using such information in retrieval. MPR
and GUMR are two heuristic methods on utilizing age and
gender information. Notice that if these methods can also
improve the search accuracy, it further demonstrates the
importance of considering user-speciÔ¨Åc information in music
retrieval. In the above methods, PAR and MPR are re-
ranking methods and thus are compared in the re-ranking
task (Sect. 5.2). Other methods are compared in the ad-hoc
search task (Sect. 5.1).
4.2.2 Metrics. Precision at k(P@k)and Mean Average
Precision (MAP) are used as evaluation metrics. As the
top search results are more important, we report P@10 and
MAP@10 in experimental results.
4.2.3 Parameter Setting. In our implementation, the hy-
perparameters in the topic model are turned in a wide range
of values. In the UIA-MIT model, without prior knowledge
aboutthetopicdistributionsofusersindiÔ¨Äerentagesandgen-
ders, we setu,aandgto be symmetric. For simplicity,
we set them to be the same and tune them in the range of =
u=a=g2f0:01;0:05;0:1;1:0;5:0g. Similarly, w,v
andsare also set to be symmetric and tuned in similar man-
ners:=w=v=s2f0:01;0:05;0:10;0:15;0:20;0:25g.
The values of u;aandgbias the tendency of choosing
music according to user‚Äôs personal, age or gender music pref-
erences. We would like the tendency to be learned from the
data, thusu,a,gare all set to 1. In Gibbs sampling for
the training of topic models, 100 sampling iterations were
run as burn-in iterations and then 50 sampling iterations
with a gap of 10 were taken to obtain the Ô¨Ånal results. In
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
661Table 6: Comparison of retrieval performance on
TC1
Method1-Term Query 2-Term Query 3-Term Query
P@10 MAP P@10 MAP P@10 MAP
TAG .164 .134 .054 .041 .022 .014
WLC .178 .142 .058 .051 .022 .023
GBR .248 .267 .136 .147 .122 .125
GUMR .133 .115 .030 .026 .015 .012
G-MIT .276 .283 .139 .155 .116 .134
A-MIT .250 .259 .135 .151 .111 .128
AG-MIT .339* .335* .177* .184* .149* .166*
the result presentation in Sect. 5, the reported results are
based on the parameters with the best results.
5 EXPERIMENTAL RESULTS
In all the reported results, the symbol (*) after a numeric
value denotes signiÔ¨Åcant diÔ¨Äerences ( p< 0:05, a two-tailed
paired t-test) with the corresponding second best measure-
ment. In our experiments, for each user group (i.e., male
users between 16-20 years old), all the 1-term, 2-term and
3-term queries are used for retrieval and evaluation. All the
results presented are the average values over all user groups
in each test collection.
5.1 Performance on TC1
5.1.1 Retrieval Performance. Retrieval results of the pro-
posed methods using age and/or gender information with the
baselines are reported in Table 6. From the table, we observe
that: First, for the three types of queries, it is obvious that
queries with more terms is more diÔ¨Écult for all methods.
As can be seen, the proposed method using age and gender
information (AG-MIT) outperforms all the other methods
over all types of queries. Besides, A-MIT and G-MIT meth-
ods obtain comparable or better performance over the GBR
method, which shows the best performance besides our pro-
posed methods. The results demonstrate the eÔ¨Äectiveness of
the proposed retrieval methods.
Second, compared to the TAG method which only uses
text information in retrieval, WLC and GBR that use both
text and acoustic features obtain better performance. WLC
uses a linearly combination of similarities based on TAG and
acoustic features. It can only slightly improve the search
performance. Notice that the WLC uses the Ô¨Årst search
results of TAG as acoustic query, the search accuracy of
TAG thus aÔ¨Äects the improvement of the WLC method.
GBR method explore both text and acoustic information by
discovering and using the intrinsic correlation between the
semantics of terms and the acoustic contents. It obtain much
better results than the TAG method.
Third, it can be seen that the G-MIT and A-MIT methods
can improve the search performance over the GBR method
for 1-term query. The AG-MIT method can further improvethe performance for 1-term, 2-term, and 3-term queries. The
eÔ¨Äects of using age or gender information in retrieval are
comparable, and the gender information seems to be slightly
more eÔ¨Äective than age information. The performance of
the AG-MIT method is obviously better than the G-MIT
and A-MIT methods, indicating that the use of both age and
gender information together is more eÔ¨Äective than using them
individually. The results of GUMR are quite poor, because
of the simple method on modeling user‚Äôs music preferences
with age and gender information.
5.1.2 Re-ranking Performance. This section presents the
re-ranking performance based on the top 100 results of dif-
ferent retrieval methods (i.e., TAG, WLC, and GBR). The
results are reported in Table 7. In the table, the row starting
with ‚Äú-" shows the performance obtained by the correspond-
ing baseline methods. Overall, the results are improved
greatly and signiÔ¨Åcantly by the re-ranking methods, even
for 2- and 3-term queries whose initial results are very poor.
An interesting Ô¨Ånding is that the initial results of the TAG
method is worse than the WLC method, however, the TAG
method can obtain much better re-ranking results than the
WLC method by all re-ranking methods. The results indicate
that the WLC method can obtain better search results in top
positions (e.g., top 10 results), while it reduces the number
of relevant results in a longer list (i.e. top 100 results).
The eÔ¨Äectiveness of the re-ranking methods based on the
proposed models can be observed by comparing our meth-
ods (A-MIT, G-MIT, or AG-MIT) with the PAR method.
The improvements of our methods are much greater than
that of the PAR method. Notice that the UIA-MIT model
explores the relevance between queries (semantic concepts
or tags) and songs in a latent music interest space, which is
discovered based on the music preferences of a large num-
ber of listeners. In other words, the method leverages the
collaborative knowledge of crowds to estimate the relevance
between query concepts and songs , or the music preference
of general users on songs with respect to query concepts.
The external knowledge is complementary to the information
used by the TAG, WLG, or GBR methods, which compute
the relevance between the query and song only based on
the contents. Consequently, using the estimated relevance
between query and song based on the UIA-MIT model for
re-ranking can signiÔ¨Åcantly improve the search performance.
The beneÔ¨Åts of utilizing user information in music retrieval
can be well demonstrated by the MPR method, which can
improve the search performance greatly using a heuristic
method - re-ranking the songs according to their popularity
in user groups. For the 1-term query on P@10, the rela-
tive improvement by MPR achieves more than 133%and
43%over the TAG and GBR methods, respectively. The
improvement over 2-term and 3-term queries is even larger.
G-MIT and A-MIT methods obtain much better results than
the MPR method. The AG-MIT method can further im-
prove the performance. Notice that improvements achieved
by A-MIT, G-MIT and AG-MIT are contributed by both
the learned associations between query and songs and the
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
662Table 7: Comparison of re-ranking performance on TC1
Method1-Term Query 2-Term Query 3-Term Query
TAG WLC GBR TAG WLC GBR TAG WLC GBR
P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP P@10 MAP
-.164 .134 .178 .142 .248 .267 .054 .041 .058 .051 .136 .147 .022 .014 .022 .023 .112 .125
PAR .233 .249 .086 .074 .330 .309 .099 .116 .036 .035 .197 .199 .068 .078 .022 .023 .182 .175
MPR .383 .409 .185 .148 .355 .347 .237 .257 .063 .069 .241 .257 .184 .204 .039 .042 .206 .223
G-MIT .460 .470 .197 .223 .439 .450 .260 .278 .069 .079 .263 .268 .191 .209 .048 .053 .215 .230
A-MIT .444 .470 .195 .221 .431 .434 .254 .277 .077 .086 .273 .285 .192 .211 .045 .046 .215 .233
AG-MIT .479* .495 .228* .242 .444* .463* .267 .294* .087* .091* .285* .287 .200 .225* .053 .048 .226 .246*
Table 8: Comparison of re-ranking performance on
TC2 based on the results of TAG
Method1-Term Query 2-Term Query 3-Term Query
P@10 MAP P@10 MAP P@10 MAP
TAG .122 .120 .042 .020 .022 .017
MPR .216 .277 .135 .138 .103 .106
A-MIT .240 .318 .146 .182 .103 .106
G-MIT .228 .304 .135 .158 .104 .108
C-MIT .244 .318 .144 .181 .104 .107
AG-MIT .252 .330 .144 .176 .104 .108
AGC-MIT .375* .505* .160 .199* .105 .110
captured age and gender music preferences in UIA-MIT. The
relative improvement of the AG-MIT for the 1-term query
onP@10achieves more than 192%and 79%over the TAG
and GBR methods, respectively. UIA-MIT captures the age
music preference and gender music preference together and
achieves consistent and better improvement over A-MIT and
G-MIT. It shows that the inÔ¨Çuence of age and gender are
correlated in aÔ¨Äecting users‚Äô music interests. Thus, it is not
optimal to use age-based music preference and gender-based
music preference individually.
5.2 Performance on TC2
Similar results can be observed in TC2 for both ad-hoc and
re-ranking tasks. Due to space limitation, we only present
the performance of re-ranking performance based on TAG,
because from the experimental results on TC1: (1) using
our method in re-ranking can greatly improve the search
results; and (2) re-ranking performance based on TAG are
comparable or better than the performance based on WLC
and GBR. Table 8 presents the re-ranking results of diÔ¨Äerent
methods on TC2. The second row shows the search results
of TAG method, and 3 - 9 rows show the re-ranking results
of diÔ¨Äerent methods. From the table, it can be observed
that user‚Äôs country information (C-MIT) can also be used
to improve the performance. AGC-MIT obtains the best
performance, demonstrating that the UIA-MIT model can
be easily extended to include other types of user information
and the utilization of more types of user information can
obtain better performance.Table 9: Mean values of in the UIA-MIT model
Test Male Groups Female Groups
Collection agcagc
TC1 .119.170-.149.217-
TC2 .089.137.183.143.179.185
5.3 Impact of DiÔ¨Äerent User Information
In the UIA-MIT model, u,a, andgcontrol the contri-
butions of users‚Äô unique music interests and general music
interests of age and gender in music selection. Thus, the
values of these parameters show the relative importance of
diÔ¨Äerent types of user information in users‚Äô music preferences.
Table 9 shows the mean values of a,g, andcof male and
female groups in two test collections. cis a similar parame-
ter asa, controlling the weight of the general music interest
of users in diÔ¨Äerent countries. Notice that UIA-MIT in TC1
has not considered country information. The values of those
three parameters vary greatly across diÔ¨Äerent user groups,
indicating that those three factors are inter-correlated to af-
fect users‚Äô music preferences. Overall, the diÔ¨Äerences of three
parameters between male and female groups are more obvi-
ously (comparing to diÔ¨Äerent age or country groups). In the
table, we show the values of three parameters in male groups
and female groups separately. Some interesting observations
can be found on the global level: (1) users‚Äô personal music
interests (u) dominate the music selection, as u>0:56; (2)
the value of cis greater than those of ganda, indicating
that the eÔ¨Äects of country on music selection is larger than
age and gender; (2) the value of gis slightly larger than
that ofa, indicating that gender has more impact on the
music preferences than age.
6 CONCLUSION
In this paper, we proposed a novel User-Information-Aware
Music Interest Topic (UIA-MIT) model to discover the latent
music interest space of general users and capture the music
preferences of users in diÔ¨Äerent ages and genders. Based
on the proposed model, a music retrieval method is devel-
oped for text-based music retrieval, which can eÔ¨Äectively
incorporate user information to improve the search results.
Extensive experiments were conducted to demonstrate the
eÔ¨Äectiveness of exploiting user‚Äôs age and gender information
6u= 1 a gin TC1 oru= 1 a g cin TC2
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
663in music retrieval. The results demonstrate the importance
and potential of utilizing user-speciÔ¨Åc information in music
retrieval systems. We hope this work can shed light on the
direction of developing user-centric music retrieval systems
and motivate more research eÔ¨Äorts in this area.
ACKNOWLEDGMENTS
This research is supported by the National Research Foun-
dation, Prime Minister‚Äôs OÔ¨Éce, Singapore under its Interna-
tional Research Centre in Singapore Funding Initiative.
REFERENCES
[1]L. Barrington, M. Yazdani, D. Turnbull, and G. RG Lanckriet.
2008. Combining feature kernels for semantic music retrieval. In
ISMIR.
[2]P. Berkers. 2012. Gendered scrobbling: Listening behaviour of
young adults on Last.fm. Interactions: Studies in Communica-
tion & Culture 2, 3 (2012), 279‚Äì296.
[3]T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere. 2008.
Autotagger: A model for predicting social tags from acoustic fea-
tures on large music databases. Journal of New Music Research
37, 2 (2008), 115‚Äì135.
[4]D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation.
J. Mach. Learn. Res. 3 (2003), 993‚Äì1022.
[5]J. Bu, S. Tan, C. Chen, C. Wang, H. Wu, L. Zhang, and X. He.
2010. Music recommendation by uniÔ¨Åed hypergraph: combining
social media information and music content. In ACM MM.
[6]M. Casey, C. Rhodes, and M. Slaney. 2008. Analysis of minimum
distances in high-dimensional musicalspaces. IEEE Trans. Audio,
Speech, Language Process. 16, 5 (2008), 1015‚Äì1028.
[7]M. A Casey, R. Veltkamp, M. Goto, M. Leman, C. Rhodes, and M.
Slaney. 2008. Content-based music information retrieval: Current
directions and future challenges. Proc. IEEE 96, 4 (2008), 668‚Äì
696.
[8]√íscar Celma. 2010. Music Recommendation. In Music Recom-
mendation and Discovery , √íscar Celma (Ed.). Springer, Chap-
ter 3, 43‚Äì86.
[9]Z. Cheng and J. Shen. 2014. Just-for-me: An adaptive personal-
ization system for location-aware social music recommendation.
InACM ICMR .
[10]Z. Cheng and J. Shen. 2016. On eÔ¨Äective location-aware music
recommendation. ACM Trans. Inf. Syst. 34, 2 (2016), 13.
[11]Z. Cheng, J. Shen, and S. CH Hoi. 2016. On eÔ¨Äective personal-
ized music retrieval by exploring online user behaviors. In ACM
SIGIR.
[12]J S. Downie. 2008. The music information retrieval evaluation
exchange (2005-2007): A window into music information retrieval
research. Acoustical Science and Technology 29, 4 (2008), 247‚Äì
255.
[13]K. Ellis, E. Coviello, A. B. Chan, and G. RG Lanckriet. 2013.
A bag of systems representation for music auto-tagging. IEEE
Trans. Audio, Speech, Language Process. 21, 12 (2013), 2554‚Äì
2569.
[14]T. L GriÔ¨Éths and M. Steyvers. 2004. Finding scientiÔ¨Åc topics.
PNAS101, Suppl 1 (2004), 5228‚Äì5235.
[15]B.-J. Han, S. Rho, S. Jun, and E. Hwang. 2010. Music emotion
classiÔ¨Åcation and context-based music recommendation. Mul-
timed. Tools Appl. 47, 3 (2010), 433‚Äì460.
[16]N. Hariri, B. Mobasher, and R. Burke. 2013. Personalized text-
based music retrieval. In Workshops at AAAI Conference on
ArtiÔ¨Åcial Intelligence.
[17]T. Hofmann. 1999. Probabilistic latent semantic indexing. In
ACM SIGIR .
[18]T. Hofmann and J. Puzicha. 1999. Latent class models for collab-
orative Ô¨Åltering. In IJCAI.
[19]J.-Y. Kim and N. J Belkin. 2002. Categories of music description
and search terms and phrases used by non-music experts. In
ISMIR.
[20]P. Knees, T. Pohle, M. Schedl, D. Schnitzer, and K. Seyerlehner.
2008. A document-centered approach to a natural language music
search engine. In ECIR.
[21]P. Knees, T. Pohle, M. Schedl, D. Schnitzer, K. Seyerlehner, and
G. Widmer. 2009. Augmenting text-based music retrieval withaudio similarity. In ISMIR.
[22]A. LeBlanc, Y. Jin, L. Stamou, and J. McCrary. 1999. EÔ¨Äect of
age, country, and gender on music listening preferences. Bull.
Counc. Res. Music Educ. 141 (1999), 72‚Äì76.
[23]M. Levy and M. Sandler. 2008. Learning latent semantic models
for music from social tags. J. New Music Res. 37, 2 (2008),
137‚Äì150.
[24]M. Levy and M. Sandler. 2009. Music information retrieval using
social tags and audio. IEEE Trans. Multimed. 11, 3 (2009),
383‚Äì395.
[25]C. Liem, M. M√ºller, D. Eck, G. Tzanetakis, and A. Hanjalic. 2011.
The need for music information retrieval with user-centered and
multimodal strategies. In ACM workshop on Music Information
Retrieval with User-centered and Multimodal Strategies .
[26]R. Miotto and G. Lanckriet. 2012. A generative context model
for semantic music annotation and retrieval. IEEE Trans. Audio,
Speech, and Language Process. 20, 4 (2012), 1096‚Äì1108.
[27]R. Miotto and N. Orio. 2012. A probabilistic model to combine
tags and acoustic similarity for music retrieval. ACM Trans. Inf.
Syst.30, 2 (2012), 8.
[28]J. Nam, J. Herrera, M. Slaney, and J. O. Smith. 2012. Learning
sparse feature representations for music annotation and retrieval.
InISMIR.
[29]H.-S. Park, J.-O. Yoo, and S.-B. Cho. 2006. A context-aware
music recommendation system using fuzzy bayesian networks with
utility theory. In FSKD.
[30]A. Popescul, D. Pennock, and S. Lawrence. 2001. Probabilistic
models for uniÔ¨Åed collaborative and content-based recommenda-
tion in sparse-data environments. In UAI.
[31]M. Riley, E. Heinen, and J. Ghosh. 2008. A text retrieval approach
to content-based audio retrieval. In ACM SIGIR .
[32]G. Salton, A. Wong, and C.-S. Yang. 1975. A vector space model
for automatic indexing. Commun. ACM 18, 11 (1975), 613‚Äì620.
[33]M. Schedl and A. Flexer. 2012. Putting the user in the center of
music information retrieval. In ISMIR.
[34]M. Schedl, A. Flexer, and J. Urbano. 2013. The neglected user
in music information retrieval research. J. Intell. Inf. Syst. 41, 3
(2013), 523‚Äì539.
[35]M. Schedl, D. Hauger, K. Farrahi, and M. Tkalƒçiƒç. 2015. On
the inÔ¨Çuence ofuser characteristics on music recommendation
algorithms. In ECIR.
[36]M. Schedl, S. Stober, E. G√≥mez, N. Orio, and C. CS Liem. 2012.
User-aware music retrieval. Multimod. Music Process. 3 (2012),
135‚Äì156.
[37]J. Shen, W. Meng, S. Yan, H. Pang, and X. Hua. 2010. EÔ¨Äective
music tagging through advanced statistical modeling. In ACM
SIGIR.
[38]J. Shen, H. Pang, M. Wang, and S. Yan. 2012. Modeling concept
dynamics for large scale music search. In ACM SIGIR .
[39]D. Tingle, Y. E Kim, and D. Turnbull. 2010. Exploring automatic
music annotation with acoustically-objective tags. In ISMIR.
[40]D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet. 2008.
Semantic annotation and retrieval of music and sound eÔ¨Äects.
IEEE Trans. Audio, Speech, Language Process. 16, 2 (2008),
467‚Äì476.
[41]D. Turnbull, L. Barrington, D. Torres, and G. RG Lanckriet. 2007.
Towards musical query-by-semantic-description using the cal500
data set. In ACM SIGIR .
[42]D. R Turnbull, L. Barrington, G. Lanckriet, and M. Yazdani.
2009. Combining audio content and social context for semantic
music discovery. In ACM SIGIR .
[43]A. L Uitdenbogerd and R. Schyndel. 2002. A review of factors
aÔ¨Äecting music recommender success. In ISMIR.
[44]J.-C. Wang, Y.-C. Shih, M.-S. Wu, H.-M. Wang, and S.-K. Jeng.
2011. Colorizing tags in tag cloud: a novel query-by-tag music
search system. In ACM MM .
[45]M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. 2016. Scalable
semi-supervised learning by eÔ¨Écient anchor graph regularization.
IEEE Trans. Knowledge Data Eng. 28, 7 (2016), 1864‚Äì1877.
[46]M. Wang, X. Liu, and X. Wu. 2015. Visual ClassiÔ¨Åcation by
ell1-Hypergraph Modeling. IEEE Trans. Knowledge Data Eng.
27, 9 (2015), 2564‚Äì2574.
[47]K. Yoshii, M. Goto, K. Komatani, T. Ogata, and H. G Okuno.
2008. An eÔ¨Écient hybrid music recommender system using an
incrementally trainable probabilistic generative model. IEEE
Trans. Audio, Speech, Language Process. 16, 2 (2008), 435‚Äì447.
[48]B. Zhang, J. Shen, Q. Xiang, and Y. Wang. 2009. CompositeMap:
a novel framework for music similarity measure. In ACM SIGIR .
Session 6A: Personalization and Privacy
SIGIR‚Äô17, August 7-11, 2017, Shinjuku, Tokyo, Japan
664