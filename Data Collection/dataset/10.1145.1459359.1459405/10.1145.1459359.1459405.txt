Search Trails using User Feedback to Improve Video 
Search  
 
*Frank Hopfgartner  *â€ David Vallet  *Martin Halvey  *Joemon Jose  
 
*Department of Computing Science,  
University of Glasgow,  
Glasgow, Scotland, United Kingdom.  â€ Universidad AutÃ³noma de Madrid,  
Escuela PolitÃ©cnica Superior Ciudad Universitaria de 
Cantoblanco, 28049 Madrid, Spain.  
 
{hopfgarf, halvey, jj} @ dcs.gla.ac.uk, david.vallet@uam.es  
ABSTRACT  
In this paper we present a n innovative  approach  for aiding users in 
the difficult task of video search.  We use community based 
feedback  mined  from  the interactions of previous users of our 
video search system to aid users in their search tasks. This 
feedback  is the basis for providing recommendations to users of 
our video retrieval system . The ultimate goal  of this system is to 
improve the quality of the results that users find, and in doing so , 
help users to explore a large and difficult information space and 
help them consider search options that they may not have 
considered otherwise.  In particular we wish to make the difficult 
task of search for video much easier for users.  The results of a 
user evaluation indicate that we achieved our goals, the 
performance of the users in retrieving relevant videos improved, 
and users were able to explore the collection to a greater extent.  
Categories and Subject Descriptors  
H.5.1 Multimedia Information Systems,  H.5.3 Group and 
Organization Interfaces  
General Terms  
Experimentation, Human Factors.  
Keywords  
Video, search, collaborative,  community,  feedback, recommender, 
user studies.  
1. INTRODUCTION  
With the improving capabilities and the decreasing prices of 
current hardware systems, there are ever growing possibilities to 
store and manipulate videos in a digital format. In addition to this , 
with ever increasing broadband capabilities it is now feasible to 
view video online at home as easily as text -based pages were 
viewed when the Web first appeared .  People now build their own 
digital libraries from materials created through digital cameras 
and camcorders, and use a number of systems to place this 
material on the web, as well as store them as their own personal 
collection. However the systems that c urrently exist to organise and retrieve theses videos are not sufficient to deal with such large 
and rapidly growing volumes of video. In particular there is an 
ever increasing need to develop tools and techniques to assist 
users in the complex task of sea rching for video clips. Current 
state of the art systems rely on using annotations provided by 
users, methods that use the low level features available in the 
videos or on an existing representation of concepts associated 
with the retrieval tasks. None of these methods are sufficient 
enough to overcome the problems associated with video search 
(see Section 2.1 for a full discussion).  
In order to alleviate some these problems associated with video 
search we have developed a video retrieval  system  that uses the 
actions involved in previous user searches to help and inform 
future users of the system, through video recommendation. Our 
system does not require users to alter their normal searching 
behaviour, provide annotations or provide any other 
supplementary  feedback . We achieve this  outcome  by utilising the 
available  information about user interactions. In addition to this , 
our system does not require a representation of the concepts in the 
video that  the user wishes to retrieve, while still offering a work 
around for the problems associated with the semantic gap [11]. 
We believe that the use of this system  can result in a number of 
desirable outcomes for users. In particular , improved user 
performance in terms of task completion, it can aid user 
exploration of the collection and can also increase user 
satisfaction with their search and their search results.  An 
evaluative study was conducted, in order to examine and validate 
these assumptions. A baseline system that provides no 
recommendations was compared with our system that provides 
recommendations. The two systems and their respective 
performances were ev aluated both qualitatively and 
quantitatively.  
The remainder of this paper is organised as follows: We will 
provide a rationale for our work, and describe the state of the art 
in a video search. Subsequently, in Section 3 we will describe our 
approach for using implicit feedback to provide recommendations. 
Section 4 will describe two systems which were used in our study. 
In Section 5 we will then describe our experimental methodology, 
which is followed by the results of our experiments. Finally we 
will prov ide a discussion of our work and some conclusions.   
2. BACKGROUND AND MOTIVATION  
2.1 Interactive Video Retrieval  
Interactive video retrieval refers to the process of users 
formulating and carrying out video searches, and subsequently  
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted withou t fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires pr ior specific permission and/or a fee.  
MMâ€™08, October 26 -31, 200 8, Vancouver , British Columbia , Canada . 
Copyright 200 8 ACM 978-1-60558 -303-7/08/10 â€¦$5.00.  
 
339reformulating queries and res ults based on the previously 
retrieved results. As video is extremely rich content there are a 
number of different ways that users can query video retrieval 
systems. The use of the low -level features that are available in 
images and videos, such as colour,  texture and shape to retrieve 
results , is one  common approach. This approach is often used for 
query by example, where users provide sample images or video 
clips as examples to retrieve similar images or video clips. While 
this approach seems reasonable i t also presents a number of 
problems . It requires a representation and extraction of all of the 
features required from all of the videos presenting issues of 
efficiency.  Also the difference between  the low-level data 
representation of videos and the highe r level concepts users 
associate with video, commonly known as the semantic gap [11], 
provide difficulties. Bridging the semantic gap is one of the most 
challenging  research issues in multimedia information retrieval 
today. In an attempt to bridge this semantic gap, a great deal of 
interest in the multimedia search community has been invested in 
search by concept. The idea is that semantic concepts such as 
â€œvehicleâ€ or â€œpersonâ€ can be used to aid retrieval; an example of 
this is the Large Scale Ontology for Multimedia (LSCOM) [14]. 
However query by concept also has a number of issues that hinder 
its use, it requires a large number of concepts to be represented 
and to date has not been deployed on a large scale for general 
usage.  
Query by text is the most popular method  of searchin g for video. 
It is used in many large scale video retrieval systems such as 
YouTube and GoogleVideo, and is also the most popular query 
method at TRECVID [2]. Quer y by text is simple and users are 
familiar with this paradigm from text based searches, in addition 
to this , query by text does not require a representation of concepts 
or features associated with a video.  Query by text does however 
rely on the availabili ty of sufficient textual descriptions of the 
video and its content. Textual descriptions in some cases may be 
extracted from closed captions or through automatic speech 
recognition; however a study of a number of state of the art video 
retrieval systems [10] concludes that the availability of these 
additional resources varies for different systems. Where these 
resources are available , they may not always b e reliable,  due to 
limitations i n automatic speech recognition or language 
differences for example. More recent state of the art online 
systems, such as YouTube and Google Video, rely on using 
annotations provided by users to provide descriptions of  videos. 
However, quite often users can have very different perceptions 
about the same video and annotate that video differently. This can 
result in synonyms, polysemy and homonymy, which makes it 
difficult for other users to retrieve the same video.  It has also 
been found  that users are reluctant to provide an abundance of 
annotations unless there is some benefit to the user [7].  
While each of these methods outlined above have pro blems , they 
have been used in conjunction with each other in a number of 
systems, including Informedia [2] and MediaMill [20]. These 
systems have been amongst the most successful systems at recent 
TRECVID interactive search evaluations. However, these top 
results are for â€œexpertâ€ users, who establish the idealistic 
performance of users [3], also a combination of these approaches 
requires a vast amount of metadata to be extracted and stored for 
each individual video clip.  
As we  have seen there are a number of different ways in which a 
user can query a video retrieval system;  as has been shown  these 
include query by text, query by example and query by concept. Each of these approaches have had limited success, and to date 
none of  these approaches has provided an adequate solution to 
providing the tools to facilitate video search [2]. With this in mind 
we are proposing a n approach  that util ises the actions involved in 
previous usersâ€™ searches to provide feedback to help future 
searches . This collaborative feedback based approach does not 
require any additional representation of video clips, unlike query 
by example, or any additional metadata , unlike query by concept 
or query by text, but instead uses the actions that users would 
carry out naturally while searching for video , to improve their 
search results . 
2.2 Collaborative Information Access  
Many of the earliest collaborative techniques emerged  online in 
the 1990â€™s [6], [15], [17]. Since those early days collaborative or 
community based methods have evolved and been used to aid 
browsing [22], e-learning [5] and in collaborative search engines 
[19]. These techniques rely on user feedback.  Relevance feedback  
based on the content of video has also been used in conjunction 
with related information, e.g. tags, to provide video search 
recommendations to user s [25]. However, we believe that such 
techniques are insufficient where there is a lack of associated 
information [7] and will also suffer from problems associated with 
the semantic gap [11]. There has also been some recent initial 
research into carrying out collaborative video search [1]. This 
work, however, concentrated on two users carrying out a search 
simultaneously rather than using the implicit interactions from 
previous searches to improve future searches.  
Traditionally explicit re levance feedback has been used  to provide 
feedback for these methods ; however there are a number of 
problems with this approach. Providing explicit feedback can be a 
cognitively taxing process, users are forced to update their need 
constantly and this can be a difficult process when their 
information need is vague [21] or when they are unfamiliar with 
the document collection [16]. Also previous evaluations have 
found that users of explicit feedback systems often do not provide 
sufficient levels of feedback for adaptive retrieval algorithms to 
work [8]. With this in mind in our system we concentrate on  
implicit relevance feedback.  
Implicit feedback has been shown to be a good indicator  of 
interest in a number of areas in IR [12].  Hopfgartner et al. [9] 
have suggested that implicit relevance fe edback can aid users 
searching in digital video library systems. White et al. [23] use the 
concept of â€œsearch trailsâ€, meaning the search queries and 
document inter actions sequences performed by the users during a 
search session, to enhance web search. Craswell and Szummer [4] 
apply a random walk on a graph of user click  through data, to help 
retrieve relevant documents for user searches. Liu et al. [13] used 
a graph representation based on the textual features associated 
with a video to improve result list ranking.  Yang et al. [25] 
provide  a multi -modal content -based video recommender system 
thatâ€™s uses a combination of  textual similarity over A SR and OCR 
data, visual similarity and aural similarity , in conjunction with 
relevance feedback . The relevance feedback is applied when 
fusing the multimodal rankings, to give more  to a particular  
feature depending on negative and positive relevance exampl e. 
For example , if a user search for â€œMercedesâ€ and clicks on 
recommended videos which share visual similarities, the 
recommendation system will give more weight on the visual 
feature for the recommendations of the current search session.  
This recommendati on approach is content -based, whereas the 
approach that we use in this paper is based on  click  through data . 
340Using some of this previous work  that uses click through data  [4], 
[23] as a basis , we have developed our own graph based model of 
implicit actions  and recommendation strategy , which we use to 
provide recommendations. This model is described in detail in a 
follow ing section , but first we provide a description of our video 
retrieval system . 
3. SYSTEM DESCRIPTION  
Our collaborative feedback approach has been implemented in an 
interactive video retrieval system. This allows us to have actual 
end users test our system and  approach. The system consists of 
four main components, a search interface, a keyframe index, a 
retrieval engine and our recommendation model. The keyframes 
in our keyframe index were indexed based on automatic speech 
recognition transcript and machine tra nslation output. The 
retrieval engine is based on t he Okapi BM25 retrieval model , 
which was used to rank retrieval results that were returned to the 
user by text searches. In addition to the ranked list of search 
results, the system provides users with add itional 
recommendations of video shots that might match their search 
criteria based on our recommendation graph (see Section 4 for 
details on the recommendation graph).   
The interface for this system is shown in Figure 1 and can be 
divided into three main panels, search panel (A), result panel (B) 
and playback panel (C). The search panel (A) is where users 
formulate and carry out their searches. Users enter a text based 
query in the search panel (A) to begin their search.  The users are 
presented with text based recommendations for search queries that 
they can use to enhance their search (b). The users are also 
presented with recommendations of video shots that might match 
their search criteria (a), each recommendation is o nly presented 
once, but may be retrieved by the user at a later stage if they wish 
to do so.  
The result panel is where users can view the search results (B). 
This panel is divided into five tabs, the results for the current search, a list of results that the user has marked as relevant, a list 
of results that the user has marked as maybe being relevant, a list 
of results that the user has marked as irrelevant and a list of 
recommendations that the user has been presented with 
previously. Users can mark res ults in these tabs as being relevant 
or irrelevant by using a sliding bar (c). In the result panel 
additional information about each video shot can be retrieved . 
Hovering  the mouse tip over a video keyframe, will result in that 
keyframe being highlighted, along with neighbouring keyframes 
and any text associated with the highlighted keyframe (d). The 
playback panel (C) is for viewing video shots (g). As a video is 
playing it is possible to view the current keyframe for that shot 
(e), any text associated wit h that keyframe (f) and the 
neighbouring keyframes. Users can play, pause, stop and can 
navigate through the video as they can on a normal media player, 
and also make relevance judgements about the keyframe (h). 
Some of these tools in the interface allow u sers of the system to 
provide the explicit and implicit feedback, which  is then used to 
provide recommendations to future users. Explicit feedback is 
given by users by marking video shots as being either relevant or 
irrelevant (c, h). Implicit feedback is given by users playing a 
video (g), highlighting a video keyframe (d), navigating through 
video keyframes (e) and selecting a video keyframe (e).   
In order to provide a comparison to our recommendation system, 
we also implemented a baseline system that pr ovides no 
recommendations to users . The baseline system has previously 
been used for the interactive search task track at TRECVID 2006 
[18]; the performance of thi s system was average when compared 
with other systems at TRECVID that year. A tooltip feature which 
shows neighbouring keyframes and the transcript of a shot was 
added to this system to improve its performance. Overall the only 
difference between the basel ine and recommendation system is 
the provis ion of keyframe recommendations (a). 
 
 
Figure 1: Interface of the video retrieval syste m
 
3414. FEEDBACK BASED 
RECOMMENDATION:  A GRAPH BASED 
REPRESENTATION  
For the implementation of our recommendation model  based on 
user actions, there are two main desired properties of the model 
for action  information storage. The first property is the 
representation of all of the user interactions with the system, 
including the sea rch trails for each in teraction. T his allows us to 
fully exploit all of the interactions to provide richer 
recommendations. The second property is the aggregation of 
implicit information from multiple sessions and users into a single 
representation, thus facilitating the analys is and exploitation of 
past implicit information. To achieve these properties we opt for a 
graph -based representation of the usersâ€™ implicit information. We 
take the concept of trails f rom White et al . [23]; however unlike 
White et al.  [23] we do not limit the possible recommended 
documents to those documents that are at the end of the search 
trail. The reason for this is that we believe  that during an 
interactive search the doc uments that most of the users with 
similar interaction sequences interacted with are the documents  
that could be most relevant for recommendation , not just the final 
document in the search trail . Thus the main difference between 
our search trail and that o f White et al . [23] is that ours is a more 
complex  representation . Similar to Craswell and Szummer [4], our 
approach represents queries and documents in the same graph, 
however we represent the whole interaction sequence, unlike their 
approach, where the clicked documents are linked directly t o the 
query node.  The approach from Crasswel l and Szummer [4] does 
not represent search trails;  their approach is  based on finding 
correlations query -clicked docum ent.  We use search trails  
because once again we want to recommend potentially important 
documents that are part of the interaction sequence.  Another 
difference between our approach and previous work is that we 
take into consideration other types of implic it feedback actions, 
related to multimedia search, e.g. length of play time, browsing 
keyframes etc., as well as click through data. This additional data 
allows us to provide a richer representation of user actions and 
potentially better recommendations. Overall our representation 
exploits a greater range of user interactions in comparison with 
other approaches [4], [23], [25],  this results in a more full  
representation of a wide range of user actions that may facilitate 
better recommendations.  In addition while these other approaches 
[4], [23] have been successful in other domains they have not 
been applied to video search.   These properties an d this approach 
results in two graph -based represen tations of user actions. The 
first uses a Labelled Directed Multigraph (LDM) for the detailed 
and full representation of implicit information. The second graph 
is a Weighted Directed Graph (WDG), which interprets the 
information given by the LDM and repres ents it in such a way that 
is more easily exploitable for a recommendation algorithm. In our 
system the recommendations are based on three different analyses 
techniques based on the WDG. The two graph representation 
techniques and the recommendation techni ques are described in 
detail in the following sections.  
4.1 Labelled Directed Multigraph (LDM)  
A user session s can be represented as a set of queries  ğ‘„ğ‘ , which 
were input by the user ğ‘¢, and a set of multimedia documents ğ·ğ‘  
the users interacted with during the search session. Queries and documents are represented as nodes ğ‘ğ‘ = ğ‘„ğ‘  âˆªğ·ğ‘   of our graph 
represent ation,  ğºğ‘  = ğ‘ğ‘ ,ğ´ğ‘  . The interactions of the user during 
the search session are represente d as a set of actions arcs  
ğ´ğ‘  ğº = ğ‘›ğ‘–,ğ‘›ğ‘—,ğ‘,ğ‘¢,ğ‘¡ , each action arc indicates that, at a time  ğ‘¡,  
the user ğ‘¢ performed an action of type a that lead the user from 
the query or document node ğ‘›ğ‘– to node ğ‘›ğ‘—, ğ‘›ğ‘–,ğ‘›ğ‘—âˆˆğ‘ğ‘  . Note that  
ğ‘›ğ‘—  is the object of the action and that actions can be reflexive, for 
instance when a user clicked to view a video and then navigate 
through it. Actions types depend on the kind of actions recorded 
by the implicit feedback system , in our system we recorded 
playing a video, navigating through a video, highlighting a video 
to get additional metadata and selecting a video . Links can contain 
extra associated metadata, as type specific attributes, e.g. length of 
play in a play type action. The graph is multilinked , as different 
actions can have same source and destination nodes. The session 
graph ğºğ‘  = ğ‘ğ‘ ,ğ´ğ‘   will then be constructed by all the accessed 
nodes and linking actions, and will represent the whole interaction 
process for the userâ€™s session  s. Finally, all the session -based 
graphs can be aggregated into a single graph ğº =ğº ğ‘,ğ´ , 
ğ‘= ğ‘ğ‘ ğ‘ ,  ğ´= ğ´ğ‘ ğ‘  which represents the overall pool of 
implicit information.  Quite simply all of the nodes from the 
individual graphs ar e mapped to one large graph, and then  all of 
the action edges are mapped onto the same graph. This graph may 
not fully connected, as it is possible that users selected different 
paths through the data or entered a query and took no further 
actions etc.  While the LDM gives a detailed representation of user 
interaction with the collection, it is extremely difficult to use to 
provide recommendations. The multiple links make the graph 
extremely complex. In addition to this all of the actions are 
weighted  equall y, this is not always  a true representation ; some 
actions may be more important than others and should be 
weighted  different ly. 
4.2 Weighted Directed Graph (WDG)  
In order to exploit  the previous representation by our 
recommendation algorithm , we convert  the LDM to a WDG by 
collapsing all links interconnecting two nodes into one  single 
weighted edge . This pr ocess is carried out as follows. G iven the 
detailed LDM graph  of a session s, ğºğ‘  = ğ‘ğ‘ ,ğ´ğ‘  , we compute 
the interpreted weighted graph ğºğ‘ = ğ‘ğ‘ ,ğ‘Šğ‘  . Links ğ‘Šğ‘ =
 ğ‘›ğ‘–,ğ‘›ğ‘—,ws   indicate that at least one action lead the user from the 
query or document node ğ‘›ğ‘– to ğ‘›ğ‘—. The weight value ws represents  
the probability that  node  ğ‘›ğ‘—, was relevant to the user for the given 
session , this value is either given explicitly by the user, or 
calculated by means of the implicit evidence obtained from the 
interactions of the user with that node : 
ws ğ‘›ğ‘— = âˆ’1,ğ‘–ğ‘“ğ‘“ ğ‘’ğ‘¥ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘¡ ğ‘–ğ‘Ÿğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’ ğ‘“ğ‘œğ‘Ÿ ğ‘›ğ‘— 
 0,1 = ğ‘™ğ‘Ÿ ğ‘›ğ‘— ,ğ‘–ğ‘šğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘¡ ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’ ğ‘“ğ‘œğ‘Ÿ ğ‘›ğ‘— 
1,ğ‘–ğ‘“ğ‘“ ğ‘’ğ‘¥ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘¡ ğ‘Ÿğ‘’ğ‘™ğ‘’ğ‘£ğ‘ğ‘›ğ‘ğ‘’ ğ‘“ğ‘œğ‘Ÿ ğ‘›ğ‘—  
In the case that there is only implicit evidence for a node n, the 
probability value is given by the local relevance  ğ‘™ğ‘Ÿ ğ‘›  . ğ‘™ğ‘Ÿ ğ‘›  
returns a value between 0 and 1 that approximates a probability 
that node ğ‘› was relevant to the user given the different 
interactions that the user had with the node.  For instance if the 
user opened a video and play ed it for the whole of its duration, 
this can be enough evidence that the video has a high chance of 
being relevant to the user. Foll owing this idea, and based on 
previous work on the impact of implicit feedback importance 
weights [9], the local relevance function is defined as  
342ğ‘™ğ‘Ÿ ğ‘› =1âˆ’1
ğ‘¥ ğ‘›  , where  ğ‘¥ ğ‘›  is the total of added weights 
associated to each type of action in which node n is an object of. 
This subset of actions is defined as 
ğ´ğ‘  ğºğ‘ ,ğ‘› = ğ‘›ğ‘–,ğ‘›ğ‘—,ğ‘,ğ‘¢,ğ‘¡ ğ‘›ğ‘—=ğ‘› ,ğ‘› âˆˆğ‘ğ‘ , these weights are 
natural positive values returned by a function  ğ‘“ ğ‘ : ğ´â†’â„•  
mapping each type of action to a number. These weights ar e 
higher for an action that is understood to give more evidence of 
relevance to the user. In this way, ğ‘™ğ‘Ÿ ğ‘›  is closer to 1 as more 
actions are observed t hat involve n and the higher the associated 
weight given to each action type. In our weighting model some of 
the implicit actions are weighted nearly as highly as explicit 
feedback. The accumulation of implicit relevance weights can 
thus be calculated as  ğ‘¥ ğ‘› = ğ‘“ ğ‘âˆˆğ´ğ‘  ğºğ‘ ,ğ‘›  ğ‘ . Table 1 shows an 
exampl e of function  ğ‘“, used during our evaluation process;  all of 
these actions were described in the system description (see 
Section 3). As was stated earlier these weights are based on 
previous work on implicit feedback for video search [9].   Figure 
2 shows an example of LDM and its correspondent WDG for a 
given session.  
Action  f(a) Action  f(a) 
Play ( Sec)  3 Navigate Browse 
R/L 2 
View  10 Tooltip  1 
 
Table 1: Values for f function  for each action type  used in the 
system . 
 
Figure 2: Node based graph representation vs. weight based 
representation  for a search for â€œBushâ€  
Similarly to the detailed LDM graph, the session -based WDGs 
can be  aggregated into a single overall graph ğº= ğ‘,ğ‘Š , which 
will be called the implicit relevance pool, as  it collects all the 
implicit relevance evidence  of all users across all sessions. The 
nodes of the implicit pool are all the nodes involved in any past 
interaction  ğ‘= ğ‘ğ‘ ğ‘ , whereas the weighted links combine  the 
probabilities of all the session -based values . In our approach we 
opted  for a simple aggregation of the se probabilities ,  ğ‘Š=
 ğ‘›ğ‘–,ğ‘›ğ‘—,ğ‘¤ ,ğ‘¤= ğ‘¤ğ‘ ğ‘ . Each  link represent s the overall implicit 
(or explicit, if available) relevance that all users, which actions 
lead from node ğ‘›ğ‘– to ğ‘›ğ‘—, gave to node ğ‘›ğ‘—. Figure 3 shows an 
example of implicit relevance pool.  4.3 Relevance Pool  Based  Recommendation  
In our system we recommend  both queri es and documents to the 
users, t hese recommendations are based on the status of the 
current user session. As the user interacts with the system, a 
session -based WDG is constructed. The current userâ€™s se ssion is 
thus represented by ğºğ‘ â€²= ğ‘ğ‘ â€²,ğ‘Šğ‘ â€² . This graph is the basis of the 
recommendation algorithm which has three components; each 
component uses the implicit relevance pool in order to retrieve 
similar nodes that were somehow relevant to  other users.  The first 
two components are neighbourhood based. A neighbourhood 
approach is a way of obtaining related nodes; quite simply we 
define the node neighbourhood of a given node n, as the nodes 
that are within a distance d of n, without taking th e link 
directionality  into consideration . These nodes are somehow 
related to n by the actions of the users, either because the users 
interacted with n after interacting with the n eighbour nodes, or 
because they  are the nodes the user interacted with after 
interacting with n. More formally as a way of obtaining related 
nodes, we define the node neighbourhood of a given node ğ‘› as:  
ğ‘ğ»(ğ‘›)= ğ‘›1,â€¦,ğ‘›ğ‘€ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ ğ‘›,ğ‘›ğ‘š <ğ·ğ‘€ğ´ğ‘‹,ğ‘›ğ‘šâˆˆğ‘  
which are the nodes that are within a distance ğ·ğ‘€ğ´ğ‘‹ of  ğ‘› , not 
taking link directionality into account.  Using the properties 
derived from the implicit relevance pool, we can calculate the 
overall relevance value for a given node, this value indicates the 
aggregation of implicit relevance that  users gave historically to ğ‘›, 
when ğ‘› was involved with the usersâ€™ interactions. Given all the 
incident weighted links of ğ‘›, defined buy the subset ğ‘Šğ‘  ğºğ‘ ,ğ‘› =
 ğ‘›ğ‘–,ğ‘›ğ‘—,ğ‘¤ ğ‘›ğ‘—=ğ‘› ,ğ‘› âˆˆğ‘ğ‘ , the overall relevance value for ğ‘› is 
calculated a s follows:  
ğ‘œğ‘Ÿ ğ‘› = ğ‘¤
ğ‘¤âˆˆğ‘Šğ‘  ğºğ‘ ,ğ‘›  
Given the current session of the user and the implicit relevance 
pool we can then define the node recommendation value as:   
ğ‘›ğ‘Ÿ ğ‘›,ğ‘ğ‘ â€² = ğ‘™ğ‘Ÿâ€² ğ‘›ğ‘– âˆ™ğ‘œğ‘Ÿ ğ‘›  ğ‘›âˆˆğ‘ğ»(ğ‘›ğ‘–)  
ğ‘›ğ‘–âˆˆğ‘ğ‘ â€² 
where  ğ‘™ğ‘Ÿâ€² ğ‘›ğ‘–   is the local relevance computed for the current 
session of the user ğºğ‘ â€², so that the relevance of the node to the 
current session is taken into consideration. We can then define the 
first recommendation value ğ‘Ÿ1 ğ‘›,ğ‘ğ‘ â€² =ğ‘›ğ‘Ÿ ğ‘›,ğ‘„ğ‘ â€²  ğ‘„ğ‘ â€²âˆˆğ‘ğ‘ â€² , i.e. 
the node recommendation value for the queries related to the 
current session. Similarly, we can define the second 
recommendation value ğ‘Ÿ2 ğ‘›,ğ‘ğ‘ â€² =ğ‘›ğ‘Ÿ ğ‘›,ğ·ğ‘ â€²  ğ·ğ‘ â€²âˆˆğ‘ğ‘ â€² , which 
recommends using the  documents   instead.  The last 
recom mendation component is based on  the usersâ€™  interaction 
sequence. The interaction sequence  recommendation approach 
tries to take into consideration the interaction process of the user, 
with the scope of recommending those nodes that are following 
this sequen ce of interactions. For instance, if a user has opened a 
video of news highlights, the recommendation could contain the 
more in -depth stories that previous users found interesting to view 
next.  The recommendation value ğ‘Ÿ3 ğ‘›,ğ‘ğ‘ â€² , called inte ractive 
recommendation, can thus be defined as follows:  
ğ‘–ğ‘Ÿ ğ‘›,ğ‘ğ‘ â€² = 
    ğ‘™ğ‘Ÿâ€² ğ‘›ğ‘– âˆ™ğœ‰ğ‘™âˆ’1âˆ™ğ‘¤   âˆƒ ğ‘=ğ‘›ğ‘–â†ğ‘›ğ‘—â†’ğ‘›
ğ‘¤ âˆˆ ğ‘›ğ‘—,ğ‘›,ğ‘¤ 
ğ‘™=ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡ğ‘• ğ‘ 
ğ‘™<ğ¿ğ‘€ğ´ğ‘‹     
ğ‘›ğ‘–âˆˆğ‘ğ‘ â€²
LDM WDG
343Figure 3: Graph illustrating implicit relevance pool  
where p is the path between any node ğ‘›ğ‘–  and node ğ‘›, taking into 
consideration the link directionality. l is the length of the path 
(counted as the number of links) and the distance is lower than a 
maximum length ğ¿ğ‘€ğ´ğ‘‹. Finally, ğœ‰ is a length r eduction factor, this 
was set to 0.8 in our system for all of our evaluations . This length 
reduction factor allows us to give more importance to those 
documents that directly follow the interaction sequence, however 
if a document with high levels of intera ction occurs two or three 
steps away it will be recommended as well.  
 In a final step, we obtain the three recommendation lists from 
each recommendation component  and merge them into a single 
final recommendation list. For this we use a rank -based 
aggregation approach, the scores of the final recommendation s are 
the sum of the rank -based normalis ed score of each of the 
recommendation list, i.e. using a  score 1
ğ‘Ÿ(ğ‘›) where ğ‘Ÿ(ğ‘›) is the 
position of ğ‘› in the recommended list. The final list is then split 
into recommended queries and recommended documents; these 
are then presented to the user.  
5. EXPERIMENTAL METHODOLOGY  
5.1 Hypothesis  
In order to measure the effectiveness of our proposed approach we 
conducted a user -centred evaluation. The goal of our evaluation 
was to investigate the effect of  using  community based  implicit 
feedback to aid search in a video search paradigm. There are a 
number of potential benefits of our a pproach, which we would 
like to test:  
x The performance of the system, in terms of precision of 
retrieved videos, will improve with the use of 
recommendations based on implicit feedback.  
x The users will be able to explore the collection to a 
greater extent, and also discover aspects of the topic that 
they may not have considered, serendipitously.  
x The users will be more satisfied with the system that 
provides feedback, and also be more satisfied with the 
results of their search.  
5.2 Collection and Tasks  
With the purpose of determining  the effects of implicit feedback 
users were required to carry out a number of video search tasks 
based on the TRECVID 2006 evaluations [18]. For our evaluation 
we focused on search tasks fr om the interactive search track . In 2006 the TRECVID collection contained 79,848 shots  from 
English, Arabic, and Chinese news video. This data collection is 
noisy and hence the state of the art retrieval syst ems do not do 
achieve the same P R (Precision and Recall)  as for text.  In the 
TRECVID 2006  interactive search evaluations there were  a total 
of 24 tasks . For our evaluation we are limiting the number of tasks 
that the users carry out to 4.  Limiting the num ber of t asks allowed 
us to carry out more evaluations, as 24 individual search topics 
did not have to be carried out for each participant. For this 
evaluation we chose the four tasks for which the median precision 
in the 2006 TRECVID workshop was the worst . In essence these 
are the most difficult tasks. The four tasks were:  
x Find shots with a view of one or more tall buildings 
(more than 4 stories) and the top story visible (Task 1)  
x Find shots with one or more soldiers, police, or guards 
escorting a prisoner  (Task 2)  
x Find shots of a group including at least four people 
dressed in suits, seated, and with at least one flag (Task 3)  
x Find shots of a greeting by at least one kiss on the cheek 
(Task 4)  
The users were given the topic and  a maximum of fifteen minute s 
to find shots relevant to the  topic. The users could carry out text 
based queries. The shots that were  marked as relevant were then 
compared with the ground truth in the TRECVID collection.  
5.3 Experimental Design  
For our evaluation we adopted 2 -searcher -by-2-topic Latin Square 
designs. Each part icipant carried out two  tasks  using  the baseline 
system , and two tasks using the recommendation system . The 
order of system usage was varied as was the order of the tasks; 
this was to avoid any order effect associated  with the tasks or with 
the systems.  In order to determine the effect of adding more 
implicit actions to the implicit pool, p articipants in the experiment 
were placed in groups of four. For each group , the 
recommendation system used the implicit feedback from all of the 
previous users.  At the beginning of the evaluation there was no 
pool of implicit actions , therefore  the first group of four user s 
received no recommendations; their interactions formed the 
training set for the i nitial evaluations . Using thi s experimental 
model we can evaluate the effect of the implicit feedback within a 
group of participants, and also the effect of additional implicit 
feedback across the entire group of participants. In addition to 
this, ground truth provided in the TRECVID 2006 collection 
allowed us to carry out analyses that we may not have been able 
to do with other collections.  Each participant was given five 
minutes training on each system and each participant was allowed 
to carry out training tasks. These training tasks  were the tasks for 
which participants had performed the best at TRECVID 2006. For 
each participant their interaction with the system was logged, the 
videos they marked as relevant were stored and they also filled 
out a number of questionnaires at differen t stages of the 
experiment.  The purpose of using this experimental methodology 
was to validate our three hypotheses.   
6. RESULTS  
24 participants took part in our evaluation . The participants were 
mostly postgraduate students and research ers at a university . The 
participants consisted of 18 males and 6 females with an average 
age of 25.2 years (median: 24.5) and an advanced proficiency 
with English.  The participants indicated that they regularly 
interacted with and searched for multimedia. The y were paid a 
sum of Â£10 for their participation in the experiment , which took 
344approximately 2 hours . The results of the user trials were analysed 
with respect to our hypotheses  that were given in the previous 
section . The evidence for and against each of t hese benefits is laid 
out in the following sections.  
6.1 Task Performance  
Since we were using the TRECVID collection and tasks, we were 
able to calculate precision and recall values for all of the tasks.  
Figure 4 shows the P@N for the baseline and recommendat ion 
systems for varying values of N. P@N is the ratio between the 
number of relevant documents in the first N retrieved documents 
and N. The P@N value focuses on the quality of the top results, 
with a lower consideration on the quality of the recall of the  
system.  
 
Figure 4: P@N for the Base line and Recommendation Systems  
The results show that the system that uses recommendations 
outperforms the baseline system in terms of precision. It can be 
seen quite clearly from Figure 4 that the shots returned by the 
recommendation system have a much higher precision over the 
first 5 -30 shots than the baseline system. We verified that the 
difference between the two P@N values for values of N between 
5 and 100 was statistically significan t using a pair wise t -test (p = 
0.0214, t = 3.3045). It can also be seen over the next 100 -2000 
shots that the difference is negligible. However, it is unlikely that 
a user would view that number of shots; given that in total our 24 
participants viewed 303 4 shots, in the entire trial, 24 hours of 
video viewing. This demonstrates that the use of the implicit 
feedback can improve the retrieval results of the system, and thus 
be of greater assistance  to users.   
Figure 5 shows the mean average precision  (MAP)  for baseline 
and recommendation systems for different groups of users. Each 
group of four users also had additional feedback from previous 
participants, which the previous group of four users did not have.  
MAP is the average for the 11 fixed precision valu es of the PR 
metric, and is normally used for a simple and convenient systemâ€™s 
performance comparison.  
It can be seen quite clearly that the MAP of the shots that the 
participants selected using the recommendation system is higher 
than the MAP of the shots  that the participants selected using the 
baseline system. We verified that the difference between the two 
sets of results were statistically significant using a pair wise t -test 
(p = 0.0028, t = 6.5623). The general trend is that the MAP of the 
shots foun d using the recommendation system is increasing with 
the amount of training data that is used to propagate the graph 
based model. There is a slight dip in one group; however, this may 
be due to the small sample groups that we are using.  These results 
show that, as well as participants finding more related shots in the 
data set, that they are finding new and diverse relevant shots in the 
data set.   
Figure 5: Mean Average Precision for Baseline and 
Recommendation Systems for Different Groups of Users  
Howeve r, these findings are not quite borne out by the recall 
values for the tasks. Despite having high er precision values  for the 
recommender system in comparison with the baseline , the recall 
for the tasks is  still quite low. While recall is an important aspect 
we feel that it is more important that the users found accurate 
results and that the y perceived that th ey had explored the 
collection.  For the  measured P@N and MAP values;  it has been 
shown that the recommend ation system outperforms the baseline 
system, and that this difference is statistically significant. This 
demonstrates the validity of our first hypothesis.  In the following 
section we will discuss user exploration of the collection in more 
detail.  
6.2 User E xploration  
6.2.1 Analysis of Interaction Graph  
To begin our investigation of user exploration we analysed the 
graph of interactions. The number of nodes, the number of unique 
queries and the number of links that were present in the graph, at 
each stage where the  graph had additional information  for the 
previous four users added , were analysed. Table 2 shows the 
results of that analysis; it can be seen that the number of new 
interactions with the collection increases with the number of 
participants.  
Users  Number of 
Nodes  Number of 
Queries  Number of 
Links  Total Graph 
Elements  
4 1001 
(28.31%)  115 
(18.51%)  2505 
(23.09%)  3621 
(24.13%)  
8 1752 
(49.56%)  258 
(41.54%)  4645 
(42.81%)  6655 
(44.35%)  
12 2488 
(70.38%)  388 
(62.48%)  7013 
(64.63%)  9989 
(66.57%)  
16 3009 
(85.12%)  452 
(72.79%)  8463  
(78%)  11924 
(79.46%)  
20 3313 
(93.72%)  550 
(88.57%)  9868 
(90.95%)  13731 
(91.5%)  
24 3535 
(100%)  621  
(100%)  10850 
(100%)  15006 
(100%)  
Table 2: Number of graph elements in graph after each group 
of four users.  
The majority of nodes in our graph are video shots, as the number 
of participants increases so do es the number of unique shots that 
have been view ed. On further investigation of the graph and logs 
it was found that , overall, 49% of documents selected by users 1 -
12 were sel ected at  least by one user in users 13-24. Users 1 -12 
clicked 1050 unique  documents, whereas users 13 -24 clicked 596 
00.050.10.150.20.250.30.35
5 10 15 20 30 100 200 500 1000 2000Precision
NP@N 
Baseline
RecommendationN
00.0010.0020.0030.0040.0050.0060.0070.008
User 5 -8User 9 -12User 13 -16User 17 -20User 21 -24Baseline Recommendation
345unique  documents. Also,  users  1-12 produced 1737 clicks, 
whereas user 13-24 produced 102 4. This can be interpreted as 
users 13-24 were satisfied mo re quickly than users 1 -12. It was 
also found that the number of unique queries  also increases (see 
Section 6.2.2 ) with the additional users . These results give an 
indication that further participants are not just using the 
recommendations to mark relevant  videos, but also interacting 
with further shots.   
6.2.2 Text Queries  
In both the baseline and the recommender based systems the 
participants were also presented with query expansion terms that 
they could use to enhance their queries. We found however, that 
the majority of participants cho se not to use the query expansion 
terms  provided by the baseline system  as they found them 
confusing. The query terms returned by the baseline system were 
stemmed and normalised and hen ce were not in the written form 
users expec ted them to be. Whereas the queries recommended by 
the recommender system were queries that previous users had 
used. One participant stated that â€œThe query expansion terms 
didnâ€™t have any meaning.â€ Another participants said that the 
â€œquery expansion did no t focus on real search taskâ€. This can be 
explained in part by specificities of some of the chosen topics, for 
example when a user enters the name of a city (â€œNew Yorkâ€) to 
get a shot of the cityâ€™s sky line, the query expansion terms did not 
help to specif y the search query. In fact in the top ten queries for 
each task query expansions only occur twice, both for the same 
topic. Across the 24 users and 4 topics there is relatively little 
repetition of the exact same queries, there were 621 unique 
queries  out of 1083 total queries . In fact only 4 queries occur 10 
times or more, and they were all for the same task.  
The results in this section indicate that the users explore the 
collection to greater extent using the recommendations. Later 
users did not merely interact with videos that the previous users 
had interacted with, but instead could see what previous users had 
done and explore new video shots, Nodes were added to the graph 
of implicit actions through out the evaluation (see table 2). Also 
there was ver y little query repetition, and newer users used new 
and diverse query terms.  These results give an indication that we 
are achieving the second benefit of our approach; that users will 
be able to explore the collection to a greater extent, and also 
discove r aspects of the topic that they may not have considered. 
However, this finding has not been fully validated. In order to do 
this we analyse d the use r perceptions of the results and systems , 
this analysis is presented in the following section.  
6.3 User Perceptions  
In order to provide further validation for our second hypothesis 
and to validate our third h ypothesis, we analysed the post task and 
post experiment questionnaires that our participants filled out.  
6.3.1 Retrieved Videos  
In post search task question naires we solicited subjectsâ€™ opinions 
on the videos that were returned by the system. We wanted to 
discover if participants explored the video collection more based 
on the recommendations or if it in fact narrowed the focus in 
achievement of their tasks.  The following Likert 5 -point scales 
and semantic differentials were used . Some of these are 
contradictions and some of the scales were inverted to reduce 
bias. The scales and differentials were:  â€œI had an idea of which 
kind of videos were relevant for the topic before starting the 
searchâ€  (Initial Idea) , â€œDuring the search I have discovered more 
aspects of the t opic than initially anticipatedâ€  (Change 1) , â€œThe video(s) I chose in the end match what I had in mind before 
starting the search â€ (Change 2) , â€œMy i dea of what videos and 
terms were relevant changed throughout  the task â€ (Change 3) , â€œI 
believe I have seen all possible videos that satisfy my 
requirement â€ (Breadth) , â€œI am satisfied with my search results â€ 
(Satisfaction)  and the following Semantic differe ntials : The 
videos I have received through the searches were:  â€œrelevantâ€ / 
â€irrelevantâ€,  â€œappropriateâ€ / â€inappropriateâ€,  â€œcompleteâ€ / 
â€incompleteâ€,  â€œsurprisingâ€ / â€œexpectedâ€ . Table 3 presents the 
average responses for each of these scales and differentials, using 
the labels after each of the Likert scales in the bulleted list above. 
The values for the four semantic differentials are included at the 
bottom of the table. The most positi ve response across for each 
system  is shown in bold.   
Differential  Baseline  Recommendation  
Initial Idea  3.625  4.175  
Change 1  3.1 3.5 
Change 2  3.475  3.725  
Change 3  2.725  3.05 
Breadth  2.625  3.075  
Satisfaction  2.95 3.4 
Relevant  1.925  2.55 
Appropriate  3.125  3.775  
Complete  2.225  2.5 
Surprising  1.55 1.725  
Table 3: Perceptions of System (Higher = Better)  
From the results in Table 3 it appears that participants have a 
better perception of the video shots that they found during their 
tasks using the recommendation system. It also appears that the 
participants believe more strongly that this system changed their 
perception of the task and presented them with more options, this 
would back up the findings in Section 6.2 that the participants 
explored the collection to a greater extent when presented with the 
recommendations. W e applied two -way analysis of variance 
(ANOVA) to each differen tial across both systems and the four 
tasks to test these assertions . The initial ideas that the participants 
had about relevant shots were dependent on the task (p < 0.019 for 
significance of task). The changes in their perceptions were more 
dependent on the system that they used rather than the task, as 
was the participants belief that they had found relevant shot 
through the searches (p < 0.217 for significance of system). This 
demonstrates that the recommendation system helped the users to 
explore the c ollection to a greater extent, and also indicates that 
the users have a preference for the recommendation system, this 
finding strengthens the argument that our recommendation s are 
providing benefits in terms of exploration and user perception.  
6.3.2 Ranking of Systems  
After completing all of the tasks and having used both systems we 
attempted to discover whether the participants preferred the 
system that provided recommendations or the system that did not. 
The participants were asked to complete an exit question naire 
where they were asked which system they preferred for  particular 
aspects of the task. The participants  could also indicate if they 
found no difference between the systems. The participants were 
asked, â€œ Which of the systems did youâ€¦ â€: â€œfind best overa llâ€ 
(Best), â€œ find easier to learn to useâ€  (Learn), â€œ find easier to use â€ 
(Easier), â€œpreferâ€ (Prefer), â€œ find changed your perception of the 
taskâ€ (Perception) and â€œfind more effective for the tasks you 
performedâ€ (Effective). The users were also given some s pace to 
provide any feedback that they felt may be useful.  
346Differential  Recommendation  Baseline  Same  
Best 16 2 1 
Learn  7 2 11 
Easier  5 2 13 
Prefer  17 1 2 
Perception  11 3 6 
Effective  14 3 3 
Table 4: User preference s for each system .  
It can be seen clearly that the participants had a preference for the 
system that provided the recommendations. It is also encouraging 
that the participants found there to be no major difference in the 
effort and time required to learn how to use the recommendations 
that are provided by the system with recommendations. This 
further indicates that users were more satisfied with the system 
that provides recommendation, thus realising the third goal of our 
system  that users  will be more satisfied with the system that 
provide s feedback.  In this section it has been seen that the users 
have a definite preference for the recommendation system. The 
participants also indicated in their post task questionnaires that the 
system that provided recommendations helped them to explore the 
task and find aspects of the task that they otherwise would not 
have considered, in comparison with the baseline syst em. Thus 
validating part three of our hypothesis, and also helping validate 
part two of our hypothesis.  The results of our analysis have 
addressed all of the points of our hypotheses and have 
demonstrated that we have achieved our goals.   
6.4 Follow Up  Evaluat ion 
In order to expand on some of our results we p erformed a follow 
up evaluation . The goal of this evaluation was to validate our 
approach using related but not identical tasks . For this evaluation 
we used the same two systems that have been described ear lier in 
this paper (see Section 3), the same dataset (see Section 5.2) and 
the same experimental methodology (see Section 5.3); however 
we use four different tasks. Two of these tasks were related to 
tasks that had been carried out in the first evaluation,  and two 
further tasks that were not related. Some of these tasks were  not 
from TRECVID 2006 so we cannot perform all of the same 
evaluations that we have presented in this paper, as we do not 
have the ground truth data that we had available for the initia l 
evaluation. However, we can get an indication of user task 
performance and perceptions, when users are not repeating the 
same tasks. The pool of implicit actions from the previous 
experiment was used to provide recommendations for this 
evaluation. Three independent human judges judged the shots that 
were marked as relevant, so that we could perform some analysis.  
Four users carried out the new evaluation, as this evaluation was 
to validate findings to date and not to re -test the hypotheses. After 
the expe riment was completed it was found that for the two 
related tasks the users retrieved more video shots using the 
recommendation system in comparison with the baseline system. 
For the unrelated task the participants retrieved slightly less 
videos with the re commendation system, however the difference 
was not significant. In terms of precision, for one of the related 
tasks the precision of the results is increased three fold using the 
recommendation system, for the second related task the precision 
is slightly  lower, in this case the difference was not significant. In 
terms of the unrelated tasks the precision was greater for one of 
the tasks with the recommendation system, and lower for the 
other, again this difference was not significant. The participants 
indicated in their post task questionnaires that the system that provided recommendations helped them to explore the task and 
find aspects of the task that they otherwise would not have 
considered. All of the participants had a preference for the 
recommendati on system. Some of the variations in these results 
may be due to using such a small sample of users, but overall the 
trends support the conclusion s found in the first evaluation . It 
appears that overall the use of recommendations does not hinder 
performanc e on unrelated tasks, while still helping users with 
related but not identical tasks.  
In another body of related work we simulated over 7200 search 
sessions  using our graph based approach for recommendations 
and compared the results with the search trails approach from 
White et al. [23] and the random walk from Craswe ll and 
Szummer [4]. The full details of the experimental setup are 
available in Vallet et al. [24]. Figure 6 shows the MAP for each 
approach with respect to the number of elements that have been 
added to the graph.  
 
Figure 6: MAP for different recommendation approaches 
with respect to graph size  
It can be seen quite clearly from Figure 6 that our approach 
(represented by relevance pool) outperforms the other  approaches 
in terms of MAP. Figure 6  also illustrate s the scalability of our 
approach;  the relevance pool  consistently gained performance as 
more users were added to the implicit graph (up to a test corpus of 
1.25M total graph elements ). For this evaluation we did not 
provide a direct comparison with other graph based work from 
Yang et al. [25] that was cited earlier in this paper. As was 
pointed out earlier ( see Section 2.2 ) this recommendation 
approach is content -based, whereas ours is based on sole click  
through dat a much like the work of Crasswell and Szummer [4] 
and White et al. [23], so a direct comparison would not be 
appropriate. The following section will provide some final 
conclusions and a discussion of our findings.  
7. DISCUSSION AND CONCLUSIONS  
We have presented a no vel video retrieval system,  which uses  
feedback from previous users to inform and aid users of a video 
search system. The recommendations provided are based on user 
actions and on the previous interaction pool. There are a number 
of conclusions that can be  made about using community based 
implicit feedback to provide recommendations.  For the results of 
task performance (see Section 6.1), we measured P@N and MAP 
values, it has been shown that the recommendation system 
outperforms the baseline system, and tha t this difference is 0.0060.0070.0080.0090.010.0110.0120.013
0.19 0.28 0.38 0.47 0.56 0.66 0.75 0.84 0.94 1.03 1.13MAP
Graph Elements (Millions)Relevance Pool
Search trails
Random walk
No 
recommendation
347statistically significant. This demonstrates  that the performance of 
users of the recommendation system will improve with the use of 
recommendations based on implicit feedback . The statistics 
presented in Section 6.2, show that the use rs are pursuing the tasks 
sufficiently differently. They were able to explore the collection to 
a greater extent and find more relevant videos.  This indicates that 
users will be able to explore the collection to a greater extent, and 
also discover aspects  of the topic that they may not have 
considered. This second hypothesis is further validated in Section 
6.3 where the users gave an indication that the recommendation 
system helped them to explore the collection. The participants 
indicated in their post task questionnaires that the system that 
provided recommendations helped them to explore the task and 
find aspects of the task that they otherwise would not have 
considered, in comparison with the baseline system. It is also 
shown that the users have a definite preference for the 
recommendation system. These results successfully demonstrate 
the potential of using implicit feedback to aid multimedia search, 
and that this area deserves further investigation to be fully  
developed . To this end we carried out some brief follow up 
experiments to investigate some of our findings further.  It was 
shown that the recommendations are useful for related tasks, while 
not hindering unrelated tasks.  These follow up evaluations 
demons trated further uses of our approach , however  there is future 
work that can be carried out.  In particular, these techniques could 
be extended with other types of querying, e.g. query by example, to 
provide even more improved query results for users.  In conc lusion, 
the results of the evaluation , for our system that uses a co llection of 
user actions has highlighted the promise  of this approach to 
alleviate the major problems that users have while searching for 
multime dia, thus presenting a potentia l work aroun d to  the 
semantic gap [11] and other problems associated with video 
search . 
8. ACKNOWLEDGEMENTS  
This research work was partially supported by the EC under 
contracts: K -Space (FP6 -027026) and SEMEDIA (FP6 -045032).  
9. REFERENCES  
[1] Adcock, J., Pickens, J., Cooper, M., Anthony, L., Chen, F. 
and Qvarfordt, P.  FXPAL Interactive Search Experiments for 
TRECVID 2007. In Proc. TRECVID 2007.  
[2] Christel, M.G, and Conesc u, R.M. Mining Novice User 
Activity in TRECVID Interactive Retrieval Tasks. In Proc 
CIVR 2006, 21 -30. 
[3] Christel, M.G. Establishing the Utility of Non -Text Search 
for News Video Retrieval with Real World Users. In Proc 
ACM MM 2007, 707 -716.  
[4] Craswell, N. and Szummer, M., Random walks on the click 
graph. In Proc. SIGIR 2007,  ACM Press (2007), 239 -246.  
[5] Freyne, J., Farzan, R., Brusilovsky, P., Smyth, B. and Coyle, 
M. Collecting Community Wisdom: Integrating Social 
Search and Social Browsing. In Proc. IUI 2007, ACM Press 
(2007), 52 -61. 
[6] Goldberg, D., Nichols, D., Oki, B.M., and Douglas, T. Using 
Collaborative Filtering to Weave an Information Tapestry. 
Communications of the ACM 35, 12 (1992), 61 -70. 
[7] Halvey, M. and Keane, M.T. Analysis of Online Vid eo 
Search and Sharing. In Proc.  ACM HT 2007, ACM Press 
(2007), 217 -226. [8] Hancock -Beaulieu, M. and Walker, S. An evaluation of 
automatic query expansion in an online library catalogue. 
Journal of Documentation  48, 4 (1992), 406 â€“421. 
[9] Hopfgartner, F.,  Urban , J.,   Villa, R. and    Jose, J.    Simulated 
Testing of an Adaptive Multimedia Information Retrieval 
System. In Proc. CBMI 2007 , IEEE (2007), 328 -335. 
[10] Hopfgartner, F. Understanding Video Retrieval. VDM 
Verlag (2007)  
[11] Jaimes, A., Christel, M., Gilles, S., Rames h, S., and Ma, W -
Y. Multimedia Information Retrieval: What is it, and why 
isnâ€™t anyone using it? In  Proc MIR, ACM Press (2005),  3â€“8. 
[12] Kelly, D., and Teevan, J. Implicit feedback for inferring user 
preference: A bibliography. SIGIR Forum 32, 2 (2003), 18 -28. 
[13] Liu, J., Lai, W., Hua, X -S., Huang, Y. and Li, S. Video 
Search Re -Ranking via Multi -Graph Propagation, In Proc 
ACM MM, 208 -217. 
[14] Naphade, M., Smith, J.R., Tesic, J., Chang, J -S., Hsu, W., 
Kennedy, L., Hauptmann, A. and Curtis, J. Large -Scale 
Ontology for M ultimedia. In IEEE MultiMedia 13(3), 2006, 
86-91. 
[15] Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P. and Riedl, 
J. (1994).GroupLens: An Open Architecture for Collaborative 
Filtering of Netnews. In Proc. CCSCW 1994, 165 -173. 
[16] Salton, G. and Buckley, C. Impr oving retrieval performance 
by relevance feedback. Readings in information retrieval 
(1997), 355 â€“364. 
[17] Shardanand, U. and Maes, P. Social Information Filtering: 
Algorithms for Automating â€œWord of Mouthâ€. In Proc. CHI 
1995, ACM Press (1995), 210 -217. 
[18] Smeaton , A. F., Over, P., and Kraaij, W. 2006. Evaluation 
campaigns and TRECVid. In Proc. MIR 2006, ACM Press 
(2006), 321 -330. 
[19] Smyth, B., Balfe, E., Freyne, J., Brigg s, P., Coyle, M. and 
Boydell, O . Exploiting Query Repetition and Regularity in an 
Adaptive Commun ity-Based Web Search Engine. U MUAI  
14, 5 (2004). 383 -423. 
[20] Snoek, C., Worring, M., Koelma, D., and Smeulders, A. 
Learned Lexicon -Driven Interactive Video Retrieval. In Proc 
CIVR 2006, 11 -20. 
[21] Spink, A, Greisdorf, H., and Bateman, J. From highly 
relevant to n ot relevant: examining different regions of 
relevance. Inf. Process. Management 34 , 5 (1998), 599 â€“621. 
[22] Wexelblat, A. and Maes, P. Footprints: History rich tools for 
information foraging. In Proc. CHI 1999, ACM Press (1999), 
270-277. 
[23] White, R., Bilenko, M. and Cucerzan, S., Studying the use of 
popular destinations to enhance web search interaction . In 
Proc. SIGIR 2007,  ACM Press (2007), 159 -166. 
[24] Vallet, D., Hopfgartner, F., and Jose, J. Use of Implicit 
Graph for Recommending Relevant Videos: A Simulated 
Evaluation. In Proc ECIR 2008.   
[25] Yang, B., Mei, T., Hua, X -S, Yang, L., Yang, S -Q and Li, M. 
Online Video Recommendation Based on Multimodal Fusion 
and Relevance Feedback. In Proc. SIGIR 2007, ACM Press 
(2007), 73 -80.   
348