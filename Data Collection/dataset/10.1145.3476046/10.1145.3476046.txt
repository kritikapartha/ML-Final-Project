305
Algorithmic Folk Theories and Identity: How TikTok Users
Co-Produce Knowledge of Identity and Engage in
Algorithmic Resistance
NADIA KARIZAT, University of Michigan, USA
DANIEL DELMONACO, University of Michigan, USA
MOTAHHARE ESLAMI, Carnegie Mellon University, USA
NAZANIN ANDALIBI, University of Michigan, USA
Algorithms in online platforms interact with users’ identities in different ways. However, little is known about
how users understand the interplay between identity and algorithmic processes on these platforms, and if
and how such understandings shape their behavior on these platforms in return. Through semi-structured
interviews with 15 US-based TikTok users, we detail users’ algorithmic folk theories of the For You Page
algorithm in relation to two inter-connected identity types: person andsocial identity. Participants identified
potential harms that can accompany algorithms’ tailoring content to their person identities. Further, they
believed the algorithm actively suppresses content related to marginalized social identities based on race
and ethnicity, body size and physical appearance, ability status, class status, LGBTQ identity, and political
and social justice group affiliation. We propose a new algorithmic folk theory of social feeds— The Identity
Strainer Theory —to describe when users believe an algorithm filters out and suppresses certain social identities.
In developing this theory, we introduce the concept of algorithmic privilege as held by users positioned
to benefit from algorithms on the basis of their identities. We further propose the concept of algorithmic
representational harm to refer to the harm users experience when they lack algorithmic privilege and are
subjected to algorithmic symbolic annihilation. Additionally, we describe how participants changed their
behaviors to shape their algorithmic identities to align with how they understood themselves, as well as to
resist the suppression of marginalized social identities and lack of algorithmic privilege via individual actions,
collective actions, and altering their performances. We theorize our findings to detail the ways the platform’s
algorithm and its users co-produce knowledge of identity on the platform. We argue the relationship between
users’ algorithmic folk theories and identity are consequential for social media platforms, as it impacts users’
experiences, behaviors, sense of belonging, and perceived ability to be seen, heard, and feel valued by others
as mediated through algorithmic systems.
CCS Concepts: •Human-centered computing →Collaborative and social computing ;Empirical studies
in HCI .
Additional Key Words and Phrases: Algorithm; folk theories; algorithmic resistance; social media; algorithmic
symbolic annihilation; algorithmic privilege; algorithmic representational harm; identity strainer theory;
algorithmic identity; identity; co-production; marginalization; marginalized identity
Authors’ addresses: Nadia Karizat, nkarizat@umich.edu, University of Michigan, Ann Arbor, Michigan, USA; Daniel
Delmonaco, delmonac@umich.edu, University of Michigan, Ann Arbor, Michigan, USA; Motahhare Eslami, meslami@
andrew.cmu.edu, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; Nazanin Andalibi, andalibi@umich.edu,
University of Michigan, Ann Arbor, USA.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
©2021 Association for Computing Machinery.
2573-0142/2021/10-ART305 $15.00
https://doi.org/10.1145/3476046
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:2 Nadia Karizat et al.
ACM Reference Format:
Nadia Karizat, Daniel Delmonaco, Motahhare Eslami, and Nazanin Andalibi. 2021. Algorithmic Folk Theories
and Identity: How TikTok Users Co-Produce Knowledge of Identity and Engage in Algorithmic Resistance. Proc.
ACM Hum.-Comput. Interact. 5, CSCW2, Article 305 (October 2021), 44 pages. https://doi.org/10.1145/3476046
1 INTRODUCTION
Algorithms employed in online platforms (e.g., social media) are often opaque to the individuals who
use or are impacted by platform processes. The development of algorithmic folk theories, unofficial
theories a user holds to explain how a technological system operates and generates various outputs
[27], is a powerful way for users of online platforms to make sense of what they see and experience
on these platforms [ 26,30,32]. A construct relevant to algorithmic folk theories is identity as
algorithms and users’ identities interplay in online spaces in several ways. For example, content
creators present and express their identities online, and viewers interact with online content
related to their identities, interests, and curiosities; all of which are facilitated by algorithmic
processes. However, algorithmic processes might impact specific identities unjustly. For example,
in 2019, a group of YouTube content creators filed a lawsuit claiming YouTube’s recommendation
algorithm demonetizes and hides content created by members of the LGBTQ community [ 7]. These
users worked for months to develop and test theories about how this algorithm interacts with
users’ identities on the platform, and whether it discriminates against the LGBTQ content creators
[68]. TikTok garnered similar anecdotal critiques around the stifling of content related to certain
identities including race, gender, and sexuality [ 12,13,28,67]. These examples demonstrate the
complex interaction between algorithms and users’ identities. Yet, the interplay between identity,
algorithmic folk theories, and subsequent user behavior on social media platforms remains unclear.
In this paper, we explore the relationship between algorithmic folk theories and identity, and
theories’ effects on user behavior in the context of the popular video-sharing platform TikTok.
TikTok describes itself as “the leading destination for short-form mobile video” [ 1]. Users open the
TikTok app and land on the For You Page (FYP); the platform’s algorithmically-generated main
feed with videos to view. Users can comment, like, and share videos from this page to other social
media platforms or via messaging applications such as Facebook Messenger. In this research, we
investigate how TikTok users interpret and experience the algorithm and develop algorithmic folk
theories in relation to identity. We ask the following research questions:
•RQ1: How do TikTok users believe the TikTok algorithm operates in relation to identity?
•RQ2: How do user perceptions of TikTok algorithm’s interplay with identity in turn shape
their behaviors on the platform?
We conducted semi-structured interviews with 15 US-based adult TikTok users (content creators,
content viewers) about their experiences with and perceptions of the platform’s algorithm (note:
we did not ask about the "algorithm" directly, as detailed in our methods section). We found that
participants held complex algorithmic folk theories that perceived identity as integral to how the
algorithm chose to recommend videos on the platform. Participants made sense of the algorithm
with respect to two inter-connected identity types: person andsocial . Stets and Burke define person
identity as the characteristics an individual understands as making them distinct from another
person, such as their interests in pop culture or cooking, and social identity as the meanings
attributed to belonging to a certain social group, such as a certain race, gender or class [ 19].
Participants spoke to beliefs that the algorithm negotiates and balances between how it perceives
their person and social identities. Participants felt the algorithm understood their interests and
discerned their person identity based on their personal engagement, networks on and off TikTok,
and what content was popular on the platform at a given moment. Participants’ remarks also
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:3
highlighted a belief that the algorithm suppressed content related to marginalized social identities
based on race and ethnicity, body size and physical appearance, ability status, class status, LGBTQ
identity, and political and social justice group affiliation.
We further show how participants’ algorithmic folk theories and their beliefs in relation to
person and social identities influenced their behavior to try and shape the algorithm and how the
algorithm understands their person identities, as well as how the algorithm affects different social
identities on the platform. On person identities, participants intentionally engaged with videos on
the platform in ways that they expected would train the algorithm to display or not display content
on their main feed, the For You Page (FYP), to achieve alignment between their algorithmic identity
[22]—how the algorithm is believed to understand them—and how they understand themselves.
Participants also identified risks of how the algorithm’s tailoring to one’s person identity could
create FYPs that cause and reinforce harm to users the algorithm identified as being interested in
harms such as unhealthy behavior and racism. On social identities, we identified three primary
ways participants resisted the perceived suppression of certain social identities on the platform:
individual actions, collective actions, and content creators altering the ways they performed in
their video content.
We theorize our findings by applying a co-productionist [ 44] lens to demonstrate the ways
users and the algorithm interact and co-produce knowledge of person and social identity [ 19]
on the platform. We expand on previous scholarship on folk theories of social feeds [ 30] and
propose The Identity Strainer theory to capture users’ beliefs that an algorithm filters content based
on social identity, resulting in the suppression of marginalized social identities on a platform’s
social feed. Through this theorizing, we introduce the concept of algorithmic privilege as privilege
held by users who are positioned to benefit from how an algorithm operates on the basis of
identity. We then interpret participants’ behaviors prompted by their folk theories as forms of
algorithmic resistance [ 77] (i.e., intentional behaviors to produce algorithmic outcomes different
from what would otherwise be produced) and efforts to achieve representational belonging [ 21]
(i.e., the positive emotional response to seeing members of one’s community and its intricacies
represented) and thus to combat algorithmic symbolic annihilation [ 9] (i.e., algorithms furthering
normative and reductive understandings of phenomena and identities, rendering some invisible
and marginalized), ultimately countering what we introduce as algorithmic representational harm .
Algorithmic representational harm describes the kind of harm algorithmic systems’ users face
because of lack of algorithmic privilege and being targeted by algorithmic symbolic annihilation. We
argue that the relationship between users’ algorithmic folk theories and identity are consequential
for social media platforms, as it impacts users’ experiences including their behaviors on the platform,
sense of belonging, and perceived ability to be seen, heard, and feel valued by others as mediated
through algorithmic systems.
2 LITERATURE REVIEW
2.1 Algorithms, Identity, and Bias
Algorithms curate the social media feeds of platforms like TikTok and thus contribute to online
identity construction. The proprietary secretive nature of these algorithms and the technical
knowledge required to understand them presents challenges when investigating social media
sites’ impacts on individuals, particularly those with marginalized identities [ 82]. Some social
media users are unaware that these algorithms shape the content seen on their various feeds.
For example, Eslami et al. found in a study with Facebook users that many participants did not
know a Facebook News Feed algorithm existed [ 32]. Whether or not users experience awareness of
platform algorithms, the algorithms can uphold existing systems of power and oppression and shape
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:4 Nadia Karizat et al.
users’ experiences, including construction and understanding of their own identities. Social media
platforms are spaces for identity work for many marginalized people including LGBTQ individuals
[20,39], Black individuals [ 15], and disabled people [ 48]. For instance, Brock presents Black Twitter
and other Black online spaces as places for identity construction and increased understanding
about the heterogeneous nature of Black identity [ 15]. As users, especially those with marginalized
identities, use social media platforms as part of their identity work, these platforms’ algorithms
directly influence this process.
In this paper, we focus on how TikTok content creators and viewers conceive of the interplay
between the TikTok algorithm and identity. Identity theory has roots in structural symbolic
interaction. There are three branches of identity theory as synthesized by Burke and Stets in
[19]: interactional emphasis [ 52], structural emphasis [ 74], and perceptual emphasis [ 18]. In Identity
Theory [19], Burke and Stets provide an excellent overview on identity theory scholarship and
history. Notably, they critique how much identity theorists have focused on role identity and ignored
other identity aspects: social and person [ 19, 129]. They reference prior identity theorists (e.g., [ 41,
115], [ 6]) who have conceptualized these identity aspects. They proceed to provide definitions of
person and social identity drawing from their own and other identity theorists which we found
helpful to help interpret our data. We draw from Stets and Burke’s [ 19] definitions of social and
person identity: social identities are perceived by individuals discerning a ’fit’ and membership with
specific social groups and person identities are based on internalized characteristics individuals
attribute to themselves, such as their interests [19].
In the context of identity being perceived by algorithms, we turn to Cheney-Lippold’s concept
of algorithmic identity—"an identity formation that works through mathematical algorithms to
infer categories of identity on otherwise anonymous beings" [ 22, 165]—to refer to how algorithms
create a user’s algorithmic identity according to "algorithmically-inferred attributes" [ 31] about
a user. We note that the social computing literature has long investigated the interplay between
identities and technologies broadly, yet a complete review of that work is outside the scope here;
as such we keep our review focused on those directly relevant or inspirational to our study.
In their application of critical race theory to the field of human-computer interaction, Ogbonnaya-
Ogburu et al. discuss the racism embedded within digital platforms and larger sociotechnical systems
and call for critique and examination of racial bias of technology beyond just biased algorithms [ 58].
Algorithms can recreate and reinforce existing biases related to user identity as the driving forces
behind online platforms. For example, search engines can perpetuate racism, sexism, homophobia,
and other biases through auto-filling search results and privileging bigoted results as those returned
first to users [ 10,57,60]. Noble exposes the discriminatory practices of search engines produced by
the recreation of existing human biases in algorithmic code, the monopolization of a few platforms
in this search engine space, and the for-profit ad-driven business models of search engines [ 57]. As
filters for much of the information users receive online, algorithms serve as gatekeepers and are
often imbued with biases of those who create and operate them rather than being neutral code [ 14].
Benjamin refers to these algorithms as part of what she calls “the New Jim Code”: “the employment
of new technologies that reflect and reproduce existing inequities but that are promoted and
perceived as more objective or progressive than the discriminatory systems of a previous era” [ 11,
5-6]. Social biases, Benjamin argues, are embedded in algorithms, but their technical nature gives
them the appeal of impartiality without public liability. The very presence of algorithms warrant
ongoing investigations into their impacts and consequences, as well as how they are experienced by
humans. Noble and Benjamin’s works [ 11,57] point to a tide of critically engaging with algorithms
as pieces operating in society that can further racial inequity as opposed to inherently unbiased
technology, a tide seeping into the algorithmic folk theories of technology users as we see.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:5
Algorithms’ biases can be expansive and diffuse in their impacts because of the ubiquity of algo-
rithmic systems. Some research proposes resistance to these biases by the same users experiencing
suppression and discrimination. Ettlinger discusses productive algorithmic resistance, in which
users of algorithmic systems resist subjection by utilizing the same affordances the systems use for
governance of their users [ 33]. Velkova and Kaun urge users to influence algorithmic processes of
systems to work in their own favor [ 77]. Wang presents one such example of algorithmic resistance
with the Chinese dating app Blued where users reported using the app’s algorithmic sorting of
photos to shape their desired dating outcomes [ 80]. On Reddit, Dosono and Semaan explored the
ways moderators of Asian Americans and Pacific Islander (AAPI) subreddits engage in collective
behaviors to resist what they call “algorithmic hegemony,” the continuation of whiteness being
normative and privileged by algorithmic systems, and perform identity work, such as through
"recording collective memory to circumvent systemic erasure" [ 29] by creating archives on external
platforms.
These acts of algorithmic resistance connect to a history of identity playing a particularly
important role in online social movements. For example, Liu et al. investigated identity and social
media in the context of the identity hashtag movement #ILookLikeAnEngineer [ 49]. Participants
found community building and empowerment in the various identities presented by those posting
photos as part of this movement [ 49]. Other identity-based social media movements include
#SayHerName to create dialogue and bring attention to state-sanctioned violence against cisgender
and trans-gender Black women [ 16], #MeToo to reduce stigma around disclosures of gender-based
sexual violence [ 34], #DisabilityMarch [ 48], and #BlackLivesMatter in response to police brutality
and its formation of a collective identity [ 65]. These prior works speak to the legacy of identity as
part of resistance against bias on social media platforms.
This previous research on algorithms and bias emphasizes the impacts of algorithmic systems on
reinforcing and reproducing many harmful biases such as racism and sexism. What happens when
users understand algorithmic systems as biased and suppressing certain identities but do not know
how to make sense of this situation or contend with it? We utilize folk theories as a productive
framework for how users come to theorize and make sense of their experiences on an algorithmic
platform in relation to identity. Folk theories become especially relevant when online platforms’
social feed algorithms perpetuate biases and suppress user content pertaining to marginalized
identities.
2.2 Algorithmic Folk Theories
Technological systems’ users develop folk theories to understand their own experiences with a
system [ 35,46]. The concept of folk theories is used in a variety of digital contexts for understanding
user behaviors and perceptions of their own experiences with online platforms. For example,
Toff and Neilsen apply folk theories to understanding the phenomenon of distributed discovery
in everyday news consumption across individuals’ digital environments [ 75]. Users attempt to
understand social media algorithms through the development of algorithmic folk theories [ 26,30],
defined as "intuitive, informal theories that individuals develop to explain the outcomes, effects, or
consequences of technological systems, which guide reactions to and behavior towards said systems"
[27]. Social media users benefit from the knowledge of algorithms and the ways these algorithms
curate social media experiences [ 32]. Even with the knowledge that an algorithm exists behind
various social media feeds, the specifics of many platform algorithms, such as those shaping the
Facebook News Feed or the TikTok FYP, remain unclear to the user. As a result, users develop their
own theories of how these algorithms work to make sense of their experiences.
Folk theories used to research users’ algorithmic experiences specifically on social media plat-
forms show us how people think about algorithmic systems when they impact the uniquely social
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:6 Nadia Karizat et al.
aspects of these platforms. Eslami et al. point to the ease with which users were able to develop
folk theories of algorithmic systems and their impacts once made aware of the algorithmic system
operating their curated Facebook News Feeds [ 30]. Social media users draw from a variety of infor-
mation sources in developing complex and adaptable algorithmic folks theories [ 26]. In relation to
self-presentation, knowledge about social media platforms and their operations is necessary for
this social process to occur [ 26]. While [ 26] teaches us how users form folk theories of social media
feeds, our goal is not to focus on the sources of these theories’ formation, but on their interplay with
identity and how these theories affect users’ actions toward algorithms and identity on social media
platforms at large. Devito et al.’s work on self-presentation explores how folk theories help shape
self-presentation decisions on social media by guiding user behaviors to achieve self-presentation
goals such as appearing authentic or polished [ 26]. As a result, Devito et al. begin to show how
folk theories can impact identity expression, such as a person choosing to present in a way they
feel authentically reflects how they conceptualize themselves on social media [26].
“Algorithmic awareness” [ 30] is a vital component in the development of algorithmic folk theories
by users, but further knowledge of algorithms and their effects on users’ social media experiences
may also result in “algorithm disillusionment” if users’ algorithmic expectations do not match the
actual processes occurring [ 31]. Algorithmic folk theories may also service algorithmic resistance
[33,77]. For example, algorithmic folk theories can influence users’ actions through online hashtag
campaigns to expose and resist possible platform changes [ 27]. We investigate social media users’
identities’ roles in the development of algorithmic folk theories, and how those theories then shape
user behavior on social media. We explore this space by focusing on the social media platform,
TikTok.
2.3 Video-Sharing Platforms and TikTok
Previous research on video-sharing platforms includes analyses of posted content, motivations
for video sharing, and user experiences on video platforms [ 36,53,54,83,84]. Earlier work on
understanding users’ behaviors and experiences on video platforms informs our research. The
former video platform Vine is one of the closest comparisons to TikTok as both facilitate “mobile
instant video clip sharing” [ 84], filled with users recreating dance trends to sharing personal
experiences. Snapchat Stories and Instagram Reels (launched after Tiktok, recently in 2020) [ 43] are
other similar video clip sharing platforms. TikTok emerges as a specifically mobile and short-form
video platform which may allow for more accessible video creation. Furthermore, TikTok is unique
in its hyper-visual format and the flexibility to connect to known or unknown ties as well as how
to present oneself (e.g., pseudonym, legal name) compared to other social media platforms that
prior algorithmic folk theory has focused on.
Additionally, with increased media attention and high popularity, TikTok also faces new attention
from academia. Zulli and Zulli’s analysis of TikTok explored how the platform encourages the
imitation and replication of video content, creating what they call an ’imitation public’, networks
that "form through processes of imitation and replication, not interpersonal connections, expressions
of sentiment, or lived experiences" [ 87]. Mackenzie and Nichols present the potential “subversive”
power of TikTok and the ability to create innovative content in an accessible short form video [ 50].
Recent work focuses on the potential motivations for using TikTok [ 59,81]. Omar and Dequan
argue that TikTok user motivations of archiving, self-expression, social connection, and escapism
influence user behavior on the platform [ 59]. As a relatively new and popular platform, TikTok
also may present a unique place for information interventions. For example, Zhu et al. studied
the use of TikTok as an outreach tool by Provincial Health Committees in China and argue that
the platform would be a productive space for these health organizations to reach more citizens
on what feels like a personal level [ 85]. A more recent study begins to explore users’ perceptions
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:7
of the FYP TikTok algorithm and suggests that LGBTQ users both have their identities affirmed
and violated on TikTok [ 72]. The authors focus on how those who hold the LGBTQ marginalized
identity (and sometimes other marginalized identities) respond to their identity being invalidated
by content appearing on their For You Page through various forms of “resilience” [ 72]. Our work
extends this past work and includes but goes beyond one salient identity facet as a starting point
(i.e., LGBTQ), and takes an algorithmic folk theory lens to investigate users’ perception of and
actions around algorithms and identity as a whole (e.g., person and social identity) as well as users
with marginalized and non-marginalized identities (by race, gender, class, sexual orientation, etc.).
The media attention regarding suppression of marginalized identities by TikTok, some of which
was admitted to by the company [ 13], potentially highlights a disconnect between user experiences
and the algorithms guiding the FYP feed and other aspects of the video platform. There are anecdotal
reports of suppression of identities, such as the case in which TikTok moderators were told to delete
content featuring anyone with an undesirable body type or appearance [ 12] and the algorithmic
suppression of videos featuring disabled users and LGBTQ users [ 13,67]. National conversation
in the United States in the summer of 2020 turned to the possibly questionable data practices of
TikTok [ 28,63,70] and efforts by the Trump administration to ban the application in the United
States [ 4,66]. In response to some of these pressures, TikTok opened a “Transparency Center”
and alleges to be making the algorithms driving the platform more transparent to users [ 5,63].
According to TikTok’s own Community Guidelines, the content of one’s FYP feed is determined by:
1. User interactions; 2. Video information; and 3. Device and account settings [2].
What we miss in this previous research is social media users’ algorithmic folk theories in relation
to identity, and how these perceptions in turn shape users’ behavior, which we explore in this
study. In speaking to TikTok users, some of whom regularly produce video content, we seek to
understand the folk theories they develop as platform users to make sense of their identities and
experiences with TikTok and its FYP algorithm.
3 METHODS, DATA, AND ANALYSIS
3.1 Recruitment
We conducted semi-structured interviews (N=15) with adult TikTok users in the U.S. using a
research recruiting service. Participants from the service’s potential pool of applicants completed
a screening survey to be considered for our study. Our screening survey received 284 responses.
Of these, 257 met the minimum eligibility criteria for our study. We contacted 27 respondents,
and conducted interviews with those who followed up to set an interview time and completed
the informed consent process. Participants received $20 for their time, and our university’s IRB
approved our study. We stopped recruiting participants when we achieved saturation, the point
at which we began to hear similar narratives from participants across data sources and no new
codes were being developed during analysis [ 73]. Additionally, once we were done with coding the
data, we ensured that themes are consistent across data sources in relation to our RQs. Drawing
on Hennink and Kaiser [ 40], our frequent conversations, the first author’s memos, and our formal
analysis helped us determine that we had collected sufficient data to develop a valid understanding
of the phenomenon we focus on in this paper.
3.2 Screening Survey
The screening survey asked respondents if they have used the TikTok app for at least 6 months, if
they used TikTok at least once a day, if they were located in the U.S., and their age. If any of the
respondents responded no to either of our questions, or were younger than age 18, they did not
meet our study’s minimum eligibility criteria and the survey ended. We asked respondents which
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:8 Nadia Karizat et al.
social media they used, as well as more questions regarding their usage of TikTok: how long they’ve
used TikTok, and their engagement with TikTok (i.e. producing and watching TikTok content, or
mostly only watching TikTok content). Respondents briefly described the kinds of videos they
watched, encountered, and/or produced on TikTok, and how they primarily watch videos on the
platform (such as on the main feed—the FYP—,the platform’s Discover page, etc.). Lastly, the survey
included questions about respondents’ demographics.
3.3 Interview Participants and Protocol
We invited participants to participate based on their responses to the screening survey. We accounted
for our already collected data and themes that we had begun to identify when inviting and selecting
new participants. Considering our RQs, we purposefully and carefully prioritized participants
whose responses to identities and experiences reported in the screening survey signaled to us that
we would be able to collect in-depth data in relation to TikTok use and identity. Of course that is
not to say that those we did not interview would not have had valuable experiences to share with
us. For example, if someone did not respond to screening survey questions with some level of detail
and only said they use TikTok "because they like it", they were not prioritized. While this choice
may have led to not having collected some potentially interesting data, our analysis demonstrates
that the collected data was deep and allowed us to surface similar narratives across data sources
with respect to our RQs.
More specifically, we followed a process to identify interview participants: The first, second, and
last author independently went through survey responses and scored participants’ responses on
a scale from 0 to 3; 3 being a Top Priority. These scores were summed to find their total priority
score. For example, a priority score of 9 meant all three authors ranked a respondent as a top
priority. Priority scores were higher for those who held marginalized identities, as well as those
who shared and consumed videos from a wide range of topics on the platform, rather than mundane
content such as cat videos. These criteria were part of a holistic evaluation and goal to acquire
a diverse range of participants based on the breadth of experiences they could share with us.
While accounting for the respondent’s priority score, we were also mindful of overall respondent
demographics and made efforts to pull a diverse and wide range of perspectives along the axis of
race, gender and age. Table 1 provides some of our participants’ information.
Participant Age Gender Race Sexual Orientation Total Household Income Education Use of TikTok
P1 21 Woman Black Bisexual Less than $25,000 Some College PW
P2 23 Woman Black Straight Less than $25,000 Some College PW
P3 36 Woman White Bisexual $25,000 to $34,999 Some College PW
P4 44 Woman Black Heterosexual $35,000 to $49,999 Undergraduate OW
P5 42 Man White Straight $100,000 to $149,999 Postgraduate Degree PW
P6 26 Man Hispanic/Latino Gay/Queer $50,000 to $74,999 Some College PW
P7 19 Woman White Straight $100,000 to $149,999 Some College OW
P8 50 Woman White Cis straight but asexual $75,000 to $99,999 Postgraduate Degree OW
P9 45 Woman Black Heterosexual $50,000 to $74,999 Postgraduate Degree PW
P10 18 Man Asian Heterosexual $75,000 to $99,999 Finished High School PW
P11 18 Woman Indian Straight $75,000 to $99,999 Some College OW
P12 20 Woman Black Straight $25,000 to $34,999 Some College PW
P13 28 Man Black Gay $50,000 to $74,999 Undergraduate Degree OW
P14 18 Woman Multi-Racial Asian Asexual $150,000 to $199,999 Didn’t Finish High School OW
P15 21 Woman Asian Straight $100,000 to $149,999 Some College PW
Table 1. Self-Reported Participant Demographics. Abbreviations for Use of TikTok: Produces and Watches
Videos: PW, Only Watches Videos: OW
Our interview protocol guided participants through a series of questions related to their con-
ceptualization of TikTok, their consumption and production of content, how they navigated the
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:9
app (including the ways they primarily sought out content), as well as questions touching on
their perceptions of the algorithm and how videos are recommended to them and others on the
platform. It is important to note that we did not use the word algorithm, AI or any words referencing
technological systems in our questioning, unless mentioned specifically by the participant where
we’d then use the participant’s word choice in any follow-up questions to their remarks.
We conducted interviews using Zoom’s video and audio calling services. The interviewer recorded
the interviews, and took notes. Interviews lasted from 48 to 107 minutes (average=75 min).
3.4 Data Analysis
Interviews were primarily conducted by the first author. At weekly meetings, sometimes more
frequently, the first, second, and last authors discussed themes emerging in interviews and the
first author’s notes/memos to inform future data collection as appropriate (e.g., to ask more about
interesting themes coming up that we wanted to explore more in relation to our study goals). All
interviews were audio recorded, transcribed, and coded using Dedoose qualitative coding software.
Initially, the first, second, and last authors independently coded one interview where they conducted
open coding following [ 73]. After meeting to discuss these initial codes in-depth and in detail, the
first author coded the remaining interviews one by one, while engaging in weekly meetings with
the last author to refine, discuss, and solidify codes and connections between them throughout
the process. The first author’s development of new codes had stopped before finishing all coding,
helping us ensure that we had reached saturation. After all interviews were open coded, the first,
second, and last author collaboratively used axial coding [ 73] to determine themes and to identify
relationships between them. We did not set out to code interviews for person and social identity,
specifically. Rather, our data and analysis highlighted for us what identity aspects and types were
important to participants in relation to their interactions with the FYP. Drawing from the identity
theory literature, we determined that Burke and Stet’s conceptualizations of person and social
identity [ 19] provided an excellent interpretive frame for our analysis, allowing us to situate our
qualitative codes (e.g., race, gender) within person and social identity types.
3.5 Limitations and Reflections
Our results are based on interviews with those who used TikTok regularly. We defined “regularly”
as using TikTok at least once per day over the last six months or longer. This was important
to our study because we wanted to ensure participants have had enough experience with the
platform to ground the interviews in. All participants either only watched videos, or produced and
watched videos on the app. This leaves out the experiences of TikTok users who are less frequent
in their engagement with the app and who might have started using the app more recently than
our six-month requirement. We used a recruiting service for reaching participants, which makes
our sample limited to individuals who are interested in participating in research studies, and that is
a limitation but one that does not interfere with our ability to address our research questions. We
recognize this is a privilege our team had to be able to use this service. However, this approach also
allowed us to reach a diverse sample, go beyond our individual and extended networks, and not
rely on opaque recommendation algorithms for our study recruitment had we chosen to share on
our individual TikTok profiles. We purposefully selected interview participants from the large pool
of survey respondents and determined that this limitation does not interfere with the contributions
we make in this work.
The relationship between TikTok and identity was prevalent in all interviews. Our analysis,
however, can only speak to those identities represented in our sample and willingly shared by
participants, as well as identities perceived by our sample. For example, information about body type
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:10 Nadia Karizat et al.
and disability was not collected from participants but some disclosed these facets of their identities
during their interviews and how these identities shape their experiences with the platform.
The timeframe of our data collection also impacted the responses of participants, particularly
related to identity. Media coverage of TikTok and national conversation in the United States
regarding the video platform is in flux and our interviews capture sentiments of frequent users at
a specific moment in time (June and July 2020). This moment included national protests against
police brutality in support of Black Lives Matter and the COVID-19 global pandemic. Many of
participants’ examples regarding identity and content on the platform reflect these current events.
We cannot conjecture what other examples may have been shared if the study was conducted at a
different historical moment.
All participants also had some knowledge or level of awareness of an algorithm on TikTok, even
though we did not use the term "algorithm" or ask about it specifically. Our sample lacks users who
were unaware of the algorithm’s presence. There is a possibility that a larger sample may have
resulted in some participants without awareness of an algorithm operating on TikTok.
3.6 Research Positionality Statement
Some identities represented in our research team included women of color, LGBTQ, immigrant,
and Arab-American. Our team includes experts in folk theories, social media, identity, marginality,
and algorithmic bias. All authors were familiar with TikTok.
4 RESULTS
We identified two major themes through our analysis. When discussing how they believed TikTok’s
algorithm works, participants primarily spoke of and explained their FYP on the platform, and thus
our focus here.
In response to RQ1, we identify the algorithmic folk theories participants developed in relation
to identity, specifically in relation to what Burke and Stets calls person and social identities [ 19,
112]. We explain how participants’ felt TikTok’s algorithm negotiates and balances these identity
types when curating their main feed, the FYP. We identify how participants thought the algorithm
understands their person identity, as well as the dangers of tailoring content to one’s person identity
when a user’s interests are perceived to be harmful. We also show how participants thought the
platform’s algorithm inequitably values different social identities.
In response to RQ2, we discuss how these folks theories led participants to attempt to resist the
algorithm to shape their "algorithmic identity" [ 22], as well as to ultimately change how TikTok’s
algorithm values different social identities.
5 ALGORITHMIC FOLK THEORIES AND IDENTITY ON TIKTOK
We address RQ1 in this section. Our first research question was concerned with how TikTok
users think the platform’s algorithm operates in relation to identity. We begin by describing users’
algorithmic folk theories and how they theorized the algorithm to impact and be impacted by
their identities, as well as risks and harms they believed algorithms seeing identities could lead
to. This establishes a foundation for understanding the rationale behind behaviors participants
thought would allow them to shape and/or have their experience shaped by the algorithm (which
we describe in response to our second research question).
We found that person and social identities [ 19] co-exist in participants’ folk theories, although
one might seem more salient at times. While these identity concepts provide us with vocabulary
to describe what identity aspect was more salient in a folk theory, we emphasize these aspects as
fluid, rather than static and disjoint. As a reminder, person identity refers to the characteristics
an individual understands as making them distinct from others, such as their interests, and social
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:11
identity refers to characteristics and meanings attributed to a social group that an individual
may feel they themselves or others belong to, such as a certain race, gender, social justice group
affiliation, etc [ 19]. For example, a user may be interested in anti-racism efforts, and also affiliate
themselves with Black Lives Matter and the protests occurring in the US in 2020. We interpret an
interest in anti-racism as part of one’s person identity, and affiliation with Black Lives Matter as
part of one’s social identity, even though both are informed by anti-racism.
5.1 Algorithm Seeing Person Identity
Participants shared many ideas about how they thought TikTok’s algorithm came to perceive and
understand their person identity. First, we describe how through a user’s personal engagement,
networks on and off TikTok, as well as popular content across the platform, participants felt the
algorithm defined their person identity and tailored their FYP to reflect their interests. We end with
reporting on risks participants noted in relation to algorithms’ interplay with person identities and
possibilities for causing and reinforcing harm.
5.1.1 Personal Engagement. Participants’ statements pointed to a belief that the algorithm was
aware of their interests and thus attuned to their person identity, identifying their interests and
choosing to recommend specific videos to them based on how they use the app. They held Eslami et
al.’s Personal Engagement Theory of Social Feeds [ 30] observed in the Facebook context, believing
actions such as liking or commenting on a video informed the algorithm and influenced what
they saw on their FYP. As P3 said: “My For You page is mental health, suicide, positive stuff, moms,
younger kids, because, you know, I’ve commented on stuff like that. TikTok really creates your For You
page based on what you’ve commented and liked and engaged with. ” This example points to a belief
that the algorithm identified their interests and noticed their engagement patterns to curate their
FYP. Participants’ accounts suggest that the algorithm activates a user’s person identity through
curating content that relates to a user’s individual self-concept (i.e. how a person understands
and sees themselves [ 61]). As explained by P11: “...depending on what I liked and commented on...it
started to get really similar to my life and was really relatable. ” P11’s FYP being ‘relatable’ to them
activates their person identity through its salience to how P11 conceptualizes their life. When a
user first begins to use TikTok, their FYP is not curated to their interests and is believed to slowly
change as a user engages with content and informs the algorithm that then categorizes the user as
holding certain traits and qualities.
5.1.2 On and Off TikTok Networks. Participants’ remarks highlighted a belief that the algorithm
understands their person identity and interests based on who they choose to follow and who follows
them. While believing their engagement shapes the algorithm, participants felt the engagement
of those they choose to follow is also important in how their feed is curated and tailored to their
personal interests. When discussing how videos end up on their FYP, P6 expressed they felt it is a mix
of“who I follow and...what the people I follow are liking. ” Participants thought that the connections
they form on TikTok actively influences the content the algorithm chooses to recommend to them.
Participants perceived the algorithm to also recommend another user’s content on their FYP
when it believed that user is part of their networks on (those they know solely on the platform)
and off TikTok (people they know offline). The algorithm accounting for who a user is following
and this contributing to how content is visible and recommended for one’s FYP was described
by P14: “A couple of weeks ago, I had some very specific posts about [state name removed] that only
[people in state name removed] would understand for someone who lived in [state name removed].
Again, I saw this friend who I recognized the name...if I follow somebody I know, I’m going to be
recommended to people they follow and I could follow or I could know. ” It is important to note that the
algorithm recommending to P14 the users she knows offline can have privacy and context collapse
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:12 Nadia Karizat et al.
[51] implications for TikTok users, as they may work under the assumption that TikTok is a space
separate from their existing networks where they can connect with a broader audience with whom
they do not have pre-existing ties.
In these cases, the algorithm is thought to perceive a user’s interests and thus, person identity,
by assuming a user would be interested in content their networks on and off TikTok were thought
to be interested in.
5.1.3 Popular and Trending Content. Participants also felt the algorithm accounted for popular and
trending content. They viewed the algorithm as responsible for promoting trending content more
broadly, as popular content can reflect what is ‘accepted’ and often seen as ‘ideal’ in society. This
’ideal’ reflected in popular content may inform people’s person identities, proxied through their
interests. In this sense, participants believed that the algorithm’s understanding of other users’
interests and person identities shaped the algorithm’s understanding of their person identities
and interests. P12 stated their FYP may contain “some of the challenges and things that are already
trending.” This echoed Eslami et al’s Global Popularity Theory of Social Feeds, in which the
algorithm is believed to prioritize popular content [30].
Participants’ remarks spoke to a belief that the algorithm assumes that because a video is popular,
it would be of interest to a user and be in line with their person identity. Some also perceived the
presence of trending content on their FYP as the algorithm testing out if a user enjoys or does not
enjoy the content. In other words, the algorithm testing if the user’s person identity and interests
are similar to those of many other TikTok users. For example, P15 explains: “So, I receive dance
content, fashion content, and then sometimes videos that are trending, they’ll put it onto my page just
to see if I would like it...I guess they may be testing it to see if people like it or not. ” Participants felt
the algorithm assumes one’s person identity and interests reflect and are determined by what is
popular. However, some argue this assumption can make their feed less desirable when the popular
content does not match with a user’s interests. As explained by P6: “I feel like I get fed a lot of
trendy things that aren’t necessarily what I want to see. ” Participants spoke to the belief that the
algorithm may recommend videos to them outside of their interests due to its popularity among
the platform’s users, assuming what is of interest to many would be of interest to them and part of
their person identity.
5.1.4 Dangers of Algorithm’s Tailoring Feeds to Person Identity. Participants also identified risks
when the algorithm catered to and prioritized a user’s person identity, such as the algorithm creating
a FYP promoting unhealthy behaviors when those behaviors relate to a user’s perceived interests.
P15 explained how this happens with pro-eating disorder content: “they watch a video that maybe
promotes what they eat in the day, but they’ve only eaten 500 calories for the day. . . TikTok notices
they watch that video. . . will continue to push more content like that on their page. ” Participants felt
the algorithm’s prioritization of a user’s person identity could be detrimental, particularly because
the algorithm is believed to not be proactive in adjusting how to recommend videos to users who
consume potentially harmful content.
Relatedly, participants described concerns about the algorithm creating filter bubbles, exposing a
user only to videos reinforcing their own beliefs. P8 described the heightened risk when a user,
for example, is racist, and the algorithm caters to this aspect of their person identity: “If the user’s
racist and they like white TikTokers’ content, but don’t like people of color, TikTok content, then
the algorithm’s just going to learn that and give it back to them...The algorithm isn’t specifically
programmed to not be racist, right?” P8’s comment notes potential dangers when users’ person
identities include interest or lack thereof in certain content as a consequence of being racist, and
the algorithm feeding into this harmful mindset. Of course, social identities, such as one’s own
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:13
race, likely shape what their race-related person identities are (e.g., interests in white supremacy,
anti-racism, etc.), and these identities are interconnected.
These participants’ accounts reflect tensions arising when algorithms feed content into social
feeds believed to be desirable to a user’s person identity in important contexts such as health,
identity, and politics which can have implications ranging from promoting unhealthy behavior to
racism and other harms. Recent research ([ 86], [38], [37]) has begun to explore the effect algorithm
filtering or personalization has in creating filter bubbles [ 62]. Early research identifies the effect
as exaggerated [ 38], or minimal [ 37]. However, these investigations on the effects of algorithms
filtering and personalizing content speak to participants’ and other researchers’ concerns around
how algorithms tailoring content may or may not reinforce filter bubbles.
5.2 Algorithm Suppressing Certain Social Identities
Participants’ comments pointed to the belief that some social identities are suppressed by TikTok’s
algorithm. Many participants held strong ideas about who and what is suppressed and amplified
by the algorithm. They did not believe people of all social identities have an equal chance being
amplified by the algorithm to a wide audience on TikTok’s platform. P13 explained how he felt
TikTok’s algorithm emulates society’s values: “If you’re a person a color, if you’re overweight, if
you’re not conventionally attractive, I imagine ... That’s just the way society has always been. If you
are a pretty white person, you’re probably going to get more of a chance than a pretty black person,
or a white person who maybe is heavier than you, or a white person that maybe they don’t have a
face that’s as symmetrical as yours. ” Participants had many thoughts on how the algorithm valued
different social identities; these thoughts were often informed by who they saw and did not see on
their FYP. Identities participants believed to be suppressed were on the basis of race and ethnicity,
body size and physical appearance, ability status, class status, LGBTQ identity, and political and
social justice group affiliation, as we describe in the remainder of this section.
5.2.1 Race and Ethnicity. Participants perceived TikTok’s algorithm to suppress content about and
by users who are people of color, amplifying content about and by white users. As P1 stated: “From
what I can tell, from what different people post on TikTok, is that TikTok has a habit of suppressing
artists of color and TikTokers of color, in general. From what I saw on my feed, if you would ask me
to describe the demographic of people who use TikTok, I would have told you that it was rich White
people and teenagers...I think it’s because the algorithm doesn’t see Black people as something that
other people would want to go and see. " Participants, including participants of color, noticed the
large number of white people on their FYP feeds. Participants expressed the impact this perceived
evaluation has on creators of color. As P10, in reference to his Indian ethnic identity, said: “I feel
like being brown puts me at a disadvantage as a creator. ” This was echoed by P15 who explained that
her Asian racial identity makes it more difficult to reach a larger audience on the app: “It does give
you a disadvantage maybe because I do feel like white people are more successful on TikTok [...]. ” This
perception of white creators and content being more prevalent on one’s FYP led to the belief that
the white social identity was amplified by the algorithm and deemed more valuable and sought
after than content about and by people of color, rendering people of color invisible or less visible
than white counterparts.
5.2.2 Class. Participants felt the algorithm valued content made by those of a higher socioeconomic
class by amplifying videos with backgrounds, environments and aesthetics traditionally perceived
with wealthy lifestyles. For instance, P14 described the prevalence of wealthier backgrounds and
homes in videos on TikTok: “Very nuclear, very white picket fence. It’s almost always big houses. The
ones where if you ever went in them, your friend’s house, afterward you’re like, ‘Oh my god, it’s a
mansion kind of thing. ” The perception of amplification of content about and by affluent creators
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:14 Nadia Karizat et al.
was echoed by P1, mindful of her lower socio-economic status and racial identity, who described
the lack of other lifestyles recommended for her FYP: “...I didn’t see everyday Black people who
lived in apartments that I could see like, ‘Oh yes, that’s an apartment. ’ I recognize the aesthetic of an
apartment in a, not that great, neighborhood...It almost made me feel like I didn’t quite belong on
TikTok, because I don’t have great lighting. I’m not going to buy a ring light to make TikToks, that’s
just not going to happen. It’s the sun, it’s the fluorescents, or it’s nothing. ” Participants’ remarks
spoke to beliefs that those who appear to be wealthy are valued more on the platform.
5.2.3 Lesbian, Gay, Bisexual, Transgender, and Queer Identity. Participants thought the algorithm
did not amplify content by those in the Lesbian, Gay, Bisexual, Transgender, and Queer (LGBTQ)
community. For example, P3, a Bisexual woman, felt that unless one actively sought after LGBTQ
content, the algorithm would not show it to them—the “default” algorithm excluded LGBTQ content
and curators. As she explained: “I had a thing for a while where I was watching a lot of LGBTQ and I
noticed that my For You page immediately went to every other video was a trans person or a lesbian, or
somebody who was Bi and it was refreshing. It was like, wow, I didn’t know there was all these people. I
think TikTok kind of hides that a little bit unless you make the conscious decision to be like, ‘Oh, I like
this. ’” The belief that the algorithm suppresses LGBTQ videos was also echoed by P13, a Gay man,
who explained a growing wave of videos aiming to highlight violence against transgender people
and his belief that the algorithm would probably not amplify the content because of it being related
to the LGBTQ social identity. As he described: “...We’ve been also having a little ongoing discussion
about violence against transgender people and things like that, so I would also not be surprised if
there’s some stuff about that that people were trying to talk about, or at least call attention to, that
wasn’t getting a lot of traction because of the algorithm. ” Participants thought the algorithm viewed
LGBTQ content as something not valued or worthy of recommending to its users, unlike that
of cisnormative and heteronormative content. Our findings corroborate those reported in [ 72]
finding that LGBTQ+ users thought LGBTQ content creators were not prioritized by the platform’s
algorithm, and were subjected to limited visibility on the For You Page. Furthermore, they found
that participants spoke to beliefs of the algorithm “enforcing stereotypical presentations of LGBTQ+
identity” through the LGBTQ content that was visible” [72].
5.2.4 Body Size and Physical Appearance. Participants’ comments expressed beliefs that body size
and one’s proximity to conventional beauty standards impacts how the algorithm values videos on
TikTok. P14 explained her belief that the algorithm suppresses “people who look ugly....Noticeable
facial deformities or just, they’re ugly. They will favor people who are thought of as the golden ratio
kind of thing. ” P14 went on to describe a specific example comparing two successful celebrities,
Will Smith and Lizzo, who she felt were amplified differently by TikTok’s algorithm: “The difference
is that Will Smith is big on Tik Tok and Lizzo is big on Tik Tok, but Lizzo is fat and Will Smith is a
very handsome, classic celebrity. I didn’t see Lizzo pop up until a few weeks ago. She’s been on there
for months. ” Participants felt size and attractiveness played a role in how content was valued by
the algorithm in this case, P14 felt Lizzo, a popular American Black women singer, was suppressed
due to her larger body size despite her popularity in the media.
5.2.5 Political and Social Justice Group Affiliation. Participants’ statements spoke to a belief that the
algorithm suppressed content about certain political and social justice-oriented group affiliations.
For example, P14 described what she felt was targeted suppression of different protests, comparing
the Black Lives Matter protests to those protesting mask mandates during the Coronavirus pandemic
in the US in 2020: “I think sometimes it’s a little targeted. For example, it has been known that a lot
more of the social justice protests for Black Lives Matter get taken down more than the other protests
that were against wearing a mask. The ones that happened at several capitals that also showed guns.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:15
So it’s telling I guess, that they will take down the disagreeable opinions and sometimes leave up the
other ones. ” P14 perceived the different evaluation of anti-mask and Black Lives Matter protests as
evidence of the algorithm’s targeted suppression of certain political and social justice viewpoints.
Participants proposed multiple reasons about why the algorithm did not amplify political and
social justice-related content widely on users’ FYPs. For example, some felt that political and social
justice content was suppressed in an attempt by TikTok to maintain a certain image as a company
and platform. As P13 explained: “They don’t want to be the serious, ‘Let’s have political discussions.
Let’s discuss police brutality or the election, ’ or things like that. They want to be the fun, frivolous one,
where you just turn off your brain and look at something funny for a few minutes. ”
Others felt the motivation to suppress videos of Black Lives Matter protests was related to the
recent protests occurring in Hong Kong because both protests addressed police brutality. Multiple
participants’ remarks mentioned TikTok as being a Chinese company and highlighted a belief
that it then made sense the company would sensor and heavily moderate content on the platform,
referring to the reputation of the Chinese government for filtering its media. As described by P2:
“It is a Chinese app and it’s just like, this is exactly what I would expect from a country who suppresses
the media in that country and limits what people can see, and also had a really poor response to their
own protests.” For multiple reasons, participants felt the algorithm did not amplify political and
social justice related content widely on users’ FYPs. However, whether or not these reasons for
these beliefs map reality, or to what extent the app being founded in China may have shaped how
TikTok manages content is a story worth exploring separately. For instance, one could argue that
Facebook, an American company has also been shown to have problematic content management
strategies. Here, however, we reflect on participants’ understanding of what content gets promoted
and what content does not, and the way they made sense of these observations and experiences.
5.2.6 Ability Status. Participants’ remarks highlighted beliefs that those who were able-bodied
were amplified more than those who were not. As P14 explained about her FYP: “You really don’t
see any people with disabilities with that very general sub set. ” She continued: “I do remember reading
a statement from them when people found out that they prioritized richer, whiter households over poor,
disabled, and ugly people. ” The algorithm was believed to value the content of and by able-bodied
people on the platform. While we only had one participant who discussed disability, we highlight
this example because it is still a valid experience to be heard, and we position disability along a
wider array of identities participants felt were suppressed on the platform.
6 RESISTING THE ALGORITHM AND SHAPING ALGORITHMIC IDENTITY
In this section we address RQ2, concerned with how TikTok users’ perceptions of the algorithm in
relation to identity shaped their behaviors on the app. While RQ1 explores users’ perception of
how the algorithm operates in relation to identity, RQ2 centers on how these beliefs motivate user
behaviors to attempt to change the algorithm. We show how participants attempted to shape their
algorithmic identity (i.e., the algorithm’s definition of the user) [ 22] by changing their personal
engagement with videos so that the content recommended to them by the algorithm better reflected
their understanding of themselves and what they are interested in—their person identity. We also
identify a range of individual, collective and performance behaviors that participants took to resist
and change how different social identities are affected by the algorithm.
6.1 Changing Personal Engagement to Shape Algorithmic Identity to Align with a
User’s Self-Concept
Participants changed their personal engagement on the platform to shape their algorithmic identity
[22] and influence the content shown on their FYP to better reflect how they saw themselves and
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:16 Nadia Karizat et al.
their interests. They did so to have their algorithmic identity [ 22], the algorithm’s definition of
them, match the conception they have of themselves. Participants assessed the success of the match
between their algorithmic identity [ 22] and self-concept [ 61] based on if the videos recommended
to them by the algorithm matched what they wanted to see, how they understood their person
identity, and thus their interests.
We found that participants’ algorithmic folk theories motivate certain behavior aimed to coach
the algorithm to display or not display certain content on the user’s FYP to reflect their interests and
person identities. Acting under the Personal Engagement Theory of Social Feeds [ 30] also observed
in our data, participants sometimes chose activities that directly reflected whether they were or
were not interested in a video’s content. For example, P3 said: “I have purposely liked something
so I can see more of it”. P1 provides another example of changing her personal engagement: “Just
following more people, following those mental health pages...this is content I want to have on my For
You page. ” Participants’ perception of the algorithm also influenced their on-app networks, and the
hashtags they chose to engage with in hopes of shaping the algorithm to their wants and interests.
These efforts reflect participants hoping to change how the algorithm understood their person
identity to curate a FYP where their algorithmic identity is believed to be more aligned to their
self-concept, and thus, person identity.
6.2 Resisting the Suppression of Certain Social Identities
Participants’ perceptions that the algorithm suppressed certain social identities (based on race
and ethnicity, body size and physical appearance, ability status, class status, LGBTQ identity, and
political and social justice group affiliation) resulted in changes to their behavior to alter how
the algorithm engaged with these identities. We identified these changes as individual actions,
collective actions, and content creators altering the ways they performed in their video content.
6.2.1 Individual Actions. While individual actions took place as part of the relationship between a
single user and the algorithm in attempts to impact their person identity (6.1), individual actions
also took place by users who understand their individual role within a community of users holding
a variety of social identities. As a result, in addition to person identity, participants tried to impact
social identity: they reported intentionally engaging with content about or from creators with social
identities that they perceived the algorithm to suppress, hoping this content would be amplified
by the algorithm in turn. For some participants, this meant tailoring their personal engagement
to amplify and resist a video’s (or its video category’s) suppression due to the social identities
it reflected. P10 described in great detail the ways he engages to amplify content he believes is
suppressed by the algorithm: “Ever since it came up that TikTok is suppressing certain groups, I try to,
if I find creators from that certain group, I’ll try to follow them or share their content. . . just to kind of
try and compensate for what the algorithm isn’t doing. . . sometimes if you really want a video to do
well, you can like it and then you can watch it a couple times, rather than just once, so that it’ll see the
completion rate of the video, because that’s apparently a really important metric they have, is how far
into the video did the user watch? And I’m not following them just for their race or something. I’m
following them because they generally make good content. But I’m also going out of my way to make
sure that their content also gets out to other people. Right? And so it’s not just being muffled out by
the algorithm. ” P10 demonstrated how his theory of TikTok’s algorithm suppressing certain social
identities was informed by the popular press, and thus, influenced his actions on the platform. How
a participant believes the algorithm values social identities directly motivated their engagement;
this can lead to users increasing their level of engagement with content about or creators with
social identities they believe are suppressed.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:17
Participants’ efforts in resisting the suppression of certain social identities was not only limited
to their own feed. They also tried to have a larger impact on other users’ algorithmic feeds:
participants sometimes went on following sprees of users of a certain social identity in the hope
that the algorithm will spread the user’s content to other users’ FYPs on the platform. P1 described
her FYP initially lacking racial diversity: “Because I’d never saw Black people on my For You Page. I
could not therefore go and explore other Black people...There’s just not diversity. ” She later explained
that she intentionally follows Black creators on the app: “I was just like, ‘Follow, follow, follow, follow,
follow, follow, follow, I don’t know what you’re doing, just, I’m following you. ’ Just to support. Because it
helps and maybe it’ll boost that person onto someone else’s For You Page. ” Participants’ understanding
of how content gets boosted and recommended to other users informed the ways they chose to
engage with those less likely to become widespread or popular because of the algorithm’s believed
unequal evaluation of social identity. Their perceptions of the algorithm (informed by individual
experience as well as news about the platform), paired with what social identities they thought it
values, led to interactions and activities to resist social identities’ suppression in turn.
6.2.2 Collective Actions. Participants’ resistance against the algorithmic suppression of certain so-
cial identities went beyond individual efforts when users tried to influence the algorithm collectively .
Believing social justice content on the app was important and worthy of being widespread by the
algorithm, participants noted collective actions make the algorithm amplify social justice content
on the platform. Some examples of these actions were users collectively commenting and liking a
video believing it would then be boosted by the algorithm, users with perceived privilege lending
their account to those with social identities believed to be suppressed by the algorithm, as well as
users choosing to abstain from producing content for a period of time. While prior work explores
intra community efforts by marginalized LGBTQ users in response to transgressions by TikTok’s
algorithm [ 72], our findings show inter- andintra community identity work by identifying the ways
users from different identity communities and with varying levels of privilege and marginalization
work together to challenge the perceived suppression of certain social identities.
In response to the perceived suppression of social justice-related content on TikTok (in the
American context in this case), participants detailed collective actions they have witnessed and/or
participated in to amplify the content and spread social justice messages. For example, P14 described
the collective “we” on the platform that engages with videos to spread social justice content to a
larger audience on the platform: “For spreading social justice issues, we try and help the algorithm by
the longer and more comments you leave, the better it’ll spread. The more times you like it, over and
over, the more it’ll spread, the more times you hit the share button, doesn’t matter if you actually share
it or not, it will help it spread. The algorithm picks up on all that. ” P14 continued on providing an
example of spreading social justice content exposing police brutality: “A lot of times they’ll post
videos of cops doing unjust things to protestors or against protestors as a way to get that message out.
And I will help spread those so people can understand why people are doing that [protesting] and try
and get people removed from that position if they’re not good at it. ” P10 described how his audience
collectively engaged with his videos supporting Black Lives Matter protests, noting nearly 5,000
comments similar to one such as: “Oh no, it looks like I’ve accidentally commented for the algorithm. ”
These comments reflect a shared understanding of the algorithm valuing a video’s engagement
when choosing to amplify videos. Seeing value in spreading social justice content on the platform,
yet believing that the algorithm suppresses the content of that social identity, participants engaged
in behaviors to have the algorithm amplify the content to others.
Participants recognized TikTok accounts as spaces operating within TikTok’s ecosystem—an
ecosystem with people and content that many, as described earlier, understand as being valued in
discriminatory ways by the algorithm. As P1 described, some non-Black users lent their accounts
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:18 Nadia Karizat et al.
to Black creators so they could use their platform and get important messages to an audience the
creator otherwise wouldn’t reach due to the algorithm: “There’s one creator [...] she’s very popular...she
had her friend who’s a Black woman take over her page for a day and respond to comments and posts
about the Black Lives Matter movement...And I think that more people should do that. Cause it’s a
great way to get your primarily white following to listen to these important issues. ” She contrasted
this behavior to what she felt was more performative allyship by non-Black users creating videos
where they “stand up to the police and show that they’re in support of Black Lives Matter or show
that they have privilege. ” She further explained: “It feels very performative... and like, ‘Oh, okay, so
right now Black Lives Matters the trend, so got to make sure that I’m on the right side of this trend.
And then I can go back to doing whatever I want. ’" Despite this example of what can be considered
performative activism on the app, participants also observed others on the platform using their
account and perceived privilege to amplify the voices of those with social identities’ suppressed on
the platform.
In addition to some users sharing the space on their accounts, other participants who created
content spoke of abstaining from making content for a duration of time to draw the algorithm’s
attention to social justice content. P15 detailed not producing content in hopes the algorithm would
instead boost content related to Black Lives Matter: “I would notice that other people were doing
this thing called Blackout Tuesday or silent for the whole week, and the reasoning why I chose to
participate in this because it allows content that’s informative to be pushed up. Because if you post,
that could be in the way of an informative TikTok going onto someone’s For You Page; It helps with the
algorithm” Participants producing content understood their videos operating in TikTok’s ecosystem,
understanding that their videos’ amplification means another’s suppression. This motivated actions,
like abstaining from producing content, to drive the algorithm to amplify social justice-related
content it otherwise would not.
Drawing on these insights and Section 5.2 (Algorithm Suppressing Certain Social Identities), we
introduce the concept of algorithmic privilege to refer to those holding social identities believed to
be amplified by the algorithm and unaffected by social-identity based suppression. We elaborate on
this concept in the Discussion.
6.2.3 Altered Performance of User Content. Some participants who were TikTok content creators
shared how believed suppression of social identities motivated themselves and others to alter the
ways they performed in their content. Some noted content creators produce content displaying video
aesthetics perceived to be valued by the algorithm, so their videos are amplified; the assumption
being that they would otherwise be suppressed by the algorithm due to their own and affiliated
social identities. As explained by P1: “TikTok does not love it when people speak out against the system
or really against anything that they don’t agree with. So this whole Black Lives Matter movement has
really shown TikTok’s hand in everything...So people have been kind of just trying to get around it
and trying to be like, ‘I uploaded this audio and here I’m showing you doing art, but I just wanted to
remind you that all the cops are really just hurting protesters right now. And you guys need to get out
there and protest. ’ And so it’s really uplifting to see people literally working against an app, a system
in order to get their voices heard and to get this message across. ” Users’ understanding of what the
content amplified by the algorithm looks and sounds like, shaped the aesthetic and design choices
in their videos.
P10 provided a firsthand example of P1’s observation, retelling a time he made a video in support
of Black Lives Matter protesters after videos surfaced of police using teargas: “There was this video
I made about protesting and I said, ‘Oh, it’d be a really bad thing if protestors found out that they
can buy leaf blowers on Amazon for $20 and blow teargas back at the police. ’” P10 chose to deliver
information he perceived would be suppressed by the algorithm in a way that the algorithm would
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:19
be less likely to detect for suppression. Participants’ beliefs of how the algorithm values social
identity influenced the ways they chose to present in their videos to increase the likelihood of their
videos being amplified. Participants’ theories of the algorithm altered their performance in the
content they created.
7 DISCUSSION
In this Discussion section, we first apply a co-productionist [ 44] lens to our findings to portray the
dynamic relationship between the algorithm and the user, and argue that the algorithm plays a
role in producing how we understand and make sense of different identities—such as how a user
understands their interests as part of their own person identity, as well as how they conceptualize
certain social identities. We further discuss the algorithm and user conceptions of identity in our
proposed new algorithmic folk theory— The Identity Strainer theory —where users perceive the
algorithm as a system that filters content based on social identity and creates meanings of which
social identities are valuable and deserving of visibility, as well as which social identities have
algorithmic privilege . We argue that algorithms and what algorithms are perceived to do and value
have real consequences for users and society, which we conceptualize as algorithmic representational
harm . We discuss the impacts of these consequences and their role in motivating users’ algorithmic
resistance [ 77], and advocate for considering identity and its perceived relationship with the
algorithm in future efforts to improve algorithmic experience (AX) [ 77] for users on social media
platforms to combat algorithmic symbolic annihilation [ 9] and lack of algorithmic privilege, mitigate
algorithmic representational harm and support users’ efforts to achieve representational belonging
[21].
7.1 Co-Production: Making Knowledge of Identity on TikTok
Participants in our study demonstrated that their behavior on the app was motivated and shaped
by the algorithmic folk theories they developed. They remarked that their behaviors were shaping
the FYP algorithm that shaped their experience on the platform in turn. We theorize these complex
connections using the co-production framework.
“Co-production” is a framework for knowledge-making that moves away from determinism to
an understanding that “knowledge and its material embodiments are products of social work and,
at the same time, constitutive of forms of social life" [44]. Co-production explores how knowledge
shapes and is shaped by “people’s deeper political and cultural, as well as cognitive and material
commitments" [ 44]. Co-production provides a useful lens to interpret our findings because of the
framework’s fluidity and its aim to find connections between our knowledge and how we come
to have it, as opposed to more rigid deterministic approaches. For example, a co-productionist
framework would not claim that participants intentionally engaged with content posted by members
of certain social identities, without also acknowledging that they did so because they expect the
algorithm to suppress these social identities. Additionally, the framework would continue to state
that if participants didn’t perceive social identity suppression in the past, they wouldn’t have
the knowledge to expect the algorithm to suppress certain social identities now. While we argue
these intentional engagements were co-produced by human users andthe algorithm, the human
aspects of identity within this relationship are also being co-produced; in this paper, we theorize
the co-production of knowledge of person and social identity by human users andalgorithms.
Applied to our work, co-production allows us room to grapple with what shapes and is shaped
by the diverse productions of knowledge on identity that are found in technological systems and
spaces.
We argue that TikTok users and the FYP algorithm co-produce, and thus make, knowledge of
identity on the platform, including person and social identity. Participants’ remarks reflect a belief
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:20 Nadia Karizat et al.
that the algorithm recommended videos for the FYP based on how it understood their person
identity, with the algorithm assuming their interests based on their personal engagement, networks
on and off TikTok and what is popular on the platform at a certain time. From these three things,
the algorithm was believed to create an algorithmic identity [ 22] containing inferences about
what a user’s interests and thus person identity were. Users and the algorithm co-produce the
ways in which users are ’known’ or ’defined’, thus knowledge of their person identity, on the
platform. As participants view videos recommended to them by the algorithm and engage or do
not engage with those that reflect how they understand their person identity, they attempt to
redefine their algorithmic identity [ 22] and achieve better alignment between their algorithmic
identity and self-concept [ 61]. While our findings spoke to participants and algorithms co-producing
knowledge of their person identity within an algorithmic system, further research might explore
the extent to which algorithms shape and influence a user’s own person identity. For example, if
an algorithm feeds a certain video topic into one’s social feed, how much exposure or what type
of video presentation may lead to establishing an interest or lack thereof in that topic, and thus
changing one’s person identity? This could have implications around the impact of algorithms and
targeted advertising, mental health and well-being, conspiracy theories, etc. For example, a user’s
person identity may change to include an interest or lack thereof in a certain fashion aesthetic after
receiving targeted advertisements for brands that reflect that style; of course, there is a flip side
to any such algorithmically-informed identity change, in that it could also mean harmful person
identities (e.g., interests in white supremacy, disordered eating, etc.) would be promoted.
The FYP algorithm and TikTok users also co-produce knowledge of social identity on the platform.
Participants believed that TikTok suppressed certain identities—this belief relied on participants
having formulated ideas of what a certain social identity is. When we, humans, create social cate-
gories, we formulate general ideas, beliefs and expectations about the people who are in a category
[45]. Our assessment for whether a user fits into a social category is determined by “the degree to
which observed similarities and differences between people correlate with the expected social categories”
[17]. These categories are not fixed, but dynamic, fluid, and contextual. TikToks’ algorithm and its
users co-produce definitions of race and ethnicity, body size and physical appearance, ability status,
class status, LGBTQ identity, and political and social justice group affiliation on TikTok by both
simultaneously categorizing users on the app into these various identities, and articulating what it
means to be of a certain social identity. Definitions co-produced by algorithms and users reflect
how technologies act as “instruments that enforce meaning...and help construct the social world” , as
Benjamin notes [11, 65].
Algorithms, themselves, are engineered computational methods [ 47] with biases integrated
into their design when built and implemented [ 57]. The algorithms engineered to classify social
identities are informed by their creator’s beliefs of what these identities are, and TikTok user’s
behavior on the app is influenced by how the algorithm classifies these categories of identity (i.e.
perceived suppression or amplification of a certain social identity.) For example, participants felt
TikTok’s algorithm was programmed to identify racial categories and suppress people of color. They
shared examples of changing their behavior on the app to resist this suppression; this behavior is
informed by their knowledge of what a racial identity is and this knowledge shaped the videos they
chose to interact with —— and thus their behavior on the app. We conclude that both algorithm
and the user are working together in a way that co-produces knowledge of these racial categories
and other social identities.
It is important to acknowledge that in co-producing knowledge of social identity, there are
opportunities for definitions and understandings that exclude those who do not fit expectations. In
[72], the authors found LGBTQ TikTok users had concerns that "specific normative intersections of
LGBTQ+ identity are becoming more visible and thus more normalized through the FYP algorithm"
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:21
[72], and that this propagation of specific presentations of LGBTQ identity led to invalidating
the identities of LGBTQ users who did not fit these norms, resonating with prior work around
intra-community dynamics in other LGBTQ spaces [ 79]. While [ 72] looks at the algorithm as one
decision-maker for amplifying normative representations of certain social identities, a co-production
framework, as we have argued here, allows us to further explore how human users co-producing
knowledge of social identity on the platform inform and contribute to the amplification of normative
performances of these certain social identities through their engagement with algorithmic systems.
More broadly, as we have shown, the co-production lens is useful in future studies to interrogate how
algorithms and human users engage with and create knowledge of social identity. Further research
that looks at identity work on social media platforms might explore the simultaneous factors
informing this work. For example, instead of exploring the actions users take in response to an
algorithm facilitating or interfering with community building around a social identity, researchers
should also incorporate into their analysis how users developed understandings of the algorithm,
what algorithmic folk theories directly motivated these actions, etc. In other words, instead of more
deterministic approaches to exploring relationships between algorithms, identity and their users,
we encourage future work to explore these relationships as more fluid and intertwined.
7.2 The Identity Strainer Theory and Algorithmic Privilege
Some of our findings regarding TikTok users’ algorithmic folk theories mirrored past research
of folk theories of social feeds by Eslami et al. [ 30], theories that demonstrated users believed
their social feeds were influenced by their personal engagement, the format and popularity of
content, and more. Eslami et al.’s Eye of Providence theory of social feeds [ 30], specifically, was
based on users believing the social media platform is an all-knowing, powerful entity that oversaw
all content displayed in the news feed (in that case, Facebook’s news feed), filtering out, prioritizing
and/or handling the distribution of certain content. While this theory is similar to our participants’
algorithmic folk theories, their Eye of Providence theory does not account for participants’ theories
of the algorithm filtering content based on social identity, nor their belief that they can still yield
power against the algorithm and resist the algorithm’s perceived suppression with their behavior.
We extend Eslami et al. ’s work by showing that these folk theories are not unique to a platform like
Facebook (where people are required to use “legal” names and are connected to others they have
pre-existing ties with), and apply to a platform like TikTok that does not enforce a name policy
and where people’s connections are wider beyond or separate from their existing social networks.
Building on this previous scholarship [ 30], we contribute a new folk theory of social feeds:
The Identity Strainer Theory . This theory is explained as users believing that their social feeds
are the result of an algorithm recognizing, classifying, sorting, and suppressing social identities
based on its conception of which social identities are (or are not) “valuable” and “wanted”, or
which ones (do not) deserve visibility. The algorithm acts as a strainer, impacting which social
identities appear on their FYP feeds. It furthers the idea that the algorithm contributes to the
marginalization of social identities based on race and ethnicity, body size and physical appearance,
ability status, class status, LGBTQ identity, and political and social justice group affiliation. This
translates into some users having what we refer to as algorithmic privilege , referring to benefits
stemming from algorithms operating on the basis of identity, and valuing some identities over
others. On TikTok, this translates to algorithmic privilege being held by users with social identities
amplified and not at risk of suppression by the platform’s algorithm. While users did not believe the
algorithm completely filters out this content from TikTok’s FYPs, they did share strong thoughts
and experiences reflecting the belief that it limits how widespread and visible creators and content
involving marginalized identities are on the platform. Within this theory, however, the user does
not believe the algorithm’s evaluation of a social identity’s worth on the platform is permanent.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:22 Nadia Karizat et al.
Instead, the users believe they can shape how the algorithm values different social identities by
changing their own behavior in ways that target and boost those identities.
Noble’s work in Algorithms of Oppression notably draws direct connections between algorithms,
suppression and identity through her countless examples of algorithmic bias found in search engine
results [ 57]. While our theory and Noble’s work are both concerned with suppression and identity,
we are introducing an algorithmic folk theory specifically addressing the believed algorithmic
suppression of social identity. This theory serves as a point for further research on the impacts the
belief in this folk theory has on user behavior and experiences on social media platforms. As our
findings demonstrate, participants believing this theory were motivated to engage in algorithmic
resistance, as discussed in section 7.3. Further research might specifically explore how users seek
to spread awareness of this theory within the very platform they believe it exists. Additionally,
it would be valuable for further work to audit [ 69] TikTok’s algorithm to empirically explore the
absence or presence of certain social identities on a user’s FYP in light of the identities they hold,
as our research showed that many participants’ belief in this theory was informed by noticing an
absence or abundance of certain social identities. As a final suggestion, further research could look
into how widespread belief in this theory is by users on the platform, particularly across different
demographics.
7.3 User-Algorithm Interaction: Algorithmic Resistance to Promote Representational
Belonging through Countering Algorithmic Representational Harm, Algorithmic
Symbolic Annihilation, and Lack of Algorithmic Privilege
Velkova and Kaun describe algorithmic resistance as a “complicit form of resistance, one that does
not deny the power of algorithms but operates within their framework, using them for different ends"
[77]. They describe this resistance as a form of ‘repair politics’ in its efforts to correct (and repair)
perceived representational problems in the algorithm’s outputs by working within an algorithm’s
framework to influence and shape its outputs [ 77]. Participants in our study described forms of
algorithmic resistance by expressing the ways they used the affordances of TikTok’s technological
environment in an attempt to shape (and repair) the perceived suppression of different social
identities on TikTok. Participants targeted following users, and sharing of content to resist a
video or category of video’s perceived suppression based on social identity serve as examples of
productive modes of resistance [33] to TikTok’s algorithm.
We theorize this resistance as actions that participants took to achieve what is referred to as
“representational belonging” and to combat “symbolic annihilation,” concepts rooted in Feminist
Media Studies. Representational belonging refers to “affective responses community members have
to seeing their communities represented with complexity and nuance” [21]. Symbolic annihilation
refers to how the mass media “symbolically annihilated” women by largely either ignoring them
or portraying them in stereotypical roles [ 76]. These concepts have been applied to analyzing
representation or lack-thereof in mass media for women [ 76], and marginalized groups [ 24,55,56,
78], as well as new digital media such as games [ 42]. Researchers have also applied the concept
of symbolic annihilation to the context of algorithms under the umbrella term of “algorithmic
symbolic annihilation”, describing “how algorithms perpetuate normative and stereotypical narratives
about phenomena, where what they account for has power and authority, and what they do not account
for does not” [9]. A concept similar to algorithmic symbolic annihilation [ 9] is algorithmic exclusion,
constructing exclusionary spaces that render some identities invisible and marginalized [ 72]. We
align the concepts we closely draw from and develop, within the long historical context of feminist
media scholarship reviewed above.
Building on the concepts of algorithmic symbolic annihilation [ 9] and representational belonging
[21], and recognizing the harm participants in our study experienced, we introduce the concept of
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:23
algorithmic representational harm to refer to the kind of representational harm that algorithmic
systems’ users experience as a result of being rendered invisible, trivialized, suppressed, or otherwise
further marginalized on the basis of their identities and the algorithm’s understanding of their
identities. Participants resisted algorithmic symbolic annihilation to achieve representational
belonging on the platform, and to combat algorithmic representational harm. Some participants
shared with us specific experiences of their own social identities being suppressed on the platform,
such as P10, an Indian man, and P15, an Asian woman, sharing they felt disadvantaged as content
creators due to their marginalized racial identities, and P1 sharing they felt they didn’t belong
on TikTok due to their socioeconomic class. Further research, following our work and works like
[72], could explore how algorithmic representational harm is experienced and resisted when users’
own marginalized social identities are being rendered invisible, suppressed, etc. by algorithmic
systems. Such work could also take an intersectional approach [ 25] to examining experiences of
representational harm by those holding multiple intersecting marginalized identities.
Rader and Gray argue that “because user behavior is both input for algorithms and constrained
by them, these patterns of belief may have tangible consequences for the system as a whole”
[64]. Therefore, the participants’ folk theories can have direct impacts on TikTok’s technological
system. Participants’ resistance to the algorithm was informed by their algorithmic folk theories
and influenced the input to TikTok’s algorithm, believed to be used to curate their FYP. Their
experience of the algorithm’s outputs informed their resistance, perceptions of belonging, shaped
their behavior, and affected the data fed into the algorithm in turn. This serves as an important
example of the dynamic feedback loop of user-algorithm interaction [ 22,23]. Users’ interactions
affect TikTok’s algorithmic outputs (on the FYP) and this affects the input fed into the algorithm used
to produce future outputs: curations of FYPs on the platform. Participants’ resistance attempted to
repair the perceived bias and suppression they witnessed on their FYP, in part caused by algorithmic
symbolic annihilation [ 9] leading to algorithmic representational harm, as well to change how
the algorithm represented their person identity through content recommended on their FYP. The
algorithm and TikTok users develop and grow together creating the FYPs on the platform, both
influencing the other in what Shin et al. call a feedback loop [71].
Our findings made it clear that the perceived algorithmic suppression of certain social identities
led to algorithmic representational harm and garnered resistance from TikTok users. Participants
shared experiences engaging in platform-wide organized abstinence of content production as
privileged users, witnessing users with privilege ‘passing the mic’ by letting content creators with
marginalized identities post from their account, etc. While TikTok claims publicly that its algorithm
works to bring a diversity of videos into a user’s FYP [ 3], participants consistently expressed
witnessing a lack of diversity along a wide range of identities. Resistance by the participants
shows how users try to achieve equitable visibility for marginalized identities on the platform,
regardless of if they hold them or not, as moderated by algorithmic systems. Ruha Benjamin’s
identification of seemingly objective technology furthering and reproducing societies existing
inequities as the “New Jim Code” [ 11] are connected to our findings highlighting the perceived
suppression of marginalized social identities. We echo Benjamin’s argument that it is important
to investigate technology’s outcomes—in this case, TikTok users’ beliefs of the algorithm and its
consequential algorithmic representational harm—to identify the ways it perpetuates the “New
Jim Code”, regardless of the technology’s stated intent [ 11]. Participants expressed wanting to
shape how the algorithm prioritizes certain social identities and content to be more equitable.
TikTok and other social media platforms with algorithmically-generated feeds can create a more
inclusive platform by addressing their users’ concerns that motivate actions such as resistance to
the algorithm. In this case, the concern is that social identities are believed to be unequally valued
by the algorithm while benefiting from algorithmic privilege, where some are rendered invisible,
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:24 Nadia Karizat et al.
trivial, or unworthy of being seen —— and thus symbolically annihilated [ 76]. Our findings also
show that when participants believe the algorithm does not correctly understand their person
identity, they engage in algorithmic resistance to try and repair this misunderstanding—to change
how the algorithm defines their person identity.
We also note that concepts of algorithmic privilege and algorithmic representational harm
developed in this paper can be used to interrogate other algorithmic systems in how they engage
with identities. As an example, consider social media platforms with an e-commerce component
(e.g., Instagram) which state they would like to support small businesses. Informed by the concepts
we develop, researchers could examine how and to what extent these algorithmic systems render
small businesses compared to large ones visible. Similarly, we could examine how and to what
extent marginalized small businesses feel visible, and how this shapes or informs their brand or
‘identity’ on these platforms in turn. For example, how do small businesses experiencing algorithmic
representational harm choose to amplify aspects of their identity to resist a lack of visibility and
algorithmic privilege, such as partaking in online social movements calling for consumers to buy
from businesses owned by specific identities (Black-Owned, Indigenous-Owned, Locally-Owned
Businesses, etc.)? These are examples of how the concepts developed here can inform future
research around marginality, identity, and sociotechnical systems.
7.4 Supporting Algorithmic Resistance as Part of Algorithmic Experience
The user-algorithm interactions described in this paper are all part of users’ algorithmic experience
(AX) [ 8] on TikTok. Alvarado and Waern developed the concept of algorithmic experience (AX), “an
analytic tool for approaching a user-centered perspective on algorithms, how users perceive them
and how to design better experiences with them” [ 8]. Through Alvarado and Waern’s research on
how to improve AX for Facebook’s news feed, they identify five design areas to improve users’
AX: algorithmic profiling transparency, algorithmic profile management, algorithmic awareness,
algorithmic user-control, and selective algorithmic remembering [ 8]. These design areas could be
applied to improving AX in the TikTok context. For example, algorithmic profiling management
refers to allowing a user to “corroborate and manage the profiling made by the algorithm” [ 8].
TikTok users changing their engagement on TikTok to have their algorithmic identity better align
to their self-concept could be supported by improving a user’s ability to manage how they’re
profiled and understood by an algorithm. To address TikTok’s users’ social identity suppression
concerns, TikTok should invest in understanding and improving their users’ AX, specifically as it
relates to their experiences and engagement with different social identities on the platform. Further
research should explore TikTok’s AX to identify direct design changes that can be implemented to
algorithms of social feeds that address social identity suppression concerns expressed by users.
Beyond these implications for TikTok specifically, our findings speak to the growing awareness of
algorithms for users of social media platforms, demonstrated by all of the participants being aware
of an algorithm on TikTok versus the majority unaware in Eslami et al.’s 2015 study of Facebook
[32]. Our study illustrates the ways this awareness feeds perceptions that shape user behavior and
the algorithmic inputs and outputs. Algorithms that filter and advertise personalization are not
immune from holding biases [ 14], and our findings show that users perceiving these biases act
in ways that challenge them. As algorithms are impacted by the feedback loop of user-algorithm
interaction [ 22,23], these perceptions of algorithmic bias and of the algorithm——regardless of
whether the perceptions map to what these algorithms technically do——have the potential to yield
real influence on entire technological systems of social media platforms and their feeds.
8 SUMMARY OF CONTRIBUTIONS
This work contributes:
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:25
•An in-depth articulation of TikTok users’ perceptions of the platform’s algorithm and how
they believe person and social identity are negotiated within its technological system, as well
as how algorithm’s tailoring to one’s person identity creates possibilities for causing and
reinforcing harm (e.g., recommending harmful content)
•An argument for the ways identity-related knowledge production occurs on TikTok through
co-production [ 44] between the platform’s algorithm and its users, and thus highlighting
the potential impacts of this identity co-production on how we conceptualize and group
individuals based on identity, valuing some over others
•A new folk theory of social feeds— The Identity Strainer theory —to encompass the beliefs of
users who perceive an algorithm as filtering content based on the social identities, identified
and assumed
•Introducing the concept of algorithmic privilege : privilege held by users who are positioned
to benefit from how an algorithm operates on the basis of identity
•Introducing the concept of algorithmic representational harm to describe the harm that users
of algorithmic systems experience as a result of lacking algorithmic privilege and being
targeted by algorithmic symbolic annihilation [9]
•An identification of three categories of user behaviors influenced by their folk theories to
challenge algorithmic symbolic annihilation [ 9] and algorithmic privilege through algorithmic
resistance [77], while achieving representational belonging.
9 CONCLUSION
We examined TikTok users’ perceptions of the platform’s algorithm in relation to person and social
identities to provide additional insight into algorithmic folk theories and their impacts on users’
identity on social media platforms. We revealed the ways these theories both created and reflected
various identities, and demonstrated how this motivated and influenced user behavior to resist
and/or wield power over how their identities are understood by the algorithm. We uncovered how
users believed identity operated within TikTok’s algorithmic system and how perceived social
identity suppression garnered individual and collective resistance by participants. Additionally,
we identified the anticipated harm of algorithm’s tailoring to a user’s person identity when the
algorithm identified the user as interested in harms such as unhealthy behavior. We also established
understandings of how users attempted to curate their own identities for the algorithm to achieve
closer alignment between their algorithmic identity and self-concept. We argued that both algo-
rithms and users are co-producing knowledge of identity on social media and, as such, play a role
in producing knowledge that holds weight in our social world. We articulated a new algorithmic
folk theory of social feeds— The Identity Strainer theory —to make sense of users’ perceptions of an
algorithm filtering content based on social identity, and argue this perceived filtering of identity
creates ideas of which social identities are valuable and deserving of visibility, as well as which
user hold what we refer to as algorithmic privilege . We demonstrated how users interact with the
algorithm in forms of algorithmic resistance to challenge algorithmic symbolic annihilation, achieve
representational belonging and reduce what we refer to as algorithmic representational harm on the
platform. Lastly, we advocate for the consideration of identity and how it functions (and is believed
to function) with an algorithm in all efforts to improve users’ algorithmic experience to contribute
to the development of genuinely inclusive technologies with representational belonging possible
for all its users.
ACKNOWLEDGMENTS
We are grateful to all of the participants for taking the time to speak with us and share their
experiences, especially during a global pandemic when time and energy is so scarce. We also want
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:26 Nadia Karizat et al.
to thank the AC and reviewers for their anonymous feedback that helped this work become stronger.
We are thankful to Christian Sandvig for introducing us to Co-Production as a possible lens through
which to see our data. Lastly, the first author was supported by the Research Experience for Master’s
Students (REMS) Fellowship at the University of Michigan School of Information to work with the
last author on the research project that is featured in this paper, and we want to thank REMS for
making this project possible.
REFERENCES
[1] [n.d.]. About TikTok | TikTok. https://www.tiktok.com/about?lang=en
[2] [n.d.]. Community Guidelines. https://www.tiktok.com/community-guidelines?lang=en
[3][n.d.]. How TikTok recommends videos #ForYou - Newsroom | TikTok. https://newsroom.tiktok.com/en-us/how-
tiktok-recommends-videos-for-you/
[4][n.d.]. TikTok ban: The US is ’looking at’ banning Chinese social media apps, Pompeo says - CNN. https://www.cnn.
com/2020/07/07/tech/us-tiktok-ban/index.html
[5][n.d.]. TikTok to launch Transparency Center for moderation and data practices - Newsroom | TikTok. https:
//newsroom.tiktok.com/en-us/tiktok-to-launch-transparency-center-for-moderation-and-data-practices
[6]D. Abrams and M.A. Hogg. 2006. Social Identifications: A Social Psychology of Intergroup Relations and Group Processes .
Taylor Francis. https://books.google.com/books?id=50OV4gqcFA0C
[7]Julia Alexander. 2019. LGBTQ YouTubers are suing YouTube over alleged discrimination. https://www.theverge.com/
2019/8/14/20805283/lgbtq-youtuber-lawsuit-discrimination-alleged-video-recommendations-demonetization
[8]Oscar Alvarado and Annika Waern. 2018. Towards Algorithmic Experience: Initial Efforts for Social Media Contexts.
InProceedings of the 2018 CHI Conf. on Human Factors in Computing Systems - CHI ’18 . ACM Press, Montreal QC,
Canada, 1–12. https://doi.org/10.1145/3173574.3173860
[9]Nazanin Andalibi and Patricia Garcia. 2021. Sensemaking and Coping After Pregnancy Loss: The Seeking and
Disruption of Emotional Validation Online. Proceedings of the ACM Human Computer Interaction (April 2021), 31. Issue
No. CSCW1. https://doi.org/10.1145/3449201
[10] Paul Baker and Amanda Potts. 2013. ‘Why do white people have thin lips?’ Google and the perpetuation of stereotypes
via auto-complete search forms. Critical Discourse Studies 2 (May 2013), 187–204. https://doi.org/10.1080/17405904.
2012.744320 Publisher: Routledge _eprint: https://doi.org/10.1080/17405904.2012.744320.
[11] Ruha Benjamin. 2019. Race after technology: abolitionist tools for the new Jim code . Polity, Medford, MA.
[12] Sam Biddle, Paulo Victor Ribeiro, and Tatiana Dias. 2020. Invisible Censorship: TikTok Told Moderators to Suppress
Posts by “Ugly” People and the Poor to Attract New Users. https://theintercept.com/2020/03/16/tiktok-app-moderators-
users-discrimination/ Library Catalog: The Intercept.
[13] Elena Botella. 2019. TikTok Admits It Suppressed Videos by Disabled, Queer, and Fat Creators. https://slate.com/
technology/2019/12/tiktok-disabled-users-videos-suppressed.html Library Catalog: slate.com.
[14] Engin Bozdag. 2013. Bias in algorithmic filtering and personalization. Ethics and Information Technology 3 (Sept. 2013),
209–227. https://doi.org/10.1007/s10676-013-9321-6
[15] André Brock. 2009. "Who do you think you are?": Race, Representation, and Cultural Rhetorics in Online Spaces. Poroi
1 (July 2009), 15–35. https://doi.org/10.13008/2151-2957.1013
[16] Melissa Brown, Rashawn Ray, Ed Summers, and Neil Fraistat. 2017. #SayHerName: a case study of intersectional social
media activism. Ethnic and Racial Studies 11 (Sept. 2017), 1831–1846. https://doi.org/10.1080/01419870.2017.1334934
Publisher: Routledge _eprint: https://doi.org/10.1080/01419870.2017.1334934.
[17] Jennings Bryant and Peter Vorderer. 2013. Psychology of Entertainment . Routledge. Google-Books-ID: AVnhAQAAQBAJ.
[18] Peter J. Burke. 1980. The Self: Measurement Requirements from an Interactionist Perspective. Social Psychology
Quarterly 1 (March 1980), 18. https://doi.org/10.2307/3033745
[19] Peter J. Burke and Jan E. Stets. 2009. Identity theory . Oxford University Press, Oxford ; NY. OCLC: ocn271647128.
[20] Matthew Carrasco and Andruid Kerne. 2018. Queer Visibility: Supporting LGBT+ Selective Visibility on Social Media.
InProceedings of the 2018 CHI Conf. on Human Factors in Computing Systems . ACM, Montreal QC Canada, 1–12.
https://doi.org/10.1145/3173574.3173824
[21] Michelle Caswell, Marika Cifor, and Mario H. Ramirez. 2016. “To Suddenly Discover Yourself Existing”: Uncovering
the Impact of Community Archives1.The American Archivist 1 (June 2016), 56–81. https://doi.org/10.17723/0360-
9081.79.1.56
[22] John Cheney-Lippold. 2011. A New Algorithmic Identity: Soft Biopolitics and the Modulation of Control. Theory,
Culture & Society 6 (Nov. 2011), 164–181. https://doi.org/10.1177/0263276411424420
[23] John Cheney-Lippold. 2017. We are data: algorithms and the making of our digital selves . NY University Press, NY.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:27
[24] Robin R. Coleman and Emily Chivers Yochim. 2008. Symbolic Annihilation. In The Int’l Encyclopedia of Communication ,
Wolfgang Donsbach (Ed.). John Wiley & Sons, Ltd, Chichester, UK, wbiecs124. https://doi.org/10.1002/9781405186407.
wbiecs124
[25] Kimberle Crenshaw. 1991. Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of
Color. Stanford Law Review 6 (July 1991), 1241. https://doi.org/10.2307/1229039
[26] Michael Ann Devito, Jeremy Birnholtz, Jeffery T. Hancock, Megan French, and Sunny Liu. 2018. How People Form
Folk Theories of Social Media Feeds and What it Means for How We Study Self-Presentation. In Proceedings of
the 2018 CHI Conf. on Human Factors in Computing Systems - CHI ’18 . ACM Press, Montreal QC, Canada, 1–12.
https://doi.org/10.1145/3173574.3173694
[27] Michael Ann Devito, Darren Gergle, and Jeremy Birnholtz. 2017. "Algorithms ruin everything": #RIPTwitter, Folk
Theories, and Resistance to Algorithmic Change in Social Media. In Proceedings of the 2017 CHI Conf. on Human Factors
in Computing Systems . ACM, Denver Colorado USA, 3163–3174. https://doi.org/10.1145/3025453.3025659
[28] Zak Doffman. [n.d.]. Warning—Apple Suddenly Catches TikTok Secretly Spying On Millions Of iPhone
Users. https://www.forbes.com/sites/zakdoffman/2020/06/26/warning-apple-suddenly-catches-tiktok-secretly-
spying-on-millions-of-iphone-users/ Library Catalog: www.forbes.com Section: Innovation.
[29] Bryan Dosono and Bryan Semaan. 2020. Decolonizing Tactics as Collective Resilience: Identity Work of AAPI
Communities on Reddit. Proceedings of the ACM on Human-Computer Interaction CSCW1 (May 2020), 1–20. https:
//doi.org/10.1145/3392881
[30] Motahhare Eslami, Karrie Karahalios, Christian Sandvig, Kristen Vaccaro, Aimee Rickman, Kevin Hamilton, and Alex
Kirlik. 2016. First I "like" it, then I hide it: Folk Theories of Social Feeds. In Proceedings of the 2016 CHI Conf. on Human
Factors in Computing Systems . ACM, San Jose California USA, 2371–2382. https://doi.org/10.1145/2858036.2858494
[31] Motahhare Eslami, Sneha R. Krishna Kumaran, Christian Sandvig, and Karrie Karahalios. 2018. Communicating
Algorithmic Process in Online Behavioral Advertising. In Proceedings of the 2018 CHI Conf. on Human Factors in
Computing Systems - CHI ’18 . ACM Press, Montreal QC, Canada, 1–13. https://doi.org/10.1145/3173574.3174006
[32] Motahhare Eslami, Aimee Rickman, Kristen Vaccaro, Amirhossein Aleyasen, Andy Vuong, Karrie Karahalios, Kevin
Hamilton, and Christian Sandvig. 2015. "I always assumed that I wasn’t really that close to [her]": Reasoning about
Invisible Algorithms in News Feeds. In Proceedings of the 33rd Annual ACM Conf. on Human Factors in Computing
Systems - CHI ’15 . ACM Press, Seoul, Republic of Korea, 153–162. https://doi.org/10.1145/2702123.2702556
[33] Nancy Ettlinger. 2018. Algorithmic affordances for productive resistance. Big Data & Society 1 (Jan. 2018),
2053951718771399. https://doi.org/10.1177/2053951718771399 Publisher: SAGE Publications Ltd.
[34] Ryan J. Gallagher, Elizabeth Stowell, Andrea G. Parker, and Brooke Foucault Welles. 2019. Reclaiming Stigmatized
Narratives: The Networked Disclosure Landscape of #MeToo. Proceedings of the ACM on Human-Computer Interaction
CSCW (Nov. 2019), 96:1–96:30. https://doi.org/10.1145/3359198
[35] Susan A. Gelman and Cristine H. Legare. 2011. Concepts and Folk Theories. Annual Review of Anthropology (2011),
379–398. https://www.jstor.org/stable/41287739 Publisher: Annual Reviews.
[36] Tarleton Gillespie. 2010. The politics of ‘platforms’. New Media & Society 3 (May 2010), 347–364. https://doi.org/10.
1177/1461444809342738 Publisher: SAGE Publications.
[37] Quentin Grossetti, Cédric du Mouza, and Nicolas Travers. 2019. Community-Based Recommendations on Twitter:
Avoiding the Filter Bubble. In Web Information Systems Engineering – WISE 2019 , Reynold Cheng, Nikos Mamoulis,
Yizhou Sun, and Xin Huang (Eds.). Springer Int’l Publishing, Cham, 212–227. https://doi.org/10.1007/978-3-030-34223-
4_14 Series Title: Lecture Notes in Computer Science.
[38] Mario Haim, Andreas Graefe, and Hans-Bernd Brosius. 2018. Burst of the Filter Bubble?: Effects of personalization on the
diversity of Google News .Digital Journalism 3 (March 2018), 330–343. https://doi.org/10.1080/21670811.2017.1338145
[39] Oliver Haimson. 2018. Social Media as Social Transition Machinery. Proceedings of the ACM on Human-Computer
Interaction CSCW (Nov. 2018), 63:1–63:21. https://doi.org/10.1145/3274332
[40] Monique M. Hennink and Bonnie N. Kaiser. 2020. Saturation in Qualitative Research. In SAGE Research Methods
Foundations . SAGE Publications Ltd, 1 Oliver’s Yard, 55 City Road, London EC1Y 1SP United Kingdom. https:
//doi.org/10.4135/9781526421036822322
[41] M.A. Hogg. 2006. Social Identity Theory. In Contemporary Social Psychological Theories , Peter J. Burke (Ed.). Stanford
University Press, Palo Alto, CA, 111–136. https://books.google.com/books?id=8Jzkgbq2vYwC
[42] Nina Huntemann. 2015. No More Excuses: Using Twitter to Challenge The Symbolic Annihilation of Women in Games.
Feminist Media Studies 1 (Jan. 2015), 164–167. https://doi.org/10.1080/14680777.2015.987432
[43] Instagram. 2020. Introducing Instagram Reels. https://about.instagram.com/blog/announcements/introducing-
instagram-reels-announcement
[44] Sheila Jasanoff. 2004. Afterword. In States of Knowledge (1 ed.). Routledge, 274–282.
[45] John C. Turner , Michael A. Hogg , Penelope J. Oakes , Stephen D. Reicher , Margaret S. Wetherell. 1989. Rediscovering the
Social Group: A Self-Categorization Theory. Amer. J. Sociology 6 (May 1989), 1514–1516. https://doi.org/10.1086/229205
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:28 Nadia Karizat et al.
[46] Willett Kempton. 1986. Two Theories of Home Heat Control*. Cognitive Science 1 (1986), 75–90. https://doi.org/10.
1207/s15516709cog1001_3 _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1001_3.
[47] Donald Ervin Knuth. 1997. The art of computer programming (3rd ed ed.). Addison-Wesley, Reading, Mass.
[48] Hanlin Li, Disha Bora, Sagar Salvi, and Erin Brady. 2018. Slacktivists or Activists?: Identity Work in the Virtual
Disability March. In Proceedings of the 2018 CHI Conf. on Human Factors in Computing Systems . ACM, Montreal QC
Canada, 1–13. https://doi.org/10.1145/3173574.3173799
[49] Fannie Liu, Denae Ford, Chris Parnin, and Laura Dabbish. 2017. Selfies as Social Movements: Influences on Participation
and Perceived Impact on Stereotypes. Proceedings of the ACM on Human-Computer Interaction CSCW (Dec. 2017),
72:1–72:21. https://doi.org/10.1145/3134707
[50] Sorcha Avalon Mackenzie and David Nichols. 2020. Finding ‘Places to Be Bad’ in Social Media: The Case of TikTok. In
Urban Australia and Post-Punk: Exploring Dogs in Space , David Nichols and Sophie Perillo (Eds.). Springer, Singapore,
285–298. https://doi.org/10.1007/978-981-32-9702-9_22
[51] Alice E. Marwick and danah boyd. 2011. I tweet honestly, I tweet passionately: Twitter users, context collapse, and the
imagined audience. New Media & Society 1 (Feb. 2011), 114–133. https://doi.org/10.1177/1461444810365313
[52] George J. McCall and J. L. Simmons. 1966. Identities and Interactions . Free Press, NY, NY, US. Pages: ix, 278.
[53] Sarah McRoberts, Elizabeth Bonsignore, Tamara Peyton, and Svetlana Yarosh. 2016. Do It for the Viewers!: Audience
Engagement Behaviors of Young YouTubers. In Proceedings of the The 15th Int’l Conf. on Interaction Design and Children
- IDC ’16 . ACM Press, Manchester, United Kingdom, 334–343. https://doi.org/10.1145/2930674.2930676
[54] Sarah McRoberts, Haiwei Ma, Andrew Hall, and Svetlana Yarosh. 2017. Share First, Save Later: Performance of Self
through Snapchat Stories. In Proceedings of the 2017 CHI Conf. on Human Factors in Computing Systems . ACM, Denver
Colorado USA, 6902–6911. https://doi.org/10.1145/3025453.3025771
[55] Debra Merskin. 1998. Sending up Signals: A Survey of Native American1 Media Use and Representation in the Mass
Media. Howard Journal of Communications 4 (Oct. 1998), 333–345. https://doi.org/10.1080/106461798246943
[56] Sadia Mir and Christina Paschyn. 2018. Qatar’s Hidden Women: Symbolic Annihilation and Documentary Media
Practice. Visual Communication Quarterly 2 (April 2018), 93–105. https://doi.org/10.1080/15551393.2018.1456932
[57] Safiya Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism . NY University Press, NY.
[58] Ihudiya Finda Ogbonnaya-Ogburu, Angela D.R. Smith, Alexandra To, and Kentaro Toyama. 2020. Critical Race Theory
for HCI. In Proceedings of the 2020 CHI Conf. on Human Factors in Computing Systems . ACM, Honolulu HI USA, 1–16.
https://doi.org/10.1145/3313831.3376392
[59] Bahiyah Omar and Wang Dequan. 2020. Watch, Share or Create: The Influence of Personality Traits and User
Motivation on TikTok Mobile Video Usage. Int’l Journal of Interactive Mobile Technologies 04 (March 2020), 121–137.
https://doi.org/10.3991/ijim.v14i04.12429 Publisher: Universität Kassel.
[60] Jahna Otterbacher, Jo Bates, and Paul Clough. 2017. Competent Men and Warm Women: Gender Stereotypes and
Backlash in Image Search Results. In Proceedings of the 2017 CHI Conf. on Human Factors in Computing Systems . ACM,
Denver Colorado USA, 6620–6631. https://doi.org/10.1145/3025453.3025727
[61] Daphna Oyserman, Kristen Elmore, and George Smith. 2012. Self, self-concept, and identity. In Handbook of self and
identity, 2nd ed. The Guilford Press, NY, NY, US, 69–104.
[62] Eli Pariser. 2012. The filter bubble: how the new personalized web is changing what we read and how we think . OCLC:
1120489057.
[63] Sarah Perez. 2020. TikTok to open a ‘Transparency Center’ where outside experts can examine its moderation practices
– TechCrunch. http://search.proquest.com/docview/2376021221?pq-origsite=summon& Place: NY, United States, NY
Publisher: AOL Inc.
[64] Emilee Rader and Rebecca Gray. 2015. Understanding User Beliefs About Algorithmic Curation in the Facebook News
Feed. In Proceedings of the 33rd Annual ACM Conf. on Human Factors in Computing Systems - CHI ’15 . ACM Press,
Seoul, Republic of Korea, 173–182. https://doi.org/10.1145/2702123.2702174
[65] Rashawn Ray, Melissa Brown, Neil Fraistat, and Edward Summers. 2017. Ferguson and the death of Michael
Brown on Twitter: #BlackLivesMatter, #TCOT, and the evolution of collective identities. Ethnic and Racial
Studies 11 (Sept. 2017), 1797–1813. https://doi.org/10.1080/01419870.2017.1335422 Publisher: Routledge _eprint:
https://doi.org/10.1080/01419870.2017.1335422.
[66] Reuters. 2020. TikTok: China’s ByteDance agrees to divest US operations after Trump threat. http://www.theguardian.
com/technology/2020/aug/01/tiktok-ban-china-bytedance-divest-microsoft Library Catalog: www.theguardian.com
Section: Technology.
[67] Adi Robertson. 2019. TikTok prevented disabled users’ videos from showing up in feeds. https://www.theverge.com/
2019/12/2/20991843/tiktok-bytedance-platform-disabled-autism-lgbt-fat-user-algorithm-reach-limit Library Catalog:
www.theverge.com.
[68] Aja Romano. 2019. A group of YouTubers is claiming the site systematically demonetizes queer content. https://www.
vox.com/culture/2019/10/10/20893258/youtube-lgbtq-censorship-demonetization-nerd-city-algorithm-report
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:29
[69] Christian Sandvig, Kevin Hamilton, Karrie Karahalios, and Cédric Langbort. 2014. Auditing Algorithms : Research
Methods for Detecting Discrimination on Internet Platforms.
[70] Michael Schuman. [n.d.]. Why America Is Afraid of TikTok - The Atlantic. https://www.theatlantic.com/Int’l/archive/
2020/07/tiktok-ban-china-america/614725/?s=09
[71] Donghee Shin, Bu Zhong, and Frank A. Biocca. 2020. Beyond user experience: What constitutes algorithmic experiences?
Int’l Journal of Information Management (June 2020), 102061. https://doi.org/10.1016/j.ijinfomgt.2019.102061
[72] Ellen Simpson and Bryan Semaan. Forthcoming. For You, or For "You"?: Everyday LGBTQ+ Encounters with TikTok.
Proceedings of the ACM on Human-Computer Interaction CSCW (Forthcoming), 1–33.
[73] Anselm L. Strauss and Juliet M. Corbin. 1998. Basics of qualitative research: techniques and procedures for developing
grounded theory (2nd ed ed.). Sage Publications, Thousand Oaks.
[74] Sheldon Stryker. 1980. Symbolic interactionism: a social structural version . Benjamin/Cummings Pub. Co, Menlo Park,
Calif.
[75] Benjamin Toff and Rasmus Kleis Nielsen. 2018. “I Just Google It”: Folk Theories of Distributed Discovery. Journal of
Communication 3 (June 2018), 636–657. https://doi.org/10.1093/joc/jqy009
[76] Gaye Tuchman. 2000. The Symbolic Annihilation of Women by the Mass Media. In Culture and Politics , Lane Crothers
and Charles Lockhart (Eds.). Palgrave Macmillan US, NY, 150–174. https://doi.org/10.1007/978-1-349-62397-6_9
[77] Julia Velkova and Anne Kaun. 2019. Algorithmic resistance: media practices and the politics of repair. Information,
Communication & Society (Aug. 2019), 1–18. https://doi.org/10.1080/1369118X.2019.1657162
[78] Paul Venzo and Kristy Hess. 2013. “Honk Against Homophobia”: Rethinking Relations Between Media and Sexual
Minorities. Journal of Homosexuality 11 (Nov. 2013), 1539–1556. https://doi.org/10.1080/00918369.2013.824318
[79] Ashley Marie Walker and Michael A. DeVito. 2020. "’More gay’ fits in better": Intracommunity Power Dynamics and
Harms in Online LGBTQ+ Spaces. In Proceedings of the 2020 CHI Conf. on Human Factors in Computing Systems . ACM,
Honolulu HI USA, 1–15. https://doi.org/10.1145/3313831.3376497
[80] Shuaishuai Wang. 2020. Calculating dating goals: data gaming and algorithmic sociality on Blued, a Chinese gay dating
app. Information, Communication & Society 2 (Jan. 2020), 181–197. https://doi.org/10.1080/1369118X.2018.1490796
Publisher: Routledge _eprint: https://doi.org/10.1080/1369118X.2018.1490796.
[81] Yunwen Wang. 2020. Humor and camera view on mobile short-form video apps influence user experience and
technology-adoption intent, an example of TikTok (DouYin). Computers in Human Behavior (Sept. 2020), 106373.
https://doi.org/10.1016/j.chb.2020.106373
[82] Michele Willson. 2017. Algorithms (and the) everyday. Information, Communication & Society 1 (Jan. 2017), 137–150.
https://doi.org/10.1080/1369118X.2016.1200645
[83] Svetlana Yarosh, Elizabeth Bonsignore, Sarah McRoberts, and Tamara Peyton. 2016. YouthTube: Youth Video Authorship
on YouTube and Vine. In Proceedings of the 19th ACM Conf. on Computer-Supported Cooperative Work & Social Computing
- CSCW ’16 . ACM Press, San Francisco, California, USA, 1421–1435. https://doi.org/10.1145/2818048.2819961
[84] Lei Zhang, Feng Wang, and Jiangchuan Liu. [n.d.]. Understand Instant Video Clip Sharing on Mobile Platforms:
Twitter’s Vine as a Case Study. ([n. d.]), 6.
[85] Chengyan Zhu, Xiaolin Xu, Wei Zhang, Jianmin Chen, and Richard Evans. 2019. How Health Communication via
Tik Tok Makes a Difference: A Content Analysis of Tik Tok Accounts Run by Chinese Provincial Health Committees.
Int’l journal of environmental research and public health 1 (2019), 192. https://doi.org/10.3390/ijerph17010192 Place:
Switzerland Publisher: MDPI AG.
[86] Frederik J. Zuiderveen Borgesius, Damian Trilling, Judith Möller, Balázs Bodó, Claes H. de Vreese, and Natali Helberger.
2016. Should we worry about filter bubbles? Internet Policy Review 1 (March 2016). https://doi.org/10.14763/2016.1.401
[87] Diana Zulli and David James Zulli. 2020. Extending the Internet meme: Conceptualizing technological mimesis and
imitation publics on the TikTok platform. New Media & Society (Dec. 2020), 1461444820983603. https://doi.org/10.
1177/1461444820983603 Publisher: SAGE Publications.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:30 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:31
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:32 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:33
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:34 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:35
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:36 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:37
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:38 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:39
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:40 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:41
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.305:42 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Algorithmic Folk Theories and Identity on TikTok 305:43
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.Received October 2020; revised April 2021; accepted May 2021305:44 Nadia Karizat et al.
Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW2, Article 305. Publication date: October 2021.