Information-Theoretic Limitations of Formal Systems 
GREGORY J. CHAITIN 
Buenos Aires, Argentina 
ABSTRACT. An attempt is made to apply information-theoretic computational complexity to meta- 
mathematics. The paper studies the number of bits of instructions that must be given to a computer 
for it to perform finite and infinite tasks, and Mso the time it takes the computer to perform these 
tasks. This is applied to measuring the difficulty of proving a given set of theorems, in terms of the 
number of bits of axioms that are assumed, and the size of the proofs needed to deduce the theorems 
from the axioms. 
KEY WORDS AND PHRASES: complexity of sets, computational complexity, difficulty of theorem- 
proving, entropy of sets, formal systems, GSdel's incompleteness theorem, halting problem, informa- 
tion content of sets, information content of axioms, information theory, information time trade-offs, 
metamathematics, random strings, recursive functions, recursively enumerable sets, size of proofs, 
universal computers 
CR CATEGORIES: 5.21, 5.25, 5.27, 5.6 
1. Introduction 
This paper attempts to study information-theoretic aspects of computation in a very gen- 
eral setting. It is concerned with the information that must be supplied to a computer for 
it to carry out finite or infinite computational tasks, and also with the time it takes the 
computer to do this. These questions, which have come to be grouped under the heading 
of abstract computational complexity, are considered to be of interest in themselves. 
However, the motivation for this investigation is primarily its metamathematical applica- 
tions. 
Computational complexity differs from recursive function theory in that, instead of just 
asking whether it is possible to compute something, one asks exactly how much effort is 
needed to do this. Similarly, instead of the usual metamathematical approach, we propose 
to measure the difficulty of proving something. How many bits of axioms are needed to 
be able to obtain a set of theorems? How long are the proofs needed to demonstrate them? 
What is the trade-off between how much is assumed and the size of the proofs ? 
We consider the axioms of a formal system to be a program for listing the set of 
theorems, and the time at which a theorem is written out to be the length of its proof. 
We believe that this approach to metamathematics may yield valuable dividends. 
Mathematicians were at first greatly shocked at, and then ignored almost completely, 
Gfdel's announcement that no set of axioms for number theory is complete. It wasn't 
clear what, in practice, was the significance of G5del's theorem, how it should affect the 
Copyright Â© 1974, Association for Computing Machinery, Inc. General permission to republish, 
but not for profit, all or part of this material is granted provided that ACM's copyright notice is 
given and that reference is made to the publication, to its date of issue, and to the fact that reprinting 
privileges were granted by permission of the Association for Computing Machinery. 
An early version of this paper was presented at the Courant Institute Computational Complexity 
Symposium, New York, October 1971. [28] includes a nontechnical exposition of some results of this 
paper. [1] and [2] announce related results. 
Author's address: Rivadavia 3580, Depto. 10A, Buenos Aires, Argentina. 
Journal of the Association for Computing Machinery, Vol, 21, No. 3, July 1974, pp. 403--424.404 GREGORY J. CHAITIN 
everyday activities of mathematicians. Perhaps this was because thel unprovable proposi- 
tions appeared to be very pathological singular points. 1' 2 
The approach of this paper, in contrast, is to measure the power of a set of axioms, to 
measure the information that it contains. We shall see that there are circumstances in 
which one only gets out of a set of axioms what one puts in, and in which it is possible to 
reason in the following manner. If a set of theorems constitutes t bits of information, and a 
set of axioms contains less than t bits of information, then it is impossible to deduce these 
theorems from these axioms. 
We consider that this paper is only a first step in the direction of such an approach to 
metamathematics ~; a great deal of work remains to be done to clarify these matters. Never- 
theless, we would like to sketch here the conclusions which we have tentatively drawn. 4 
After empirically exploring, in the tradition of Euler and Gauss, the properties of the 
natural numbers one may discover interesting regularities. One then has two options. The 
first is to accept the conjectures one has formulated on the basis of their empirical cor- 
roboration, as an experimental scientist might do. In this way one may have a great many 
laws to remember, but will not have to bother to deduce them from other principles. The 
other option is to try to find a theory for one's observations, or to see if they follow from 
existing theory. In this case it may be possible to reduce a great many observations into a 
few general principles from which they can be deduced. But there is a cost: one can now 
only arrive at the regularities one observed by means of long demonstrations. 
Why use formal systems, instead of proceeding empirically? First of all, if the empiri- 
cally derived conjectures aren't independent facts, reducing them to a few common princi- 
ples allows one to have to remember less assumptions, and this is easier to do, and is much 
safer, as one is assuming less. The cost is, of course, the size of the proofs. 
What attitude, then, does this suggest toward G6del's theorem that any formalization 
of number theory is incomplete? It tends to provide theoretical justification for the atti- 
tude that number theorists have in fact adopted when they extensively utilize in their 
work hypotheses such as that of Riemann concerning the zeta function. GSdcPs theorem 
does not mean that mathematicians must give up hope of understanding the properties of 
the natural numbers; it merely means that one may have to adopt new axioms as one 
seeks to order and interrelate, to organize and comprehend, ever more extensive mathe- 
matical observations. I.e. the mathematician shouldn't be more upset than the physicist 
when he needs to assume a new axiom; nor should he be too horrified when an axiom must 
be abandoned because it is found that it contradicts previously existing theory, or because 
it predicts properties of the natural numbers that are not corroborated empirically. In a 
word, we propose that there may be theoretical justification for regarding number theory. 
somewhat more like a dynamic empirical science than as a closed static body of theory. 
This paper grew out of work on the concept of an individual random, patternless, 
chaotic, unpredictable string of bits. This concept has been rigorously defined in several 
ways, and the properties of these random strings have been studied by several authors 
(see, for example, [20-28]). Most strings are random; they have no special distinguishing 
features; they are typical and hard to tell apart. But can it be proved that a particular 
In [3] and [4] von Neumann analyzes the effect of G6del's theorem upon mathematicians. Weyl's 
reaction to GSdel's theorem is quoted by Bell [5]. The original source is [6]. See also Weyl's discussion 
[7] of GSdel's views regarding his incompleteness theorem. 
2 For nontechnical expositions of G6del's incompleteness theorem, see [8, 9, 10, Sec. 1, pp. xv-xviii, 
11, and 12]. [28] contains a nontechnical exposition of an incompleteness theorem analogous to Berry's 
paradox that is Theorem 4.1 of this paper. 
s [13-16] are related in approach to this paper. [13, 15, and 16] are concerned with measuring the size 
of proofs and the effect of varying the axioms upon their size. In [14] Cohen "measures the strength 
of a [formal] system by the ordinals which can be handled in the system." 
The analysis that follows of the possible significance of the results of this paper has been influenced 
by [17 and 18], in addition to the references cited in Footnote 1. Incidentally, it is interesting to 
examine [19, p. 112] in the light of this analysis.Information-Theoretic Limitations of Formal Systems 405 
string is random? The answer is that about n bits of axioms are needed to be able to prove 
that a particular n-bit string is random. 
More precisely, the train of thought was as follows. The entropy, or information con- 
tent, or complexity, of a string is defined to be the number of bits needed to specify it so 
effectively that it can be constructed. A random n-bit string is about n bits of information, 
i.e. has complexity/entropy/information content .~n; there is essentially nothing better 
to do if one wishes to specify such a string than just show it directly. But the string con- 
sisting of 1,000,000 repetitions of the 6-bit pattern 000101 has far less than 6,000,000 bits 
of complexity. We have just specified it using far fewer bits. 
What if one wishes to be able to determine each string of complexity <n and its com- 
plexity? It turns out that this requires n + 0 (1) bits of axioms; at least n - c bits are 
necessary (Theorem 4.1 ), and n + c bits are sufficient (Theorem 4.3 ). But the proofs will 
be enormously long unless one essentially directly takes as axioms all the theorems that 
one wishes to prove, and in that.case there will be an enormously great number of bits of 
axioms (Theorem 7.6 (c)). 
Another theme of this paper arises from the following metamathematical considerations, 
which are well known (see, for example, [29]). In a formal system without a decision 
method, it is impossible to bound the size of a proof of a theorem by a reeursive function 
of the number of characters in the statement of the theorem. For if there were such a func- 
tion f, one could decide whether or not an arbitrary proposition p is a theorem, by merely 
checking if a proof for it appears among the finitely many possible proofs of size bounded 
by f of the number of characters in p. 
Thus, in a formal system having no decision method, there are very profound theorems, 
theorems that have short statements, but need immensely long proofs. In Section 10 we 
study the function e (n), necessarily nonrecursive, defined to be the least s such that all 
theorems of the formal system with < n characters have proofs of size < s. 
To close this introduction, we would like to mention without proof an example that 
shows particularly clearly the relationship between the number of bits of axioms that are 
assumed and what can be deduced. This example is based on the work of M. Davis, Ju. V. 
Matijasevi5, H. Putnam, and J. Robinson that settled Hilbert's tenth problem (cf. [30]). 
There is a polynomial P in k + 2 variables with integer coefficients that has the following 
property. Consider the infinite binary string whose ith bit is I or 0 depending on whether 
or not the set S~ = {n Â£ N{3xl, ... ,xk E N P(i,n, xl, ... ,x~) = 0} is infinite. 
Here N denotes the natural numbers. This infinite binary string is random, i.e. the com- 
plexity of an initial segment is asymptotic to its length. What is the number of bits of 
axioms that is needed to be able to prove for each natural number i < n whether or not 
the set S~ is infinite ? By using the methods of Section 4, it is easy to see that the number 
of bits of axioms that is needed is asymptotic to n. 
2. Definitions Related to Computers and Complexity 
This paper is concerned with measuring the difficulty of computing finite and infinite sets 
of binary strings. The binary strings are considered to be ordered in the following fashion: 
h, O, 1, 00, 01, 10, 11,000, 001,010, 011, 100, 101,110, 111, 0000, â¢ â¢ .. Inorder to be able 
to also study the difficulty of computing finite or infinite sets of natural numbers, we con- 
sider each binary string to simultaneously be a natural number: the nth binary string cor- 
responds to the natural number n. Ordinal numbers are considered to start with 0, not 1. 
For example, we speak of the 0th string of length n. 
In order to be able to study the difficulty of computing finite and.infinite sets of mathe- 
matical propositions, we also consider that each binary string is simultaneously a proposi- 
tion. Propositions use a finite alphabet of characters which we suppose includes all the 
usual mathematical symbols. We consider the nth binary string to correspond to the nth 
proposition, where the propositions are in lexicographical order defined by an arbitrary 
ordering of the symbols of their alphabet.â¢ 406 GREGORY J. CHAITIN 
Henceforth, we say "string" instead of "binary string," it being understood that this 
refers to a binary string. It should be clear from the context whether we are considering 
something to be a string, a natural number, or a proposition. 
Operations with strings include exponentiation: 0 k and 1 k denote the string of k O's and 
k l's, respectively. Ig (s) denotes the length of a string s. Note that the length lg (n) of a 
natural number n is therefore [.logs (n ~ 1 ).J. The maximum element of a finite set of 
strings S is denoted by max S, and we stipulate that max ~2~ = 0. # (S) denotes the num- 
ber of elements in a finite set S. 
We use these notational conventions in a somewhat tricky way to indicate how to com- 
pactly code several pieces of information into a single string. Two coding techniques are 
used. 
(a) Consider two natural numbers n and k such that 0 < k < 2". We code n and k into 
the string s = 0" --[- k, i.e. the kth string of length n. Given the string s, one recovers n 
and k as follows: n = lg(s), k = s - 0 ~g('). This technique is used in the proofs of Theo- 
rems 4.3, 6.1, 7.4, and 10.1. In three of these proofs k is #(S), where S is a subset of the 
strings having length < n; n and #(S) are coded into the string s = 0" ~ #(S). In the 
case of Theorem 6.1, k is the number that corresponds to a string s of length < n (thus 
0 <: k< 2n- 1);nandsarecodedintothestrings' = 0 n~ s. 
(b) Consider a string p and a natural number k. We code p and k into the string s = 
Ol~k)lkp, i.e. the string consisting of lg (k) O's followed by a 1 followed by the kth string 
followed by the string p. The length of the initial run of O's is the same as the length of the 
kth string and is used to separate kp in two and recover k and p from s. Note that lg(s) = 
lg(p) W 21g(k) W 1. This technique is used in the proof of Theorem 10.4. The proof of 
Theorem 4.1 uses a simpler technique: p and k are coded into the string s = 0klp. But 
this coding is less economical, for lg(s) = lg(p) ~ k -Jr- 1. 
We use an extremely general definition of computer; this has the advantage that if one 
call show that something is difficult to compute using any such computer, this will be a 
very strong result. A computer is defined by indicating whether it has halted and what it 
has output, as a function of its program and the time. The formal definition of a computer 
C is an ordered pair (C, He) consisting of two total recursive functions C : X* X N --~ 
{S E 2 2* l S is finite} and He : X* X N ~ X. HereX = 10,1}, X* is the set of all 
strings, and N is the set of all natural numbers. It is assumed that the functions C and Hc 
have the following two properties: (a) C (p, t) c C (p, t ~ 1 ), and (b) if Hc (p, t) = 1, 
thenHc(p,t + 1) = l andC(p,t) = C(p,t "-k 1). 
C (p, t) is the finite set of strings output by the computer C up to time t when its pro- 
gram is p. If He(p, t) = 1 the computer C is halted at time t when its program is p. If 
He(p, t) = 0 the computer C isn't halted at time t when its program is p. Henceforth 
whether He(p, t) = 1 or 0 will be indicated by stating that "C(p, t) is halted" or that 
"C(p, t) isn't halted." Property (a) states that C(p, t) is the cumulative output, and 
property (b) states that a computer that is halted remains halted and never outputs any- 
thing else. 
C (p), the output of the computation that C performs when it is given the program p, 
is defined to be Ut C(p, t). It is said that "C(p) halts" iff there is a t such that C(p, t) is 
halted. Furthermore, if C (p) halts, the time at which it halts is defined to be the least t 
such that C (p, t) is halted. We say that the program p calculates the finite set S when run 
on C if C (p) = S and halts. We say that the program p enumerates the finite or infinite 
set S when run on C if C (p ) = S. 
We now define a class of computers that are especially suitable to use for measuring the 
information needed to specify a computation. A computer U is said to be universal if it 
has the following property. For any computer C, there must bea natural number, denoted 
sim (C) (the cost of simulating C), such that the following holds. For any program p, there 
exists a program p~ such that: lg(p') < lg(p) "t- sire(C), U(p') halts iff C(p) halts, 
and U(p~) = C(p).Information-Theoretic Limitations of Formal Systems 407 
The idea, of this definition is as follows. The universal computers are information-theo- 
retically the most economical ones; their programs are shortest. More precisely, a univer- 
sal computer U is able to simulate any other computer, and the program p' for U that 
simulates the program p for C need not be much longer than p. If there are instructions 
of length n for computing something using the computer C, then there are instructions of 
length < n -4- sim(C) for carrying out the same computation using U; i.e. at most 
sim (C) bits must be added to the length of the instructions, to indicate the computer that 
is to be simulated. Note that we do not assume that there is an effective procedure for ob- 
taining p' given C and p. We have no need for the concept of an effectively universal com- 
puter in this paper. Nevertheless, the most natural examples of universal computers are 
effectively universal. See the Appendix for examples of universal computers. 
We shall suppose that a particular universal computer U has somehow been chosen, and 
shall use it as our standard computer for measuring the information needed to specify a 
computation. The choice of U corresponds to the choice of the standard of measurement. 
We now define I (S), the information needed to calculate the finite set S, and I~(S), the 
information needed to enumerate the finite or infinite set S. 
I(S) = minlg(p)(U(p) = S and halts); Is(S) = ~?nlg(p)(U(p) S), 
( if there are no such p. 
We say that I (S) is the complexity of the finite set S, and that I~ (S) is the e-complexity 
(enumeration complexity) of the finite or infinite set S. Note that I (S) is the number of 
bits in the shortest program for U that calculates S, and Is (S) is the number of bits in the 
shortest program for U that enumerates S. Also, Ie (S), the e-complexityof a set S, is ~ if 
S isn't r.e. (recursively enumerable). 
We say that a program p such that U (p) = S and halts is a description of S, and a 
program p s uchthat U (p) = S is an e-description (enumeration description) of S. More- 
over, if U(y) = S and halts and lg(p) = I(S), then we say that p is a minimal de- 
scription of S. Likewise, if U(p) -- S and lg(p) = Is(S), then we say that p is a mini- 
mal e-description of S. 
Finally, ue define Ie (f), the e-complexity of a partial function f. This is defined to be 
the e-complexity of the graph of/, i.e. the set of all ordered pairs of the form (n, f(n)). 
Here the ordered pair (i,j) is defined to be the natural number (2i + 1)2 j - 1; this is 
an effective 1-1 correspondence between the ordered pairs of natural numbers and the nat- 
ural numbers. Note that I~ (.f) = oo if f isn't partial recursive. 
Before considering basic properties of these concepts, we introduce an abbreviated nota- 
tion. Instead of 1 ({s}) and Ie({s} ) we shall write I(s) and Is(s); i.e. the complexity or 
e-complexity of a string is defined to be the complexity or e-complexity of its singleton set. 
We now present basic properties of these concepts. First of all, note that there are pre- n 3t 
cisely 2 pr%rams of length n, and 2 ~+~ - 1 programs of length < n. It follows that the 
number of different sets of complexity n and the number of different sets of e-complexity 
n are both < 2 ~. Also, the number of different sets of complexity _< n and the number of 
different sets of e-complexity < n are both <2 ~+~ - 1 ; i.e. the number of different ob- 
jects of complexity or e-complexity < n is bounded by the number of different descrip- 
tions or e-descriptions of length _< n, which is 2 ~+~ - 1. Thus it might be said that al- 
most all sets are arbitrarily complex. 
It is immediate from the definition of complexity and of a universal computer that 
I~(C(p)) < lg(p) "4- sim(C), andI(C(p)) < lg(p) "-k sim(C) ifC(p) halts. Thisis 
used often, and without explicit mention. The following theorem lists for reference other 
basic properties of complexity and e-complexity that are used in this paper. 
THEOREY[ 2.1. (a) Thereisacsuchthatforall stringss, I(s) < lg(s) "4- c. (b) There 
is a c such tha~ for aU finite sets S, I ( S ) < max S + c. ( e ) For any computer C, there is a 
c such that for all programs p, I~(C(p)) < I(p) + c, and I(C(p)) < I(p) q- c if C(p) 
halts.408 (~REGORY J. CHAITIN 
PROOF. (a) There is a computer C such that C (s) = {s} and halts for all programs s. 
Thus I(s) < lg(s) -t- sim(C). 
(b) There is a computer C such that C (p) halts for all programs p, and n E C (p) iff 
n < lg (p) and the nth bit of p is a 1. Thus I (S) < max S + 1 Pc sim (C). 
(c) There is a computer C' that does the following when it is given I the program s. First 
C' simulates running s on U, i.e. it simulates U(s). If and when U(s) halts, C r has deter- 
mined the set calculated by U when it is given the program s. If this isn't a singleton set, 
C' halts. If it is a singleton set {p}, C' then simulates running p on C. As C' determines the 
strings output by C (p), it also outputs them. And C' halts if C halts during the simulated 
run. 
In summary, C' (s) = C (p) and halts iff C (p) does, if s is a description of the program 
p. Thus, if s is a minimal description of the string p, then 
II, (C (p)) = I, (C' (s)) < lg (s) + sim (C') = I (p) + sim (C'), and 
I(C(p)) I(C'(s)) Slg(s) -b sim(C') = I(p) + sim(C') if C(p) halts. Q.E.D. 
It follows from Theorem 2.1(a) that all strings of length n have complexity < n -Jr c. 
In conjunction with the fact that < 2 ~-~ strings are of complexity < n - k, this shows 
that the great majority of the strings of length n are of complexity ~n. These are the ran- 
dom strings of length n. By taking C = U in Theorem 2.1(c), it follows that there is 
a c such that for any minimal description p, I (p) -k c > I (U (p)) = lg (p). Thus min- 
imal descriptions are highly random strings. Likewise, minimal e-descriptions are highly 
random. This corresponds in information theory to the fact that the most informative 
messages are the most unexpected ones, the ones with least regularities and redundancies, 
and appear to be noise, not meaningful messages. 
3. Definitions Related to Formal Systems 
This paper deals with the information and time needed to carry out computations. How- 
ever, we wish to apply these results to formal systems. This section explains how this is 
done. 
The abstract definition used by Post that a formal system is an r.e. set of propositions 
is close to the viewpoint of this paper (see [31] ).6 However, we are not quite this uncon- 
cerned with the internal details of formal systems. 
The historical motivation for formal systems was of course to construct deductive the- 
ories with completely objective, formal criteria for the validity of a demonstration. Thus, 
a fundamental characteristic of a formal system is an algorithm for checking the validity 
of proofs. From the existence of this proof verification algorithm, it follows that the set 
of all theorems that can be deduced from the axioms p by means Of the rules of inference 
by proofs ~ t characters in length is given by a total recursive function C of p and t. To 
calculate C (p, t) one applies the proof verification algorithm to each of the finitely many 
possible demonstrations having < t characters. 
These considerations motivate the following definition. The rules of inference of a class 
of formal systems is a total recursive function C :X* X N -~ [S E 2 x* I S is finite} 
with the property that C (p, t) c C (p, t + 1 ). The value of C (p, t) is the finite (pos- 
sibly empty) set of the theorems that can be proven from the axioms p by means of 
proofs _~t in size. Here p is a string and t is a natural number. C(p) = U, C(p, t) is 
tile set of theorems that are consequences of the axioms p. The ordered pair (C, p), which 
iraplies both the choice of rules of inference and axioms, is a particular formal system. 
Note that this definition is the same as the definition of a computer with the notion of 
"halting" omitted. Thus given any rules of inference, there is a computer that never halts 
whose output up to time t consists precisely of those propositions that can be deduced by 
proofs of size < t from the axioms the computer is given as its program. And given any 
computer, there are rules of inference such that the set of theorems that can be deduced 
For standard definitions of formal systems, see, for example, [32-34] and [10, p. 117].Information-Theoretic Limitations of Formal Systems 409 
by proofs of size _<t from the program, is precisely the set of strings output by the com- 
puter up to time t. For this reason we consider the following notions to be synonymous: 
"computer" and "rules of inference," "program" and "axioms," and "output up to time 
t" and "theorems with proofs of size < t." 
The rules of inference that correspond to the universal computer U are especially in- 
teresting, because they permit axioms to be very economical. When using the rules of in- 
ference U, the number of bits of axioms needed to deduce a given set of propositions is 
precisely the e-complexity of the set of propositions. If n bits of axioms are needed to ob- 
tain a set T of theorems using the rules of inference U, then at least n-sim(C) bits of 
axioms are needed to obtain them using rules of inference C; i.e. if C(a) -- T, then 
lg(a) > Ie(T) - sim(C). Thus it could be said that U is among the rules of inference 
that permit axioms to be most economical. In Section 4 we are interested exclusively in 
the number of bits needed to deduce certain sets of propositions, not in the size of the 
proofs. We shall therefore only consider the rules of inference U in Section 4, i.e. formal 
systems of the form (U, p). 
As a final comment concerning the rules of inference U, we would like to point out the 
interesting fact that if these rules of inference are used, then a minimal set of axioms for 
obtaining a given set of theorems must necessarily be random. This is just another way of 
saying that a minimal e-description is a highly random string, which was mentioned at the 
end of Section 2. 
The following theorem also plays a role in the interpretation of our results in terms of 
formal systems. 
THEOREM 3.1. Let f be a recursive function, and g be a recursive predicate. (a ) Let C be 
a computer. There is a computer C' that never halts such that C' (p, t) = {f(s) I s E C(p, t) & 
g(s)} forallpandt. (b) ThereisacsuchthatI,({f(s) l s E S &g(s)} ) _< I~(S) + cfor 
all r.e. sets S. 
PROOF. (a) is immediate; (b) follows by taking C = U in part (a). Q.E.D. 
The following is an example of the use of Theorem 3.1. Suppose we wish to study the 
size of the proofs that "n E H" in a formal system (C, p), where n is a numeral for a 
natural number. If we have a result concerning the speed with which any computer can 
enumerate the set H, we apply this result to the computer C' that has the property that 
n E C' (p, t) iff "n E H" E C (p, t) for all n, p, and t. In this case the predicate g selects 
those strings that are propositions of the form "n E H," and the function S transforms 
"n E H"ton. 
Here is another kind of example. Suppose there is a particular computer C that enumer- 
ates a set H very quickly. Then there is a computer C' that enumerates propositions of 
the form "n E H" just as quickly. In this case the predicate g is taken to be always true, 
and the function f transforms n to "n E H." 
4. The Number of Bits of Axioms Needed to Determine the Complexity of Specific Strings 
The set of all programs that halt when run on U is r.e. Similarly, the set of all true propo- 
sitions of the form "I (s) < n" where s is a string and n is a natural number, is an r.e. 
set. In other words, if a program halts, or if a string is of complexity less than or equal to 
n, one will eventually find this out. To do this one need only try on U longer and longer 
test runs of more and more programs, and do this in a systematic way. 
The problem is proving that a program doesn't halt, or that a string is of complexity 
greater than n. In this section we study how many bits of axioms are needed, and, as was 
pointed out in Section 3, it is sufficient to consider only the rules of inference U. We shall 
see that with n bits of axioms it is impossible to prove that a particular string is of com- 
plexity greater than n + c, where c doesn't depend on the particular axioms chosen 
(Theorem 4.1). It follows from this that if a formal system has n bits of axioms, then 
there is a program of length _< n + c that doesn't halt, but the fact that this program 
doesn't halt can't be proven in this formal system (Theorem 4.2).410 GREGORY J. CHAITIN 
Afterward, we show that n -b c bits of axioms suffice to be able to determine each pro- 
gram of length not greater than n that halts (Theorem 4.4), and thus to determine each 
string of complexity less than or equal to n, and its complexity (Theorem 4.3). Further- 
more, the remaining strings must be of complexity greater than n. i 
Next, we construct an r.e. set of strings P that has the property that infinitely many 
strings aren't in it, but a formal system with n bits of axioms can't prove that a particular 
string isn't an element of P if the length of this string is greater than n + c (Theorem 
4.5). It follows that Pis what Post called a simple set; that is, P iS r.e., and its comple- 
ment is infinite, but contains no infinite r.e. set (see [31], Sec. 5, pp. 319--320). Moreover, 
n -~- c bits suffice to determine each string of length not greater than n that isn't an ele- 
ment of P. 
Finally, we show that not only are n bits of axioms insufficient to exhibit a string of 
complexity > n ~ c, they are also insufficient to exhibit (by means of an e-description) 
an r.e. set of e-complexity greater than n + c (Theorem 4.6). This is because no set can 
be of e-complexity much greater than the complexity of one of its e-descriptions, and thus 
Ie(U(s)) ~ k implies I(s) ~ k -- c, where c is a constant that doesn't depend on s. 
Although these results clarify how many bits of axioms are needed to determine the 
complexity of individual strings, they raise several questions regarding the size of proofs. 
n W c bits of axioms suffice to determine each string of complexity <n and its com- 
plexity, but the method used here to do this appears to be extremely slow, that is, the 
proofs appear to be extremely long. Is this necessarily the case? The answer is "yes," as 
is shown in Section 7. 
We have pointed out that there is a formal system having as theorems all true propo- 
sitions of the form "U (p) halts." The size of the proof that U (p) halts must grow faster 
than any recursive function f of lg (p). For suppose that such a recursive bound on the 
length of proofs existed. Then all true propositions of the form "U (p) doesn't halt" could 
be enumerated by checking to see if there is no proof that U (p) halts of size _<f (lg (p)). 
This is impossible, by Theorem 4.2. The size of these proofs is studied in Section 10. 
THEOREM 4.1. (a ) There is a c such that for all programs p, if a proposition of the form 
"1 (s ) > n" (s a string, n a natural number) is in U (p ) only if I (s ) > n, then "I (s ) > 
n" is in U (p ) only if n < lg (p ) -k c. 
In other words: (b ) There is a c such that for all formal systems (U, p}, if "I (s) > n" is 
a theorem of (U, p) only if it is true, then "I(s) > n" is atheorem of (U, p) only if n< 
lg(p) + c. 
For any r.e. set of propositions T, one obtains the following from (a) by taking p to be a 
minimal e-description of T: (c ) If T has the property that "I (s ) > n" is in T only if I (s ) > 
n:, then T has the property that "I (s ) > n" is in T only if n < Is(T) ~ c. 
IDEA OFF PROOF. The following is essentially the classical Berry paradox. ~ For each 
natural number n greater than 1, consider "the least natural number that can't be de- 
ft:ned in less than N characters." Here N denotes the numeral for the number n. This is a 
Llogl0n _J ~ c character phrase defining a number that supposedly needs at least n char- 
acters to be defined. This is a paradox if LloglQn A W c < n, which holds for all suffi- 
ciently great values of n. 
The following is a sharper version of Berry's paradox. Consider: "the least natural 
number whose definition requires more characters than there are in this phrase." This 
c-character phrase defines a number that supposedly needs more than c characters to be 
defined. 
The following version is analogous to our proof. Consider this program: "Calculate the 
first string that can be proven in (U, p) to be of complexity greater than the number of 
bits in this program, where p is the following string: .-- ." Here "first" refers to first in 
the recursive enumeration U (p, t) (t = 0, 1, 2,... ) of the theorems of the formal system 
6 Although due to Berry, its importance was recognized by Russell and it was published by him [35, 
p. 153].Information-Theoretic Limitations of Formal Systems 411 
(U, p). This program is only a constant number c of bits longer than the number of bits in 
p. It is no longer a paradox; it shows that in (U, p) no string can be proven to be of com- 
plexity greater than c --{- lg(p) = c -b the number of bits of axioms of (U, p}. 
PnOOF. Consider the computer C that does the following when it is given the program 
p'. First, it solves the equation p' = 0rlp. If this isn't possible (i.e. p' ~- 0'), then C 
halts without outputting anything. If this is possible, C continues by simulating running 
the program p on U. It generates U (p) searching for a proposition of the form "I (s) > n" 
in which s is a string, n is a natural number, and n > lg (p') ~ k. If and when it finds 
such a proposition "I (s) > n" in U(p), it outputs s and halts. 
Suppose that p satisfies the hypothesis of the theorem, i.e. "1 (s) > n" is in U (p) only 
if I (s) > n. Consider C (0"im(C)lp). If C (0'im(c)lp) = { s} ; then I (s) _~ lg (0~im(C)lp) + 
sire(C) = lg(p) + 2sim(C) + 1. But C outputs s and halts because it found the 
proposition "l(s) > n" in U(p) and n > lg(p') + k = lg(0'i~(c)lp) -k sim(C) = 
lg (p) + 2sim (C) + 1. Thus, by the hypothesis of the theorem, I (s) > n > lg (p) + 
2sim (C) --k 1, which contradicts the upper bound on I (s). Consequently, C (0~im(C)lp) 
doesn't output anything (i.e. equals ~2~ ), for there is no proposition "I (s) > n" in U (p) 
with n > lg(p) + 2sim(C) -k 1. The theorem is proved with c = 2sire(C) "k 1. 
Q.E.D. 
Definition 4.1. H = {p ] U(p) halts}. (H is r.e., as was pointed out in the first para- 
graph of this section. ) 
THEOREM 4.2. There is a c such that for all formal systems (U, p}, if a proposition of the 
form "s E H" or "s ~ H" ( s a string) is a theorem of ( U, p) only if it is true, then there is 
a strings of length  lg(p) + c such that neither "s E H" nor "s ~ H" is a theorem of 
(u, p). 
PROOF. Consider the computer C that does the following when it is given the program 
p. It simulates running p on U, and as it generates U (p), it checks each string in it to see 
if it is a proposition of the form "s E H" or "s ~ H," where s is a string. As soon as C 
has determined in this way for each string s of length less than or equal to some natural 
number n whether or not "s E H" or "s ~ H" is in U(p), it does the following. 
C supposes that these propositions are true, and thus that it has determined the set 
{s E H I lg(s) _~ n}. Then it simulates running each of the programs in this set on U 
until U halts, and thus determines the set S = O U (s) (s E H & lg (s) ~ n). C then 
outputs the proposition "I (]) > n," where f is the first string not in S, and then con- 
tinues generating U (p) as was indicated in the first paragraph of this proof. Inasmuch as 
f isn't output by any program of length ~_ n that halts, it must in fact be of complex- 
ity > n. 
Thus, C(p) enumerates true propositions of the form "I(f) > n" if p satisfies the 
hypothesis of the theorem. Hence, by Theorem 4.1, "I(f) > n" is in C(p) only if n < 
I~ (C (p) ) -t- c' ~ lg (p) + sire (C) W c'. It is easy to see that the theorem is proved 
with c = sire(C) + c'. Q.E.D. 
THEOREM 4,3. Consider the set T~ consisting of all the true propositions of the form 
"I (s) = k" (s a string, k a natural number < n) and all the true propositions of the form 
"I(s) > n." I,(T~) = n + 0(1). 
In other words, a formal system (U, p') whose theorems consist precisely of all true proposi- 
tions of the form "I(s) = k" with k ~ n, and all true propositions of the form "I (s ) > n," 
requires n -k 0 (1)bits of axioms; i.e. n - c bits are necessary and n -k c bits are su~- 
cient to obtain this set of theorems. 
IDEA OF PROOF. If one knows n and how many programs of length g n halt when run 
on U, then one can find them all, and see what they calculate, n and this number h can be 
coded into an (n + 1 )-bit string. In other words, the axiom of this formal system with 
theorem set Tn is essentially "the number of programs of length _~ n that halt when run 
on U is h," where n and h are particular natural numbers. This axiom is n -k 0 (1) bits of 
information. 
I412 GREGORY J. CHAITIN 
]?ROOF. By Theorem 4.1, I~(Tn ) ~_ n -- c. It remains to show that It(T,) < n + c. 
Consider the computer C that does the following when it is given the program p of 
length _> 1. It generates the r.e. set H until it has found p -- 01~(p) programs of length 
lg(p) -- 1 that halt when run oa U. If and when it has found this set S of programs, it 
simulates running each program in S on U until it halts. C then examines each string that 
is calculated by a program in S, and determines the length of the shortest program in S 
that calculates it. If p - 0 lg(p) = the number h of programs of length _< lg(p) - 1 
that halt when run on U, then C has determined each string of complexity _< lg (p) - 1 
and its complexity. If p - 0 lg(~) < h, then C's estimates of the complexity of strings are 
to() high. And if p - 0 I~(p) > h, then C never finishes generating H. Finally, C outputs 
its estimates as propositions of the form "I (s) = k" with k ~ lg (p) - 1, and as propo- 
sitions of the form 'if(s) > k" with k = lg(p) - 1 indicating that all other strings 
are of complexity > lg (p) - 1. 
We now show how C can be used to enumerate T, economically. Consider h = # ({ s E 
H Ilg(s ) ~ n I). As there are precisely 2 "+~ -- 1 strings of length < n, 0 < h 
2 ~+1 - 1. Let p be 0 ~+~ + h, that is, the hth string of length n + 1. Then C (p) = T., 
and thus I~(T~) < lg(p) + sim(C) = n + 1 + sim(C). Q.E.D. 
THEOREM 4.4. Let T~ be the set of all true propositions of the form "s E H" or "s ~ H" 
with s a string of length < n. I~ (T~) = n + 0 (1). 
In other words, a formal system (U, p} whose theorems consist precisely of all true proposi- 
tions of the form "s E H" or "s ~ H" with lg (s) ~ n, requires n + 0 (1) bits of axioms; 
i.e. n -- c bits are necessary and n + c bits are su~icient to obtain this set of theorems. 
PROOF. Theorem 4.2 shows that It (T~) > n -- c. The proof that I~ (T,) _~ n -t- c 
is obtained from the proof of Theorem 4.3 by simplifying the definition of the computer C 
so that it outputs T,, instead of, in effect, using T~ to determine each string of complex- 
ity ~ n and its complexity. Q.E.D. 
Definition 4.2. P = {slI(s ) < lg(s)}; i.e. P contains each string s whose com- 
plexity I (s) is less than its length lg (s). 
THEOREM 4.5. (a) P is r.e., i.e. there is a formal system with the property that "s E P" 
is a theorem iff s E P. 
(b ) P is infinite, because for each n there is a string of length n that isn't an element of P. 
(c) There is a c such that for all formal systems (U, p), if "s ~ P" is a theorem only if 
it is true, then "s~ P" is a theorem only if lg(s) < lg(p) + c. Thus, by the definition of 
e-vomplexity, if an r.e. set T of propositions has the property that "s ~ P" is in T only if it 
is true, then "s Â¢i P" is in T only if Ig (s) < I~ (T) + e. 
(d) There is a c such that for all r.e. sets of strings S, if S contains no string in P (i.e. 
S c P),thenlg(maxS) < Is(S) + c. Thus maxS < 0 ~'(s)+~ = 2 ~'(s)+~ - 1, and 
#(S) < 2 I*(s)+~. 
( e ) Let T, be the set of all true propositions of the form " s ~ P" with lg ( s ) _< n. I ~ ( T~ ) = 
n + 0 (1). In other words, a formal system (U, p) whose theorems consist precisely of all true 
propositions of the form "s ~ P" with lg(s) < n, requires n + 0(1) bits of axioms; i.e. 
n -- c bits are necessary and n + c bits are sufficient to obtain this set of theorems. 
PROOF. (a) This is an immediate consequence of the fact that the set of all true propo- 
sitions of the form 'if(s) ~ n" is r.e. 
(b) We must show that for each n there is a string of length n whose complexity is greater 
than or equal to its length. There are 2 ~ strings of length n. As there are exactly 2" - 1 
programs of length < n, there are < 2 ~ strings of complexity < n. Thus at least one string 
of length n must be of complexity _> n. 
(c) Consider the computer C that does the following when it is given the program p. 
llt simulates running p on U. As C generates U (p), it examines each string in it to see if 
it is a proposition of the form "s ~i P," where s is a string of length > 1. If it is, C outputs 
the proposition 'if(s) > n"wheren = lg(s) - 1. 
If p satisfies the hypothesis, i.e. "s ~i P" is in U (p) only if it is true, then C(p) enu-Information-Theoretic Limitations of Formal Systems 413 
merates true propositions of the form "I(s) > n" with n = lg(s) -- 1. It foUowsby 
Theorem4.1thatnmustbe< I~(C (p ) ) -Jr c t _< lg(p) -I- sire(C) -I- c'. Thus lg(s ) - 
1 < lg(p) -q- sire(C) -q- c', and part (c) of the theorem is proved with c = sire(C) q- 
ct+ 1. 
(d) Consider the computer C that does the following when it is given the program p. 
It simulates running p on U. As C generates U(p), it takes each string s in U(p), and 
outputs the proposition "s Â¢ P." 
Suppose S contains no string in P. Let p be a minimal e-description of S, i.e. U (p) -- S 
and lg(p) = I,(S). Then C(p) enumerates true propositions of the form "s Â¢ P" with 
s E S. By part (c) of this theorem, lg(s) < I,(C(p)) + c' < lg(p) + sire(C) + 
c' = I, (S) + sire (C) -t- e'. Part (d) of the theorem is proved with c = sim (C) + c'. 
(e) That I,(T,) > n - c follows from part (c) of this theorem. The proof that 
I, (T,) < n -q- c is obtained by changing the definition of the computer C in the proof of 
Theorem 4.3 in the following manner. After C has determined each string of complexity < 
n and its complexity, C determines each string s of complexity < n whose complexity is 
greater than or equal to its length, and then C outputs each such s in a proposition of the 
form "s (~ P." Q.E.D. 
THEOREM 4.6. (a ) There is a c such that for all programs p, if a proposition of the form 
"I~( U ( s ) ) > n" ( s a string, n a natural number) is in U (p ) only if le( U ( s ) ) > n, then 
"I,(U(s)) > n"isinU(p)onlyifn < lg(p) q- c. 
In other words: (b ) There is a c such that for all formal systems (U, p}, if "I,(U (s) ) > 
n" is a theorem of (U, p) only if it is true, then "I, (U (s ) ) > n" is a theorem of (U, p} only 
ifn < lg(p) + e. 
For any r.e. set of propositions T, one obtains the following from (a) by taking p to be a 
minimal e-description of T: (c) If T has the property that "I,(U (s) ) > n" is in T only if 
I, ( U ( s ) ) > n, then T has the property that " I e ( U ( s ) ) > n " is in T only if n < I â¢ ( T ) + c. 
C p PROOF. By Theorem 2.1(c), there is a such that Ie(U (s)) > n implies I(s) > 
n -- c'. 
Consider the computer C that does the following when it is given the program p. It 
simulates running p on U. As C generates U(p), it checks each string in it to see if it is a 
proposition of the form "I, (U (s)) > n" with s a string and n a natural number. Each 
time it finds such a proposition in which n > c', C outputs the proposition "I (s) > m" 
whereto = n -- c' > 0. 
If p satisfies the hypothesis of the theorem, then C (p) enumerates true propositions of 
C t theform"I(s) > m." "I(s) > m" (m = n - >_ O)isinC(p)iff"I~(U(s)) > 
n" (n > c') isin U(p). By Theorem 4.1, "I(s) > m"isin C(p) onlyif m < I,(C(p)) 
+ c" _< lg(p) + sim(C) + c". Thus "I,(U (s) ) > n" (n > c')isinU(p)onlyifn 
- c' < lg(p) + sire(C) + c". The theorem is proved with c = sire(C) + c" + c'. 
Q.E.D. 
5. The Greatest Natural Number of Complexity < n 
The growth of a (n), the greatest natural number of complexity < n as a partial function 
of n, serves as a benchmark for measuring a number of computational phenomena. The 
general approach in Sections 6 to 10 will be to use a partial function of n to measure some 
quantity of computational interest, and to compare the growth of this partial function as 
n increases with that of a (n). 
We compare rates of growth in the following fashion. 
Definition 5.1. We say that a partial function f grows at least as quickly as another 
partial function g, written f >' g or g <' f, when a shift of f overbounds g. That is, when 
there is a c such that for all n, if g (n) is defined, thenf(n + c) is defined andf(n "-I- c) > 
g (n). Note that f _<' g and g <' h implies f <' h. 
Definition 5.2. We say that the partial functions f and g grow equally quickly, written 
f ='g, ifff <'gandg <'f.414 GREGORY J. CHAITIN 
We now formally define a (n), and list its basic properties for future reference. 
Definition 5.3. a(n) = max k (I(k) < n). The maximum is taken over all natural 
numbers k of complexity _< n. If there are no such k, then a (n) is undefined. 
THEOREM5.1. (a)Ifa(n)isdefined, thenI(a(n)) < n. 
(b ) If a (n ) is defined, then a (n -~ 1 ) is defined and a (n ) <_ a (n -~ 1 ). 
(c) IfI(n) < m, thenn < a(m). 
(d) n < a(I(n)). 
(e) If a(m) is defined, then n > a(m) implies I (n) > m. 
(f) IfI(n) > iforalln > m, thenm > a(i) ifa(i)isdefined. 
(g)I(a(n)) = n -F 0(1). 
(h ) There is a c such that for all finite sets S of strings, max S g a(I (S) + c). 
(i) There is a c such that for all finite sets S of strings and all n, if a(n ) is defined and 
a(n) E S, thenI(S) > n - c. 
])ROOF. (a) to (f) follow immediately from the definition of a (n). 
(g) Consider the two computers C and C' that always halt and such that C (n) = 
{n -k 11 and C' (n + 1) = {n}. It follows by Theorem 2.1(c) that I (n) = I (n + 1) "F 
0(1). By part (e) of this theorem, if a(n) is defined then I(a(n) + 1) > n. By part 
(a) of this theorem, I (a (n) ) < n. Hence if a (n) is defined we have I (a (n)) = I (a (n) + 
1) "F O(1),I(a(n)) < n, andI(a(n) + 1) > n. ItfollowsthatI(a(n)) = n + 0(1). 
(h) Consider the computer C suchthat C (p) = { max S I and halts if p is a description 
of S, i.e. if U(p) = S and halts. It follows that/(max S) < I(S) + sim(C). Thus by 
part (c) of this theorem, max S g a (I (S) + c), where c -- sim (C). 
(i) Consider the computer C such that C (p) = { 1 -{- max SI and halts if p is a descrip- 
tion of S, i.e. if U(p) = S and halts. It follows that I (1 --F max S) .< I (S) + sim (C). 
Ifa(n) E S, thenl + maxS > a(n), and thus by part (e) of this theorem l(1 + 
maxS) > n. Hencen < I(1 + maxS) ~ 1(S) + sim(C),andthusI(S) > n - 
c, where c = sire(C). Q.E.D. 
6. How Fast Does the Greatest Natural Number of Complexity _< n Grow with Increasing n? 
In Theorem 6.2 we show that an equivalent definition of a (n) is the greatest value at n of 
any partial recursive function of complexity < n. In Theorem 6.3 we use this to show that 
any partial function >_t a eventually overtakes any partial recursive function. This will 
apply directly to all the functions that will be shown in succeeding sections to be =' to a. 
In Theorem 6.4 it is shown that for any partial recursive function ], f(a(.)) <t a. 
Thus there is a c such that for all n, if a (n) is defined, then a (n) < a (n --F c) (Theorem 
6.5). 
'ITI-IEOREM 6.1. There is a c such that if f : N ~ N is a partial recursive function defined 
atnandn > I~(f),thenI(f(n)) _< n -k c. 
PROOF. Given a minimal e-description s of the graph of f, we add it to 0 ~+1. As n > 
I,.(f) = lg(s), the resulting string p = 0 ~+1 + s has both n (= Ig(p) - 1) and the 
graph of f (= U (s) = U (p - 0 l*(p) ) ) coded into it. Given this string p as its program, 
a computer C generates the graph of f searching for the pair (n, ](n)). If and when it is 
found, the computer outputs f(n) and halts. Thus, f(n), if defined, is of complexity _< 
lg(p) -F sim(C) = n -F 1 + sim(C). Q.E.D. 
Definition 6.1. b(n) = maxf(n) (It(f) < n). The maximum is taken over all par- 
tial recursive functions f : N --~ N that are defined at n and are of e-complexity <_ n. If 
there are no such functions, then b (n) is undefined. t 
THEOREM 6.2. a = b. 
PROOF. First we show that b < ' a. If b (n) is defined, then there is a partial recursive 
function f : N ~ N defined at n with I~ (f) < n, such that f (n) = b (n). By Theorem 
6.1, I(f(n)) < n + c, and thusf(n) _< a(n -F c) by Theorem 5.1(c). Hence if b(n) 
is defined, b(n) f(n) < a(n + c), and thus b _<' ao 
Now we show that a <' b. Suppose that a (n) isdefined, andconsider the constant func-Information-Theoretic Limitations of Formal Systems 415 
tion fn :N --~ N whose value is always a(n), and the computer C such that C(n) = 
{ (0, n), (1, n), (2, n), ...}. It follows byTheorem 2.1(c) that Ie(fn) < I(a(n)) + c, 
which by Theorem 5.1(a) is < n --{- c. Thus if a(n) is defined, a(n) = fn(n + e) _~ <' maxf(n + c) (I,(f) < n + c) = b(n -k c). Hencea b. Q.E.D. 
THEOREM 6.3. Let the partial function x : N --~ N have the property that x >' a. There 
is a constant c' such that the following holds for all partial recursive functions f : N ~ N. If 
f ( n ) is defined and n > I , (f ) -F c', then x ( n ) is defined and x ( n ) _> f ( n ). i >1 >t PROOF. By Theorem 6.2 and the transitivity of ~_ , x _ a _ b. Thus there is a c 
such that x (n q- e) is defined and x (n q- e) >_ b (n) if b (n) is defined. Consider the 
shifted function f(n) = f(n + c). The existence of a computer C sueh that (i,j) E 
C(p) iff (i q- c,j) E U(p) shows that I, (f' ) < I,(f) -t- sim(C). Bythe definition of 
b, x (n q- c) > b (n) >_ f' (n) if f' is defined at n and I, (.f') <_ n. Thus z (n -t- e) ~_ 
f(n + c)iffisdefinedatn + c andI,0 e) < I,(f) + sim(C) < n. In other words, 
z(n) _> f(n)iff isdefined at n andI,(]) + sire(C) + e _< n. The theorem is proved 
with c' = sim(C) q- e. Q.E.D. 
THEOREM 6.4. Let f : N ~ N be a partial recursive function, f (a (.)) <1 a. 
PROOF. There is a computer C such that C(n) = {f(n)} and halts if f (n) is defined. 
Thus by Theorem 2.1(c), if f (n) is defined, I(f(n)) <_ I(n) + c. Substituting a(n) for 
n, we obtain I (f(a(n) ) ) _< I (a(n) ) + c _< n -k c, for by Theorem 5.1 (a), I (a(n ) ) _< 
n. Thus iff(a(n)) is defined, f(a(n)) _< a(n q- c), by Theorem 5.1(e). Q.E.D. 
THEOREM 6.5. There is a e such that for all n, if a (n ) is defined, then a (n ) < a (n + c ). 
PROOF. Takingf(n) = n + l in Theorem 6.4, we obtain a (. ) "-k 1 <' a. Q.E.D. 
7. The Resources Needed to Calculate/Enumerate the Set of All Strings of Complexity ~_ n. 
7.1. We first discuss the metamathematical implications of the material in this section. 
The basic fact used in this section (see the proof of Theorem 7.3) is that for any com- 
puter C there is a c such that for all n, if a (n) is defined then max lJ C (p, a (n)) (lg (p) 
a (n)) is less than a (n + e). Thus a (n + c) cannot be output by programs of length 
a(2) in time _~ a(n). If we use Theorem 3.1 (a) to take C to be such that s E C(p, t) 
iff "I(s) = k" E C*(p, t), and we recall that a(n + c) is a string of complexity _~ 
n + c, we obtain the following result. Any formal system (C*, p) whose theorems include 
all true propositions of the form "I(s) = It" with k _~ n + c, must either have more 
than a (n) bits of axioms, or need proofs of size greater than a (n) to be able to demonstrate 
these propositions. Here c depends only on the rules of inference C*. This is a strong result, 
in view of the fact that a (n) is greater than or equal to any partial recursive function f(n) 
for n > I,(f) + c' (Theorem 6.3). 
The idea of Section 9 is to show that both extremes are possible and there is a drastic 
trade-off. We can deduce these results from a few bits of axioms (< n + c bits by Theo- 
rem 4.3) by means of enormous proofs, or we can directly take as axioms all that we wish 
to prove. This gives short proofs, but we are assuming an enormous number of bits of 
axioms. 
From the fact that a(n -/- c) > max U C(p, a(n)) (lg(p) _< a(n)), it also follows 
that if one wishes to prove a numerical upper bound on a(n + c), one faces the same 
drastic alternatives. Lin and Rado, in trying to determine particular values of ~ (n) and 
SH (n), have, in fact, essentially been trying to do this (see [36]). In their paper they ex- 
plain the difficulties they encountered and overcame for n = 3, and expect to be insur- 
mountable for greater values of n. 
7.2. Now we begin the formal exposition, which is couched exclusively in terms of 
computers. 
In this section we study the set K (n) consisting of all strings of complexity _~ n. This 
set turns out to be extremely difficult to calculate, or even to enumerate a superset of-- 
either the program or the time needed must be extremely large. In order to measure this 
difficulty, we will first measure the resources needed to output a (n).416 GREGORY J. CHAITIN 
Definition 7.1. K(n) = {slI(s) _< n}. Note that this set may be empty, and 
# (K (n)) isn't greater than 2 "+1 - 1, inasmuch as there are exactly 2 ~+1 - 1 programs 
of length _< n. 
We shall show that a (n) and the resources required to calculate/enumerate K (n) grow 
equally quickly. What do we mean by the resources required to calculate a finite set, or 
to enumerate a superset of it? It is assumed that the computer C is being used to do this. 
.Definition 7.2. Let S be a finite set of strings, r(S), the resources required to calculate 
S, is the least r such that there is a program p of length < r having the property that 
C(p, r) -- S and is halted. If there is no such r, r(S) is undefined, r,(S), the resources 
required to enumerate a superset of S, is the least r such that there is a program p of 
length _< r with the property that S c C (p, r). If there is no such r, r, (S) is undefined. 
We abbreviate r({s} ) and r~({s} ) as r(s) and re(s). 
We shall find very useful the notion of the set of all output produced by the computer 
C with information and time resources limited to r. We denote this by Cr. 
Definition 7.3. Cr -- (J C (p, r) (lg (p) (r). 
We now list for future reference basic properties of these concepts. 
THEOREM 7.0 
freezE(n) ilK(n) ~ $2~, 
(a)a(n) -- (undefined ilK(n) = ~j. 
(b) K(n) ~ ~,anda(n) isdefined, iffn ~_ n*. Heren* = minI(s),wherethemini- 
mum is taken over all strings s. 
(c) For all r, C, ~ C~+1. 
In (d) to (k), S and S' are arbitrary finite sets of strings. 
(d) S c Cre(s) if re(S) isdefined. 
(e) r,(S) < r(S) lit(S) isdefined. 
(f) If r~(S') isdefined, then S c S' impliesr~(S) < r~(S'). 
(g) If S c C(p, t), then either lg(p) > re(S) or t _> re(S). 
(h ) If C (p ) == S and halts, then either lg(p ) > r(S), or the time at which C (p ) halts 
is > r (S). 
(i) If C (p ) = S and halts, and lg (p ) < r (S), then C (p ) halts at time > r (S). 
In (j) and (k) it is assumed that C is U. Thus r (S) and r, (S) are always defined. 
(j) If r (S) > I (S), then there is a program p of length I (S) such that U (p ) = S and 
halts at time > r (S). 
(k) r(S) _> I(S). 
PROOF. These results follow immediately from the definitions. Q.E.D. 
THEOREM 7.1. There is a c such that for all finite sets S of strings, (a) I (r(S) ) < 
I (S) q- c if r(S) is gefined, and (b ) I (r~(S) ) < I (S) d- cirri(S) is defined. 
PROOF. (a) Consider the computer C' that does the following when it is given the pro- 
gram p. First, it simulates running p on U. If and when U halts during the simulated run, 
C' has determined the finite set S = U (p) of strings. Then C' repeats the following op- 
erations for r = 0, 1, 2, â¢ â¢ .. 
C' determines C(p', r) for each program p' of length < r. It checks those C(p', r) 
(lg (p') _< r) that are halted to see if one of them is equal to S. If none of them are, C' 
adds 1 to r and repeats this operation. If one of them is equal to S, C' outputs r and halts. 
Let p be a minimal description of a finite set S of strings, i.e. U (p) -- S and halts, and 
lg (p) = I (S). Then if r (S) is defined, C' (p) = { r (S)} and halts, and thus I (r (S)) _< 
lg (p) + sim (C') = I (S) + sim (C'). This proves part (a) of the theorem. 
(b) The proof of part (b) of the theorem is obtained from the proof of part (a) by 
(;hanging the definition of the computer C' so that it checks all C (p', r) (lg (p') < r) to 
see if one of them includes S, instead of checking all those C (p', r) (lg (p') < r) that are 
halted to see if one of them is equal to S. Q.E.D. 
THEOREM 7.2. max Co(.) _<' a.Information-Theoretic Limitations of Formal Systems 417 
PROOF. Theorem 2.1(c) and the existence of a computer C' such that C'(r) = C~ 
and halts, shows that there is a c such that for all r, I (C~) < I (r) -k c. Thus by Theo- 
C I rem5.1(h) and (b) thereisa suchthat for allr, maxC~ < a(I(r) + et). Henceif 
a(n) is defined, maxC~(~) ~_ a(I(a(n)) + c ~) < a(n T c r) by Theorem 5.1(a) and 
(b). Q.E.D. 
THEOREM 7.3. (a) If r~(a(n) ) is defined when a(n) is, then r~(a(.) ) =' a. (b ) If 
r (a (n)) is defined when a (n) is, then r (a (.) ) =~ a. 
PROOF. By Theorem 7.1, if re (a (n)) and r(a (n)) are defined, I (re(a(n))) ~ I(a (n)) T 
c and I (r (a (n) ) ) < I (a (n) ) -b c. By Theorem 5.1 (a), I (a (n) ) ~ n. Thus I (r~ (a (n)) ) 
n --t- c and I (r (a (n)) ) < n --[- c. Applying Theorem 5.1 (c), we obtain r~ (a (n)) <_ 
a(n -k c) and r(a(n)) ~_ a(n -b c)~ Thus we have shown that re(a(.)) <' a and 
r (a (.)) _<' a, no matter what C is. 
r(S), if defined, is > re(S) (Theorem 7.0(e)), and thus to finish the proof it is suffi- <' cient to show that a re(a(.)) if re(a(n)) is defined when a(n) is. By Theorems 7.2 
~,nd 6.5 there is a e such that for all n, if a(n) is defined, then max C~(~) < a(n T e), 
and thus a(n ~ c) ~ C~(,~. And inasmuch as for all finite sets S, S c C~Â°(s) (Theo- 
rem 7.0(d)), it follows that a(n + c) E C~(~(~+o)). 
In summary, there is a c such that for all n, if a(n) is defined, then.a(n --b c) ~ C~(~), 
and a(n + c) E C,,(,(~+~)). 
As for all r, C~ c C~+1 (Theorem 7.0 (c)), it follows that if a (n) is defined then a (n) < 
_<' re(a(n + e)). Thus a re(a(.)). Q.E.D. 
THEOaEM7.4. l(K(n)) = n -t- 0(1). 
PROOF. As was essentially shown in the proof of Theorem 4.3, there is a computer C t 
such that C'(0 ~+1 -b #({p E H ilg(p) _< n})) = K(n) and halts, for all n. Thus 
I(K(n)) < n --k 1 -k sim (C ~)foralln. 
K (n) = J2~ can hold for only finitely many values of n, by Theorem 7.0 (b). By Theo- 
rem 7.0(a), for all other values of n, a(n) E K(n), and thus, by Theorem 5.1 (i), there 
isacsuchthatI(K(n)) > n - cforallsuchn. Q.E.D. 
THEOREM 7.5. (a)Ifre(K(n))isdefinedforalln, thenr~(K(.)) =~ a. (b)Ifr(K(n)) 
is defined for all n, then r (K (.)) =' a. 
PROOF. By Theorem 7.1, if r~(K(n)) and r(K(n)) are defined, I(r~(K(n))) ~. 
I(K(n)) + c, and I(r(K(n))) ~ I(K(n)) -b c. I(K(n)) = n -~- O(1) (Theorem 
7.4),andthusthereisa thatdoesn'tdependonnsuchthatI(r~(K(n))) < n "-b c', 
and I (r (K (n) ) ) ~ n "-b c'. Applying Theorem 5.1 (c), we obtain re (K (n)) < a (n + c' ), 
and r(K (n ) ) < a(n + c' ). Thus we have shown that re(K(.)) _~' a and r(K (. ) ) <~ 
a, no matter what C is. 
For all finite sets S and S' of strings, if S c S', then re (S) < r~ (S') < r (S') if these 
are defined (Theorem 7.0(f), (e)). As a (n) E K (n) if a (n) is defined (Theorem 7.0 (a)), 
we have r~(a(n)) ~_ r~(K(n)) < r(K(n)) if these are defined. By Theorem 7.3(a), 
_~ â¢ 
re(a(.)) a ff r~(a(n)) is defined when a(n) is, and thus re(K(.)) ~' a and r(K(.)) >' 
a if these are defined for all n. Q.E.D. 
THEOREM 7.6. Suppose r, (a (n)) is defined when a (n) is. There is a c such that the fol- 
lowing holds for all partial recursive functions f :N ~ N. If n _~ I~(f) -b c and f(n) is 
defined, then (a ) a (n ) is defined, (b ) if a (n ) E C (p, t ), then either lg (p ) _~ f (n ) or t ~_ 
f(n), and (c) ilK(n) C C(p, t), then either lg(p) > f(n) or t ~_ f(n). 
PROOF. (a) and (b) By Theorem 7.3(a), re(a(.)) ~' a. Taking re(a(.)) to be the 
partial function x (.) in the hypothesis of Theorem 6.3, we deduce that if n _~ I, (f) --b 
c and f(n)is defined, then a (n)is defined and re (a (n)) ~_ f(n). Here c doesn't depend 
On f. 
Thus if a(n) E C(p, t), then by Theorem .7.0(g) it follows that eitherlg(p) ~_ 
r~(a(n)) _~ f(n) ort > r~(a(n)) ~ f(n). 
(c) Part (c) of this theorem is an immediate consequence of parts (a) and (b) and 
the fact that if a(n) is defined then a(n) E K(n) (see Theorem 7.0(a)). Q.E.D.418 GREGORY J. CHAITIN 
8. The Minimum Time Such That All Programs of Length < n That Halt Have Done So 
In this section we show that for any computer, a >' the minimum time such that all 
programs of length _< n that halt have done so (Theorem 8.1). Moreover, in the case 
of U this is true with ,,_r,, instead of ">'" (Theorem 8.2). 
The situation revealed in the proof of Theorem 8.2 can be stated in the following 
vague but suggestive manner. Suppose that one wishes to calculate a (n) or K (n) using 
the standard computer U. To do this one only needs about n bits of information. But a 
program of length n -t- 0(1) for calculating a(n) is among the programs of length 
<n T 0 (1) that take the most time to halt. Likewise, an (n + 0 (1))-bit program for 
calculating K(n) is among the programs of length _< n + 0(1) that take the most 
time to halt. These are among the most difficult calculations that can be accomplished 
by programs having not more than about n bits. 
Definition 8.1. de(n) = the least t such that for all p of length < n, if C(p) halts, 
then C (p, t) is halted. This is the minimum time at which all programs of length < n 
that halt have done so. Although it is 0 if no program of length < n halts, we stipulate 
that dc (n) is undefined in this case. 
THEOREM 8.1. dc~' a. 
:PROOF. Consider the computer C' that does the following when it is given the pro- 
C! ... gram p. simulates C(p, t) for t = 0, 1, 2, until C(p, t) is halted. If and when 
this occurs, C' outputs the final value of t, which is the time at which C (p) halts. Fi- 
nally, C' halts. 
If de(n) is defined, then there is a program p of length < n that halts when run on C 
and does this at time dc (n). Then C' (p) = { dc (n)} and halts. Thus I (de (n)) < lg (p) -t- 
sirn (C') < n + sim (C'). By Theorem 5.1(c), we conclude that de(n) is, if defined, 
_< a(n Jr sim(C')). Q.,E.D. 
THEOREM 8.2. dv = a. 
PROOF. In view of Theorem 8.1, dv _<' a. Thus we need only show that dv >' a. 
Recall that a(n) is defined iff n > n* (Theorem 7.0(b)). As C = U is a universal 
computer, r(a(n)) is defined if a(n) is defined. Thus Theorem 7.3(b) applies to this 
choice of C, and r(a(.)) >' a. That is to say, there is a c such that for all 
n ~_ n*, r(a(n + c)) ~ a(n). 
Asa _ a, taking x aandf(n) = n+ c+ 1 in Theorem6.3, we obtain the follow- 
ing. There is a such that for all n > n* + c', a(n) > f(n) n + c + 1. We con- ! 
elude that for all n >_ n* + c, a(n) > n + c. 
By Theorem 5.1(a), n + c >_ l(a(n + c)) for all n >_ n*. 
The preceding results may be summarized in the following chain of inequalities. For 
all n > n* -t- c', r(a(n -i- c) ) >_ a(n) > n -{- c >__ I (a(n -{- c) ). 
As r(a(n + c)) > I(a(n + c)), the hypothesis of Theorem 7.0(j) is satisfied, and 
we conclude the following. There is a program p of length I (a (n + c)) < n -t- c such 
that U(p) = {a(n + c)} and halts at time >__ r(a(n + c)) >_ a(n). Thus for all n >_ 
n* + c', dv(n + c) > a(n). 
Applying Theorem 5.1(b) to this lower bound on dv(n + c), we conclude that for 
all n >_ n*, dv(n + c' + c) >_ a(n T c') >_ a(n). Q.E.D. 
9. Examples of Trade-Offs Between Information and Time 
Consider calculating a (n) using the computer U and the computer C defined as follows. 
For all programs p, C (p, 0) = [p} and is halted. 
Since I(a(n)) < n (Theorem 5.1(a)), there is a program _< n bits long for calculat- 
ing a(n) using U. But inasmuch as r(a(. )) >_' a (Theorem 7.3(b)) and dv _<' a (Theo- 
rem 8.1 ), this program takes "about" a (n) units of time to halt (see the proof of Theorem 
8.2). More precisely, with finitely many exceptions, this program takes between a (n - c) 
and a(n -k c) units of time to halt. 
What happens if one uses C to calculate a(n)? Inasmuch as C(a(n)) = {a(n)} andInformation-Theoretic Limitations of Formal Systems 419 
halts at time 0, C can calculate a(n) immediately. But this program, although fast, 
is lg (a (n) ) = t_ logs (a (n) + 1 ) ._l bits long. Thus r (a (n)) is precisely lg (a (n)) if one 
uses this computer. 
Now for our second example. Suppose one wishes to enumerate a superset of K(n), 
and is using the following two computers, which never halt: C(p, t) = {s I lg(s) < t} 
and C' (p, t) = {s]lg (s) < lg (p)}. These two computers have the property that K (n), 
if not empty, is included in C(p, t), or is included in C'(p, t), iff t > lg(a(n)), or iff 
lg(p) > lg(a(n)), respectively. Thus for these two computers, r~(K(n)), which we 
know by Theorem 7.5(a) must be =' to a, is precisely given by the following: 
re (K (n)) = 0 if a (n) is undefined, and lg (a (n)) otherwise. 
It is also interesting to slow down or speed up the computer C by changing its time 
scale recursively. Let f : N --~ N be an arbitrary unbounded total recursive function 
with the property that for all n, f(n) < f(n -t- 1). C I, the ] speed-up/slowdown of C, 
is defined as follows: C+(p, t) = {s If(lg(s)) < t}. For the computer C s, r~(K(n)) is 
precisely given by the following: re(K(n)) = 0 if a(n) is undefined, and f(lg(a(n))) 
otherwise. The fact that by Theorem 7.5 (a) this must be =' to a, is related to Theorem 
6.4 that for any partial recursive function f, f(a (.)) <' a. 
Now, for our third example, we consider trade-offs in calculating K(n). We use U 
and the computer C defined as follows. For all p and n, C(p, 0) is halted, and n E C(p, 
0) iff lg (p) > n and the nth bit of the string p is a 1. 
Inasmuch as I(K(n)) = n + O(1) (Theorem 7.4), if we use U there is a program 
about n bits long for calculating K(n). But this program takes "about" a(n) units of 
time to halt, in view of the fact that r(K(. )) >' a (Theorem 7.5 (b)) and dv ~.' a 
(Theorem 8.1) (see the proof of Theorem 8.2). More precisely, with finitely many ex- 
ceptions, this program takes between a(n -- c) and a(n "t- c) units of time to halt. 
On the other hand, using the computer C we can calculate K(n) immediately. But 
the shortest program for doing this has length precisely 1 + max K(n) = a(n) + 1 if 
a (n) is defined, and has length 0 otherwise. In other words, for this computer r (K (n)) = 
0 if a(n) is undefined, and a(n) -+ 1 otherwise. 
We have thus seen three examples of a drastic trade-off between information and 
time resources. In this setting, information and time play symmetrical roles, especially 
in the case of the resources needed to enumerate a supersct. 
10. The Speed of Recursive Enumerations 
10.1. We first discuss the metamathematical implications of the material in this 
section. 
Consider a particular formal system, and a particular r.e. set of strings R. Suppose 
that a proposition of the form "s E R" is a theorem of this formal system iff it is true, 
i.e. iff the string s is an element of R. Define e(n) to be the least m such that all theorems 
of the formal system of the form "s E R" with lg (s) < n have proofs of size < m, By 
using Theorem 3.1(a) we can draw the following conclusions from the results of this 
section. First, e <' a for any R. Second, e =~ a iff 
(.) I({s E R L lg(s) < ~})~= n + 0(1). 
Thus r.e. sets R for which e =' a are the ones that require the longest proofs to show 
that "s E R", and this is the ease iff R satisfies (,). It is shown in this section that the 
r.e. set of strings {p I P 6 U(p)} has property (,), and the reader can show without 
difficulty that H and P are also r.e. sets of strings that have property (*). Thus we 
have three examples of R for which e =' a. 
10.2. Now we begin the formal exposition, which is couched exclusively in terms of 
eomputers. 
Consider an r.e. set of strings R and a particular computer C* and program p* sueh 
that C* (p*) = R. How quickly is R enumerated? That is, what is the time e(n) that it 
takes to output all elements of R of length < n?GREGORY J. CHAITIN 
Definition 10.1. R~ = {8 E R [ lg(s) _< n}. e(nn) = the least t such that R~ C C*(p*, 
t). 
We shall see that the rate of growth of the total function e (n) can be related to the 
growth of the complexity of Rn. In this way we shall show that some r.e. sets R are the 
most difficult to enumerate, i.e. take the most time. 
THEOREM 10.1.7 There is a c such that for all n, I (R, ) <_ n + c. 
PROOF. 0 _< #(R,) _< 2 "+1 --1, for there are precisely 2 "+~ -1 strings of length < 
n. Consider p, the #(R,)-th string of length n + 1; i.e. p = 0 "+l "t- #(R,). This string 
hat; both n (= lg (p)- 1 ) and #(R,) (= p - 0 lg(p)) coded into it. When this string p 
is its program, the computer C generates the r.e. set R by simulating C* (p*), until it 
has found #(R,) strings of length _< n in R. C then outputs this set of strings, which is 
R,, and halts. Thus I(R,) < lg(p) + sim(C) = n + 1 + sim(C). Q.E.D. 
_<' THEOREM 10.2. (a) There is a c such that for all n, e(n) < a(I(R,) -k c). (b) e a. 
PROOF. (a) Consider the computer C that does the following. Given a description p 
of R, as its program, the computer C first simulates running p on U in order to deter- 
mine R,. Then it simulates C*(p*, t) for t = 0, 1, 2, â¢ .- until R, C C*(p*, t). C then 
outputs the final value of t, which is e(n), and halts. 
This shows that _< sim(C) bits need be added to the length of a description of R, 
to bound the length of a description of e(n); i.e. if U(p) ffi R, and halts, then C(p) = 
{e(n)} and halts, and thus I(e(n)) < lg(p) -'b sire(C). Taking p to be a minimal de- 
scription of R,, we have Ig(p) = I(R,), and thus I(e(n)) _< I(R,) + sire(C). By 
Theorem 5.1(c), this gives us e(n) _< a(I(R,) + sire(C)). Part (a) of the tlteorem is 
proved with e = sim(C). 
(b) By part (a) of this theorem, e (n) <_ a (I (Rn) + c). And by Theorem 10.1, I (R,) <_ 
n Ã· c' for all n. Applying Theorem 5.1(b), we obtain e(n) < a(I(R,) + e) _< a(n + 
c' .~ c) for all n. Thus e a. Q.E.D. 
THEOREM 10.3. If a _<' e, then there is a e such that I (R~) _> n -cfor all n. 
PROOF. By Theorem 7.0(b) and the definition of <', if a _<' e, then there is a Co 
such that for all n _> n*, a(n) <_ e(n + Co). Andby Theorem 10.2(a), there is a cl such 
that e(n + Co) _< a(I (R~+co~ + cl) for all n. We conclude that for all n >_ nn*, a(n) < 
a(I (R,+~o) + cl). 
By Theorems 6.5 and 5.1 (b), there is a c~ such that if a (m) is defined and m < n -- c~, 
then a (m) < a (n). As we have shown in the first paragraph of this proof that for all 
n > n*, a(n) < a(I(R~+~,) + cl), it follows that I(R~+~,) -k el > n -- c2. 
In other words, for all n .~ n*, I(R~+~o) > (n + co) - Co -- el -- c2. And thus for 
all n, I (R~) _> n -- co - ci -- e2 -- M, where M = max~<~.+~0 n - co -- cl -- c~ if this 
is positive, and 0 otherwise. The theorem is proved with e = co d- el -t- e2 --~ M. Q.E.D. 
THEOREM 10.4. If there is a c such that I (R~) ~_ n - c for all n, then (a) there is a e' 
such that if t > e(n), then I(t) .~ n c', and (b) e >' ~ ~ a. 
PROOF. By Theorem 5.1(f) it follows from (a) that e(n) > a(n -- c') if a(n -- c') >' is defined. Hence e(n d- c') ~_ a(n) if a(n) is defined, i.e. e a. Thus to complete the 
proof we need only show that (a) follows from the hypothesis. 
We consider the case in which t ~_ e(n) and n ~_ I (t) = n - k, for if I (t) > n then 
any c' will do. 
There is a computer C that does the following when it is given the program 01G(~)lkp, 
where p is a minimal description of t. First, C determines lg(p) -.{- k = I(t) d- k = 
(n -- k) --~ k = n. Second, C simulates running p on U in order to determine U (p) = 
{tl. C now uses its knowledge of n and t in order to calculate R,. To do this C first simu- 
lates running p* on C* in order to determine C* (p*, t), and finally C outputs all strings 
in C* (p*, t) that are of length < n, which is R,, and halts. 
In summary, C has the property that if t ~_ e(n), I(t) = n - k, and p is a minimal 
7 This theorem, with a different proof, is due to Loveland [37, p. 664].Information-Theoretic Limitations of Formal Systems 421 
description of t, then C (0]g(k)lkp) = R. and halts, and thus 
I(R,) < lg(0~g(k)lkp) -~- sim(C) = lg(p) + 21g(k) + sim(C) 
+ 1 = I(t) -k 21g(k) + sim(C) + 1 = n - k + 21g(k)-bsim(C) + 1. 
Taking into account the hypothesis of this theorem, we obtain the following for all n: 
if t > e(n) and I (t) = n - k, then n - c _~ I (R,) < n - k + 21g(k) + sire(C) + 
1, and thus c --k sire(C) + 1 ~ k - 21g(k). As lg(k) = [_log~(k + 1)_l, this implies 
t that there isac such that for alln, ift >_ e(n) andI(t) = n- k, thenk < c'.We 
conclude that for all n, if t >_ e(n), then either I(t) > n or I(t) = n k > n . 
Thus in either case I (t) > n - c'. Q.E.D. 
THEOREM 10.5. /fR = {p I P E U(p)}, then there is a c such that I (R,) > n - c $or 
all n. 
PROOF. Consider the following computer C. When given the program p, C first 
simulates running p on U until U halts. If and when it finishes doing this, C then outputs 
each string s ~ U(p), and never halts. 
If the program p is a minimal description of R,, then C enumerates a set that cannot 
be enumerated by any program p' run on U having < n bits. The reason is that if lg (p') < 
n, then p~ E C (p ) iff p' ~ R, iff p' ~ U (p' ). Thus < sire(C) bits need be added to the 
length I (R,) of a minimal description p of R. to bound the length of an e-description of 
the set C(p) of e-complexity > n; i.e. n < I~(C(p)) < lg(p) + sire(C) = I(R~) + 
sire(C). Hence n < I(R,) + c, where c = sire(C). Q.E.D. 
<_' _ 2_' THEOREM 10.6. (a) e aand3cVnI(R~) < n + c. (b) e aiff3cVnI(R~) ~_ 
n- c. (e) If R = {pip E U(p)l, thene ='aand I(R,) = n Jr 0(1). 
PROOF. (a) is Theorems 10.2(b) and 10.1. (b)is Theorems 10.4(b) and 10.3. 
And (c) follows immediately from parts (a) and (b) and Theorem 10.5. Q.E.D. 
Appendix. Examples of Universal Computers 
In this Appendix we use the formalism of Rogers. 8 In particular, P~ denotes the xth 
Turing machine, Â¢~2) denotes the partial recursive function N X N ~ N that P~ calcu- 
lates, and D~ denotes the xth finite set of natural numbers. Here the index x is an arbi- 
trary natural number. 
First we give a more formal definition of computer than in Section 2. 
A partial recursive function c : N X N ~ N is said to be adequate (as a defining 
function for a computer C) iff it has the following three properties: (a) it is a total func- 
tion; (b) Dc(~.~) C Dc(~.~+~); (c) if the natural number 0 is an element of Dc(~,~), then 
Do(p.e) = Dc(~.t+l). 
A computer C is defined by means of an adequate function c : N X N --~ N as fol- 
lows. (a) C(p, t) is halted iff the natural number 0 is an element of Dc(p,,). (b) C(p, t) 
is the set of strings {n ] n + 1 E Dc(~,t)} ; i.e. the nth string is in C(p, t) iff the natural 
number n + 1 is an element of De(p. t). 
We now give a name to each computer. The natural number i is said to be an adequate 
index iff ~2) is an adequate function. If i is an adequate index, C ~ denotes the computer 
whose defining function is ,p~). If i isn't an adequate index, then "C ~'' isn't the name of 
a computer. 
We now define a universal computer U in such a way that it has the property that if i 
is an adequate index, then U(0~lp) = C~(p) and halts iff C~(p) halts. In what follows i 
and t denote arbitrary natural numbers, and p denotes an arbitrary string. U (0 ~, t) is 
defined to be equal to J2~ and to be halted. U(0~lp, t) is defined recursively. If t > 1 
and U(011p, t - 1) is halted, then U(0~lp, t) = U(0~lp, t - 1) and is halted. Other- 
wise U(0~lp, t) is the set of strings {n I n + 1 E W} and is halted iff 0 E W. Here W ffi 
s See [38, pp. 13-15, 21, 701.422 GREGORY J. CHAITIN 
Ut,<to D~2)(~.~,), and ~ is the greatest natural number < t such that if t' < to then P~ 
applied to (p, g) yields an output in < t steps. 
The universal computer U that we have just defined is, in fact, effectively universal: 
to simulate the computation that C ~ performs when it is given the program p, one gives 
U the program p' = 0~lp, and thus p' can be obtained from p in an effective manner. 
Our second example of a universal computer, U', is not effectively universal, i.e. there is 
no effective procedure for obtaining p' from p.9 
U' is defined as follows: 
U' ( ^ , t) = $2~ and is halted, 
U'(Op, t) U(p,t) - {1} andishaltediff U(p,t) is, and 
U'(lp, t) U(p,t) U {1} andishaltediff U(p, t) is. 
I.e. U' is almost identical to U, except that it eliminates the string 1 from the output, or 
forces the string 1 to be included in the output, depending on whether the first bit of its 
program is 0 or not. It is easy to see that U' cannot be effectively universal. If it were, 
given any program p for U, by examining the first bit of the program p' for U' that simu- 
lates it, one could decide whether or not the string 1 is in U (p). But there cannot be an 
effective procedure for deciding, given any p, whether or not the string 1 is in U (p). 
Added in Proof 
The following additional references have come to our attention. 
Part of G5del's analysis of Cantor's continuum problem [39] is highly relevant to the 
philosophical considerations of Section 1. Cf. especially [39, pp. 265, 272]. 
Schwartz [40, pp. 26-28] first reformulates our Theorem 4.1 using the hypothesis that 
the formal system in question is a consistent extension of arithmetic. He then consider- 
ably extends Theorem 4.1 [40, pp. 32-34]. The following is a paraphrase of these pages. 
Consider a recursive function f : N -~ N that grows very quickly, say f (n) = n !!!!!!!!!!. 
A string s is said to have property f if the fact that p is a description of { s} either implies 
that lg(p) > lg(s) or that U(p) halts at time > f0g(s)). Clearly a 1000-bit string 
with property f is very difficult to calculate. Nevertheless, a counting argument shows 
that there are strings of all lengths with property f, and they can be found in an effective 
manner [40, Lemma 7, p. 32]. In fact, the first string of length n with property f is given 
by a recursive function of n, and is therefore of complexity < log2n -k c. This is thus an 
example of an extreme trade-off between program size and the length of computation. 
Furthermore, an argument analogous to the demonstration of Theorem 4.1 shows that 
proofs that specific strings have property f must necessarily be extremely tedious (if some 
natural hypotheses concerning U and the formal system in question are satisfied) [40, 
Theorem 8, pp. 33-34]. 
[41, Item 2, pp. 12-20] sheds light on the significance of these results. Cf. especially the 
first unitalicized paragraphs of answers numbers 4 and 8 to the question "What is pro- 
gramming?" [41, pp. 13, 15-16]. Cf. also [40, Appendix, pp. 63-69]. 
Index of Symbols 
Section 2: lg (s), max S, # (S), X*, N, C (p, t), C (p), U, sire(C), I (S), I~ (S); Section 3: 
(C, p), (U, p); Section 4: H, P; Section 5: _<', ~', =', a(n); Section 6: b(n); Section 7: 
K(n), r(S), r,(S), C~, n*; Section 8: de(n); Section 10: Rn, e(n). 
REFERENCES 
1. CHAITIN, G.J. Information-theoretic aspects of Post's construction of a simple set. On the 
difficulty of generating all binary strings of complexity less than n. (Abstracts.) AMS Notices 19 
(1972), pp. A-712, A-764. 
9 The definition of U' is an adaptation of [38, p. 42, Exercise 2-11].Information-Theoretic Limitations of Formal Systems 423 
2. CHAITIN, G.J. On the greatest natural number of definitional or information complexity _<n. 
There are few minimal descriptions. (Abstracts.) Recursive Function Theory: Newsletter, no. 4 
(1973), pp. 11-14, Dep. of Math., U. of California, Berkeley. 
3. YON NEUMANN, J. Method in the physical sciences. J. yon Neumann--Collected Works, Vol. VI, 
A. H. Taub, Ed., MacMillan, New York, 1963, No. 36, pp. 491--498. 
4. yon N~U~aANN, J. The mathematician. In The World of Mathematics, Vol. ~, J. R. Newman, 
Ed., Simon and Schuster, New York, 1956, pp. 2053-2063. 
5. BELL, E.T. Mathematics: Queen and Servant of Science. McGraw-Hill, New York, 1951, pp. 
414-415. 
6. WEYL, H. Mathematics and logic. Amer. Math. Mon. 53 (1946), 1-13. 
7. WEYL, H. Philosophy of Mathematics and Natural Science. Princeton U. Press, Princeton, N.J., 
1949, pp. 234-235. 
8. TUBING, k.M. Solvable and unsolvable problems. In Science News, no. 31 (1954), A. W. Heas- 
lett, Ed., Penguin Books, Harmondsworth, Middlesex, England, pp. 7-23. 
9. NAfiEL, E., AND NEWMAN, J.R. G~del's Proof. Routledge & Kegan Paul, London, 1959. 
10. Davis, M. Computability and Unsolvability. McGraw-Hill, New York, 1958. 
11. QUINE, W.V. Paradox. Scientific American 206, 4 (April 1962), 84-96. 
12. KLEENE, S.C. Mathematical Logic. Wiley, New York, 1968, Ch. V, pp. 223-282. 
13. GODEL, K. On the length of proofs. In The Undecidable, M. Davis, Ed., l~aven Press, Hewlett. 
N.Y., 1965, pp. 82-83. 
14. COHEN, P.J. Set Theory and the Continuum Hypothesis. Be~njamin, New York, 1966, p. 45. 
15. ARran, M. A. Speed-up theorems and incompleteness theorems. In Automata Theory, E. R. 
Cainiello, Ed., Academic Press, New York, 1966, pp. 6-24. 
16. EHRENFEUCHT, A., AND MYCIELSKI, J. Abbreviating proofs by adding new axioms. AMS Bull. 
77 (1971), 366-367. 
17. POLYA, G. Heuristic reasoning in the theory of numbers. Amer. Math. Mon. 66 (1959), 375-384. 
18. EINSTEIN, A. Remarks on Bertrand Russell's theory of knowledge. In The Philosophy of Ber- 
trand Russell, P. A. Schilpp, Ed., Northwestern U., Evanston, Ill., 1944, pp. 277-291. 
19. HAWKINS, D. Mathematical sieves. Scientific American 199,6 (Dec. 1958), 105-112. 
20. KOLMOGOROV, A.N. Logical basis for information theory and probability theory. IEEE Trans. 
IT-I$ (1968), 662-664. 
21. MARTzN-L~F, P. Algorithms and randomness. Rev. of Internat. Statist. Inst. 37 (1969), 265-272. 
22. LOYELAND, D.W. A variant of the Kolmogorov concept of complexity. Inform. and Contr. 15 
(1969), 510-526. 
23. CHAITIN, G.J. On the difficulty of computations. IEEE Trans. IT-16 (1970), 5-9. 
24. Wiuus, D.G. Computational complexity and probability constructions. J. ACM 17, 2 (April 
1970), 241-259. 
25. ZVONKIN, A. K., aND LEVIN, L.A. The complexity of finite obiects and the development of the 
concepts of information and randomness by means of the theory of algorithms. Russian Math. 
Surveys ~5, 6 (Nov.-Dec. 1970), 83-124. 
26. SCHNORR, C.P. Zufdlligkcit und Wahrscheinlichkeit--Eine algorithmisehc Begr~ndung der Wahrs- 
cheinlichkeitstheorie. Springer, Berlin, 1971. 
27. FINE, T. L. Theories of Probability--An Examination of Foundations. Academic Press, New 
York, 1973. 
28. CHAITIN, G.J. Information-theoretic computational complexity. IEEE Trans. IT-Â¢O (1974), 
10-15. 
29. DELoNG, H. A Profile of Mathematical Logic. Addison-Wesley, Reading, Mass., 1970, See. 
28.2, pp. 208-209. 
30. DAws, M. Hilbert's tenth problem is unsolvable. Amer. Math. Mon. 80 (1973), 233-269. 
31. POST, E. Recursively enumerable sets of positive integers and their decision problems. In The 
Undecidable, M. Davis, Ed., Raven Press, Hewlett, N.Y., 1965, pp. 305-337. 
32. MINSKY, M. L. Computation: Finite and Infinite Machines. Prentice-Hall, Englewood Cliffs, 
N.J., 1967, Sec. 12.2-12.5, pp. 222-232. 
33. SI~OENFIELD, J.R. Mathematical Logic. Addison-Wesley, Reading,Mass., 1967, See. 1.2,pp. 2--6. 
34. MENDELSON, E. Introduction to Mathematical Logic. Van Nostrand Reinhold, New York, 1964, 
pp. 29-30. 
35. RUSSELL, B. Mathematical logic as based on the theory of types. In From Frege to GOdel, J. van 
Heijenoort, Ed., Harvard U. Press, Cambridge, Mass., 1967, pp. 150-182. 
36. LiN, S., AND RADO, T. Computer studies of Turing machine problems. J. ACM 1~, 2 (April 
1965), 196-212. 
37. LOVELANn, D.W. On minimal-program complexity measures. Conf. Ree. of the ACM Sympo- 
sium on Theory of Computing, Marina del Rey, California, May 1969, pp. 61--65. 
38. ROQERS, H. Theory of Recursive Functions and Effective Computability. McGraw-Hill, New 
York, 1967.424 GREGORY J. CHAITIN 
39. GSDEL, K. What is Cantor's continuum problem? In Philosophy of Mathematics, Benacerraf, 
P., and Putnam, H., Eds., Prentice-Hall, Englewood Cliffs, N.J., 1964, pp. 258-273. 
40. SCHWARTZ, J.T. A short survey of computational complexity theory. Notes, Courant Institute 
of Mathematical Sciences, NYU, New York, 1972. 
41. SchwARtz, J.T. On Programming: An Interim Report on the SETL Project. Installment l: Gen- 
eralities. Lecture Notes, Courant Institute of Mathematical Sciences, NYU, New York, 1973. 
42. CRaITIN, G.J. A theory of program size formally identical to information theory. Res. Rep. 
RC4805, IBM Res. Center, Yorktown Heights, N.Y., 1974. 
RE(IEIVED OCTOBER 1971; RBVISED JULY 1973 
Journal of the A~ociatlou for Computing Machinery, Â¥ol. 21, No. 3, July 1974