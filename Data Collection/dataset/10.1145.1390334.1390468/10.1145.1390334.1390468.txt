Towards Personalized Distributed Information Retrieval
Mark J. Carman
University of Lugano, Faculty of Informatics,
Via BufÔ¨Å 13, 6900 Lugano, Switzerland
mark.carman@lu.unisi.chFabio Crestani
University of Lugano, Faculty of Informatics,
Via BufÔ¨Å 13, 6900 Lugano, Switzerland
fabio.crestani@unisi.ch
ABSTRACT
Our aim is to investigate if and how the performance of Dis-
tributed Information Retrieval (DIR) systems can be im-
proved through personalization. Toward this aim we are
building a testbed of document collections and correspond-
ing personalized relevance judgments. In this paper we dis-
cuss our intended approach for personalizing the three dier-
ent phases of the DIR process. We also describe the test col-
lection we are building and discuss our methodology for eval-
uating personalized DIR using relevance information taken
from social bookmarking data.
Categories and Subject Descriptors
H.3.0 [ Information Storage and Retrieval ]: General
General Terms
Design, Experimentation
Keywords
personalization, distributed information retrieval, evaluation
1. INTRODUCTION
Distributed Information Retrieval (DIR) is an important
subeld of Information Retrieval that is gaining much atten-
tion as the number of Deep Web databases on the Internet
continues to grow. Research over the last few years has
shown that context aware searching and in general the per-
sonalization of search results, can improve the overall search
user experience by providing more relevant results to indi-
vidual users [2]. Our plan is to investigate the applicability
of personalization techniques to DIR.
There are three distinct phases in the DIR process, namely
resource description ,resource selection and results fusion .
In the rst phase a description is built for each of the dis-
tributed collections. Collections are usually assumed to be
non-cooperative in the sense that one cannot access their
internal indexes directly, nor crawl all of their documents,
but may only access their documents via a search interface.
In this case query-based sampling (QBS) techniques [1] are
used to generate a small set of sample documents (usually
of the order of hundreds) that will be used to represent the
resource. Various statistical representations can be built
Copyright is held by the author/owner(s).
SIGIR‚Äô08, July 20‚Äì24, 2008, Singapore.
ACM 978-1-60558-164-4/08/07.from the sample depending on the resource selection algo-
rithm chosen, often involving either the document frequency
(the number of documents containing each term in the sam-
ple) or the average term frequency (the number of times a
term appears on average in each of the sampled documents).
The estimated size of the collection is usually also calculated
based on statistics of the sample.
In the second phase the system selects for a particular
query, a subset of the collections that is most likely to con-
tain relevant documents. Typically, the number of collec-
tions chosen is predetermined and considerably less than the
total set, so that the system can benet from the cost savings
of not needing to access all resources. Commonly employed
algorithms for performing resource selection include CORI
and ReDDE [4].
Finally, the system must aggregate (or fuse) the results
from the individual collections before presenting a unied
list to the user. Algorithms for result fusion dier on how
much information they expect to receive from each collection
(some require scores for each document in the ranked list)
and whether they need to download the content of every
document (resulting in considerable overhead and latency).
Common algorithms for results fusion include CombMNZ
and ProbFuse [3].
2. PERSONALIZING DIR
We now discuss how we plan to personalize each of the
three phases of DIR. Our aim is to rst determine which of
the phases are best suited for personalization. In the follow-
ing, we assume that we have a prole of the user, denoted
u, which contains a list of previous queries along with the
collections accessed and documents downloaded, as well as a
distribution of terms describing the user's interests (mined
from the content of the documents and weighted by recency).
In order to personalize the resource descriptions generated
by QBS, we bias the sampling toward the interests of the
user. The idea is to use the user prole u(either the query
log or the content term distribution) as a source of terms for
generating the single term probe queries required for QBS.
The intuition here, is that we force the QBS algorithm to
spend more time sampling documents that are closer to the
interests of the user, and thereby build resource representa-
tions that have higher delity in these areas. The resource
selection algorithm should then have a better chance of dis-
cerning which resources are useful for a particular query pro-
vided the query is somewhat related to the user's interests.
It is possible, however, that biasing sampling toward user
interests may have a detrimental aect on resource selection
719performance in some cases by causing the resource selection
algorithm to overestimate the set of relevant documents in
a collection or underestimate the total size of a collection.
Our approach to personalizing the results of resource se-
lection is to use the data in the user prole uto learn
a mapping from the terms in the user's queries to the re-
sources accessed. In this way we will be able to characterize
the user's preference for certain collections over others (e.g.
news from CNN over the ABC), which may not be fully
captured by the term distributions in the resource descrip-
tions. The mapping will be used to generate a personalized
score for each resource that will be combined with the score
provided by standard (not-personalized) resource selection
algorithms. In this way we can personalize the resource se-
lection process without needing to change the internals of
the selection algorithm and can test the same personaliza-
tion strategy on dierent algorithms concurrently.
We plan to personalize the results of the fusion step by
taking into account the user's historical use of each resource,
so as to give precedence to documents from trusted resources.
We also plan to investigate the re-ranking of documents
based on the similarity of their content to the term distri-
bution in the user prole u.
3. EVALUATING PERSONALIZED DIR
We briey discuss our strategy for large scale evaluation of
personalized DIR, where by \large-scale" we intend that the
number of users will far outnumber that which is possible
for a typical user case-study.
In order to evaluate personalized DIR we require a testbed
that has two properties: Firstly, the set of documents must
be distributed across (or at least separable into) multiple
resources. Secondly we need a stream of user queries and
corresponding personal relevance judgments. Commercial
search engines maintain such data in the form of query logs
and click-through data. Unfortunately, we do not have ac-
cess to such personal data. Thus we investigate publicly
available personalized data in the form of social bookmark-
ing (tag) data. We use data from the website del.icio.us to
simulate a personalized DIR test collection, where we ap-
proximate query logs and URL clicks with del.icio.us tags
and bookmarked URLs. In other words, we use an individ-
ual's personal but public bookmark history as an approxi-
mate substitute for their private search activity. We have
recently collected a large quantity of bookmark data:
number of del.icio.us users 51,938
total number of URLs 1,371,441
number of distinct URLs 920,303
average number of URLs/user 26.4
In order to build a distributed test collection we need to
take this collection of documents (URLs) and distribute the
documents across dierent collections. The most obvious
way to do that, while preserving natural homogeneity in the
data, is to put all URLs from the same domain (the same
URL hostname) into the same collection. So we looked for
the most common domains in the del.icio.us dataset and
created a collection for each. The table below shows the ve
most common domains in our sample of del.icio.us:domain distinct (total) URLs
en.wikipedia.org 8053 (9699)
www.ickr.com 3673 (5301)
community.livejournal.com 3202 (3923)
news.bbc.co.uk 1755 (2418)
lifehacker.com 1697 (6801)
Obviously, the set of distinct documents per domain is
too small to constitute viable test collections for DIR. Test
collections should be at least of the order of hundreds of
thousands if not millions of documents. In order to create
such collections we are currently crawling each domain using
these distinct URLs as seeds. We are crawling the 100 most
frequent domains from del.icio.us up to depth 10, where each
domain has over 200 seed URLs associated with it.
Evaluating personalization in a Distributed IR setting is
an non-trivial undertaking. Our plan is to compare directly
a personalized system with a control (a non-personalized
system) over a set of more than 100,000 queries. For each
query (taken from the user's search history), we check to see
if aknown relevant document (i.e. the document that the
user had clicked on) moves up or down on the ranked list, as
a result of personalization. Since we don't have query logs
and click-through data we will use user tags in lieu of queries
and bookmarked documents in lieu of the clicked ones.
On average, the relevant document should move up on
the list as the result of personalization. Thus we will count
the number of times this occurs to test the ecacy of each
personalization approach. We will also measure how far doc-
uments move in the ranking relative to their initial position
in terms of the Mean Reciprocal Rank (MRR). Together
these metrics give us a good indication of the quality of the
personalization approach. We note that more traditional
metrics such as the Mean Average Precision (MAP) are not
directly applicable in our case, since we only know of one
relevant document per query and have an open-world as-
sumption (other documents may also be relevant) and thus
can only compare two systems based on the position of the
known document.
The proposed evaluation setting in which personalization
techniques for the various phases of DIR are evaluated in
situ, is dierent from traditional techniques for evaluating
QBS and resource selection algorithms. Previously, researchers
have dened metrics to measure the quality of resource rep-
resentations or selection rankings. We believe such metrics
to be necessarily sub-optimal and prefer to evaluate dier-
ent personalization approaches in terms of their overall DIR
performance.
Unfortunately preliminary results for our experiments could
not be given in this paper, due to the time-consuming na-
ture of the Web crawling required to generate the large (over
100,000 queries) test collections that we are building.
4. REFERENCES
[1] J. Callan and M. Connell. Query-based sampling of
text databases. TOIS , 19(2):97{130, 2001.
[2] X. Shen, B. Tan, and C. Zhai. Context-sensitive
information retrieval using implicit feedback. In
SIGIR'05 , 2005.
[3] M. Shokouhi. Segmentation of search engine results for
eective data-fusion. In ECIR'07 , pages 185{197, 2007.
[4] L. Si and J. Callan. Relevant document distribution
estimation method for resource selection. In SIGIR'03 ,
2003.
720