Ranking using Multiple Document Types
in Desktop Search
Jinyoung Kim and W. Bruce Croft
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
{jykim,croft}@cs.umass.edu
ABSTRACT
A typical desktop environment contains many document
types (email, presentations, web pages, pdfs, etc.) each withdierent metadata. Predicting which types of documents auser is looking for in the context of a given query is a crucial
part of providing eective desktop search. The problem is
similar to selecting resources in distributed IR, but there aresome important dierences.
In this paper, we quantify the impact of type prediction
in producing a merged ranking for desktop search and in-troduce a new prediction method that exploits type-specicmetadata. In addition, we show that type prediction per-formance and search eectiveness can be further enhancedby combining existing methods of type prediction using dis-
criminative learning models. Our experiments employ pseudo-
desktop collections and a human computation game for ac-quiring realistic and reusable queries.
Categories and Subject Descriptors
H.3.3 [ Information Storage and Retrieval ]: [Informa-
tion Search and Retrieval]
General Terms
Algorithms
Keywords
Information Retrieval, Desktop Search, Semi-structured Doc-ument Retrieval, Type Prediction, Human Computation Game
1. INTRODUCTION
People have many types of documents on their desktop
with dierent sets of metadata for each type. For instance,emails have sender and receiver elds, whereas oce doc-uments have lename and author elds. Considering thatpersonal information is now increasingly spread across var-ious places on the web, this diversity of document types is
continuing to increase.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copiesbear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.SIGIR‚Äô1 0July19‚Äì23 ,2010 ,Geneva,Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07....$10.00.Desktop search systems, which is now a standard feature
of most platforms, have tried to exploit this type information
by presenting search results for each document type sepa-rately (e.g., Apple Spotlight) or showing type informationdistinctively in a single ranked list (e.g., Google Desktop).
In both scenarios, a critical aspect of the system is be-
ing able to predict which type(s) of document(s) a user islooking for given a query. If the system displays separatetype-specic results, it can rank them by their type scores.
Alternatively, the system can incorporate type scores into
document ranking as a feature.
The type prediction problem bears some similarity to the
vertical or resource selection problem in aggregated or feder-ated search in that the system tries to score the results from
each vertical, resource, or collection based on predicted rel-
evance for a given query. In this sense, all these problemscan be put in a broad category of collection scoring. Thereare, however, several notable dierences.
First, type-specic sub-collections in the desktop are co-
operative in that all the documents are available to a sin-
gle system. This means that sampling techniques used forfederated search may not be necessary for desktop search;second, unlike typical collections used for aggregated search,
the sub-collections in the desktop environment are small and
have considerable topical overlap. This makes it challengingto apply content-based collection scoring techniques (e.g.,CORI [4]) directly; third, each sub-collection in the desktop
has unique metadata that has not been exploited in existing
collection scoring methods.
The main goal of this paper is to show how the retrieval
eectiveness of a desktop search system can be enhanced byimproving type prediction performance. We focus on known-
item queries, which are the most frequent type of request in
the desktop environment [8], and assume that the systemdisplays a nal rank list by merging type-specic results.
Our work makes several contributions: rst, we demon-
strate the impact of sub-collection retrieval and type pre-diction performance on the quality of the nal rank list;second, we suggest a type prediction method that exploitstype-specic metadata and show that the new method hasbetter performance than a state-of-the-art collection scoring
method; third, we nd that a combination of collection scor-
ing methods can improve the performance further; fourth,we employ a game interface to collect a large quantity ofknown-item queries in a reasonably realistic setting.
The rest of this paper is organized as follows. We pro-
vide an overview of related work in the next section. Thenwe introduce the retrieval model, type prediction methods,
50EmailWebpageOfÔ¨Åce File
FinalRank List-Rank List-Type Score
-Rank List-Type Score-Rank List-Type Score
Figure 1: Suggested retrieval model for desktop
search.
and test collection generation methods we used. In the ex-
periments, we report retrieval and type prediction perfor-
mance using pseudo-desktop collections and a computer sci-
ence (CS) collection where queries are collected using a game
interface.
2. RELATED WORK
Related work can be found in several dierent areas: desk-
top search, vertical search, and distributed IR.
For desktop search, people have studied user interface
issues [7], retrieval models, and evaluation methods [10].
Among these, Thomas et al. [19] views desktop search as
a meta-search problem where the results from many servers
are merged. They compared several server selection methods
using documents collected from various sources, concluding
that a selection method based on Kullback-Leibler diver-
gence [18] performed the best. Our work extends this work
by suggesting a prediction method that exploits the eld
structure and a combination method whose performance can
be improved by interaction with the user.
To evaluate desktop search, methods for building test col-
lections [5] [10] have been proposed. Among these, the
pseudo-desktop method by Kim et al. [10] generated test
collections automatically by simulation, based on a tech-
nique suggested by Azzopardi et al.[3]. Our work employs
these pseudo-desktop collections for the retrieval experiments
and improves the procedure of gathering human-generated
queries by employing a game interface.
In the context of aggregated search and distributed IR, re-
searchers have proposed many methods of scoring collections
against a given query. Approaches such as CORI [4] and
KL-Divergence [18] treat collections as large documents and
apply document scoring techniques for scoring collections.
Other methods, such as ReDDE [17] model the distribution
of relevant documents for each collection. Recently, Arguello
et al. proposed a classication approach [1] [2] where many
sources of evidences can be combined for mapping a user's
query into one or more collections. Our combination ap-
proach is similar to this work but we use features and eval-
uation methods more suitable for our problem domain.
3. RETRIEV AL MODEL
In this section we introduce a retrieval model for desktop
search. In our retrieval model, as depicted in Figure 1, type-
specic results (rank list and type score) are merged into a
nal rank list. We rst explain methods used for the retrieval
of sub-collections corresponding to each le type. Then we
introduce the type prediction and result merging methods
we used.The following notation will be used throughout this paper.
We assume that a query Q= (q1;:::;q m) is composed of
mwords and each collection Ccontains documents with
neld types ( F1;:::;F n) wherencan be dierent for each
collection. Each document din the collection may include
elds (f1;:::;f n), where each eld is marked using lowercase
letters to distinguish it from the corresponding eld type in
the schema. Model-specic parameters will be explained as
they appear.
3.1 Type-speciÔ¨Åc Retrieval
The rst step in our retrieval model is ranking documents
from each sub-collection. Since our focus is on type predic-
tion, we employ retrieval models used in the recent work by
Kim et al. [10] on desktop search, which includes document
query-likelihood (DLM), the probabilistic retrieval model for
semistructured data (PRM-S) and the interpolation of DLM
and PRM-S (PRM-D). We explain the PRM-S model in the
following section.
3.1.1 Probabilistic Retrieval Model
for Semi-structured Data
The probabilistic retrieval model for semistructured data
(PRM-S) [11] scores documents by combining eld-level query-
likelihood scores similarly to other eld-based retrieval mod-
els [13]. The main feature of the PRM-S model is that
weights for combining eld-level scores are estimated based
on the predicted mapping between query terms and doc-
ument elds, which can be eciently computed based on
collection term statistics.
More formally, using Bayes' theorem, we can estimate the
posterior probability PM(Fjjw) that a given query term w
is mapped into document eld Fjby combining the prior
probability PM(Fj) and the probability of a term occurring
in a given eld type PM(wjFj).
PM(Fjjw) =PM(wjFj)PM(Fj)P
Fk2FPM(wjFk)PM(Fk)(1)
Here,PM(wjFj) is calculated by dividing the number of
occurrences for term wby total term counts in the eld Fj
across the whole collection. Also, PM(Fj) denotes the prior
probability of eld Fjmapped into any query term before
observing collection statistics.
With the mapping probabilities estimated as described
above, the probabilistic retrieval model for semistructured
data (PRM-S) can use these as weights for combining the
scores from each eld PQL(wjfj) into a document score, as
follows:
P(Qjd) =mY
i=1nX
j=1PM(Fjjqi)PQL(qijfj) (2)
This model was shown to have better performance than
other eld-based retrieval methods, such as the mixture of
eld language models [13] and BM25F [15], for a semi-structured
document retrieval task using the IMDB [11] and TREC
email [10] collections.
3.2 Type Prediction
As briey mentioned in the introduction, an integral part
of our retrieval model is the type prediction component,
which scores each collection given a user query. The type
51prediction method will produce scores for each sub-collection
and these scores can be used for merging results into the nal
rank list or ranking sub-collection results. Section 4 deals
with type prediction methods in detail.
3.3 Result Merging
With type-specic rank lists from sub-collection retrieval
and collection scores from the type prediction component,
we can produce the nal rank list by rank-list merging algo-
rithms. In this work, we use the well-known CORI algorithm
for merging [4].
C0
i= (Ci Cmin)=(Cmax Cmin) (3)
D0= (D Dmin)=(Dmax Dmin) (4)
D00=D0+ 0:4D0C0
i
1:4(5)
Here,C0
iandD0are normalized collection and document
score, computed using the maximum and minimum of col-
lection scores ( Cmax/Cmin) and document scores ( Dmax/
Dmin), respectively. Given C0
iandD0, the nal document
scoreD00can be computed by combining these two scores.
4. TYPE PREDICTION METHODS
In this section, we introduce our type prediction methods
in detail. We rst describe existing methods for type predic-
tion which are adopted from recent works on aggregated and
federated search [1] [2]. Then we introduce a new type pre-
diction method that exploits document metadata. Lastly,
we explain how type prediction methods can be combined
using several learning methods.
4.1 Existing Methods for Type Prediction
4.1.1 Query-likelihood of Collection
Many traditional resource selection methods (e.g. CORI)
are computed from collection term statistics. Among these,
we use collection query-likelihood (CQL)[18], which is a re-
source selection method based on the language modeling ap-
proach. The approach here is to collapse all documents in
each collection into one giant `document' and use the query-
likelihood score for the document as the collection score:
CQL (Q;C) =Y
q2Q(P(qjC) + (1 )P(qjG)) (6)
Here,Cis the language model of each sub-collection and G
is the language model of the whole collection. The smooth-
ing parameter adjusts the interpolation ratio of P(qjC)
andP(qjG). CQL was shown to be the most eective among
resource selection methods in a recent evaluation [19].
4.1.2 Query-likelihood of Query Log
Another source of evidence for the type prediction is the
aggregated query terms used for nding documents that
belong to each sub-collection. As done in previous work
[1] [2], we use the query-likelihood score of the language
model (QQL) built by queries targeted for sub-collection C
as shown below:
QQL (Q;C) =Y
q2Q(P(qjLC) + (1 )P(qjLG)) (7)Here,LCis the language model of the query log corre-
sponding to collection C.LGis similarly dened using the
query log across all collections.
4.1.3 Geometric Average
Another class of resource selection methods combine the
score of top documents to evaluate each collection given the
user query. Seo et al. [16] proposed using the geometric
mean of the top mdocuments as the combination method,
GAVG (Q;C) = (Y
d2DtopP(Qjd))1
m (8)
whereDtopis the set of top mdocuments from the collec-
tion and the score P(Qjd) is padded with Pmin(Qjd) if fewer
thanmdocuments are retrieved.
4.1.4 ReDDE
ReDDE [17] [2] scores a target collection based on the
expected number of documents relevant to the query. Al-
though previous work used a centralized sample index to
derive this expectation, we can estimate this directly from
the target collection,
ReDDE (Q;C) =X
d2DtopP(Qjd) (9)
which is equivalent to using the sum of the top document
scores belonging to each collection. Intuitively, this results
in a higher score for the collection with more documents in
higher positions.
4.1.5 Query Clarity
So far, most of our methods have been derived from re-
source selection techniques developed in the context of dis-
tributed IR. Query performance prediction methods can also
be used for type prediction by assigning a higher score for
the collection with higher predicted performance. Among
such methods, we employ Query Clarity [6], which predicts
performance using the KL divergence between a query lan-
guage model and a collection language model.
Clarity (Q;C) =X
w2VP(wjLQ)log2P(wjLQ)
P(wjC)(10)
Here, query language model LQis estimated from the top
mdocuments from the collection.
4.1.6 Dictionary-based Matching
In some cases, users provide direct clues about which le
type they intended to search, by including terms such as
`sender'(for email), `pdf'(for oce document) or `www'(for
webpage). Although these terms may not occur in a ma-
jority of queries, they can be a strong indication of type
membership for a given query. We built the dictionary for
each sub-collection by using the names of the collection and
metadata elds.
4.2 Using Document Metadata Fields
for Type Prediction
Although some of methods introduced above use the col-
lection term statistics, none use the eld structure of doc-
uments available for desktop collection. Considering that
52the retrieval eectiveness of semi-structured document col-
lections have been improved by exploiting this structure [11],
we can expect similar benets for the type prediction prob-
lem.
Field-based collection query likelihood (FQL) { our new
method for type prediction { extends the collection query
likelihood model for collection scoring by combining the query-
likelihood score for each eld of the collection instead of us-
ing the score for the whole collection. In other words, if we
borrow the view of query-term and eld mapping described
in Section 3.1.1, we try to infer the mapping between a user
query and each collection by combining mapping probabili-
ties for the elds of each collection.
More formally, for a collection Cthat contains documents
ofneld types ( F1;:::;F n), we can combine the language
model score of each eld as follows:
FQL (Q;C) =comb Fi2C(P(qjFi)) (11)
Here,Fiis a smoothed language model of the ith eld
of the collection and comb can be any function that can
combinennumbers into one. We experimented with many
variations of comb function and found that arithmetic mean
gives the best performance.
4.3 Combining Type Prediction Methods
Considering that the type prediction methods introduced
so far are derived from dierent sources, it is plausible that
we can get further performance benets by combining indi-
vidual methods in a linear model where weights are found
using learning methods. In this section, we describe three
learning methods with dierent objective functions: grid
search of parameter values, a multi-class classier and a
rank-learning method.
4.3.1 Grid-search of Parameter Values
Since we have only seven features to be combined, It is fea-
sible to perform a grid search of parameter values that max-
imize the performance of a training set of queries. Specif-
ically, we can nd the optimal value for each parameter in
turns while xing the values of the parameters previously
found, and repeating the whole procedure until we reach
convergence. In searching for the optimum value of each
parameter, we employed Golden Section Search[14].
4.3.2 Multi-class ClassiÔ¨Åcation
Given that we want to predict one of kdocument types
for a given query, this is typical multi-class classication
scenario where each type corresponds to a class. Among
many choices of such methods, we used a one-vs-rest (OVR)
support vector machine classier (MultiSVM) available in
Liblinear Toolkit1. Since the output of this classier is not
suitable to be used directly as type scores, we used a simple
linear transform to convert the scores into probabilities.
4.3.3 Rank-learning Method
Alternatively, one can cast the type prediction task as a
ranking problem where we try to rank the relevant collec-
tions higher than non-relevant collections. This approach
can be especially benecial for the case where the user is
nding multiple documents with dierent types, since such
1http://www.csie.ntu.edu.tw/ cjlin/liblinear/situation is hard to model with typical multi-class classica-
tion methods. RankSVM [9] was used as the rank-learning
method.
5. TEST COLLECTION
GENERATION METHODS
Evaluation of desktop search has been considered a chal-
lenging issue due to the private nature of collections. As a
solution, methods for building reusable test collections have
emerged recently [5] [10]. Here we review the test collec-
tion generation methods and introduce our game interface
for gathering known-item queries.
5.1 Pseudo-desktop
Kim et al. [10] introduced the pseudo-desktop method of
automatically building a reusable test collection for desktop
search. They collected documents with similar characteris-
tics to a typical desktop environment and generated queries
by statistically taking terms from each of the target doc-
uments. The generated queries are validated against a set
of manually written queries, which were gathered by show-
ing people documents and asking them to write queries for
nding those documents.
Although this pseudo-desktop method provides a cost-
eective way to generate a large quantity of test data under
perfect experimental control, there are several limitations:
rst, the query generation procedure is too simple in that
it independently chooses words from the target document,
while real queries contain phrases; second, only terms in tar-
get documents are used as queries; third, the procedure of
getting manual queries is not realistic in that people were
asked to formulate queries for documents they were not fa-
miliar with.
It should be possible to mitigate the problems above by
rening the query generation method, but we decided in-
stead to improve the procedure of gathering manual queries
by developing a human computation game, as explained in
the next section.
5.2 DocTrack Game
Human computation games [20] have recently been sug-
gested as method for getting a large amount of human anno-
tations in a way that motivates participants using a game-
like setting. In the context of IR research, Ma et al. [12]
introduced Page Hunt, which is a game designed to collect
web search log data by showing each participant webpages
and asking her to nd them with the search interface pro-
vided.
By adapting Page Hunt to our problem domain, we devel-
oped the DocTrack game for gathering known-item queries
in the desktop environment, as shown in Figure 2. In addi-
tion to using documents of many types that might be found
in a desktop instead of random webpages, we made several
modications to the original Page Hunt game: rst, since
people generally have good knowledge of their own desk-
tops, we collected documents that participants are familiar
with and let each of them browse the collection for some
time before starting the game; second, to simulate a typical
known-item search scenario, we showed participants multi-
ple documents and asked them to nd one of them without
specifying which one is the target document; third, we used
a document viewer that can show documents of any types
53Figure 2: The screenshot of the DocTrack game.
The user is being shown a document.
(e.g. pdf, doc and ppt) in the same way they are seen on
the desktop.
Compared to the method of collecting manual queries in
Kim et al.[10], using the DocTrack game, we could gather
many more realistic queries together with the whole session
log data. This in turn allowed us to perform the training of
discriminative learning models which typically requires large
amounts of training data.
6. EXPERIMENTS
In this section we describe the experiments for verifying
the type prediction and retrieval performance. We used
three pseudo-desktop collections with generated queries for
the rst experiment, where we compared several type pre-
diction methods and showed the impact of type prediction
on the nal ranking. We then report on experiments us-
ing a computer science (CS) collection where queries were
collected by the DocTrack game.
In the indexing of both collections, each word was stemmed
using the Krovetz stemmer and standard stopwords were
eliminated. Indri2was used as a retrieval engine for all the
retrieval experiments. We used prediction accuracy to eval-
uate type prediction performance, since we have only one
correct collection for each query. Mean Reciprocal Rank
was used as the measure of retrieval performance for all ex-
periments, since this is a known-item task where each query
has only one relevant document.
Four retrieval methods were used for each sub-collection
(DLM / PRM-S / PRM-D / Best) and four methods (Uni-
form / CQL / FQL / Oracle) were used for type prediction.
We compared only CQL and FQL for the pseudo-desktop
experiment since CQL was shown to be the most eective
among collection scoring method [19] and FQL is the ex-
tension of CQL for semi-structured document collections.
Section 6.2 provides the comparison with other type predic-
tion methods using the CS collection and queries from the
DocTrack game.
For the Best retrieval method, we used the retrieval method
with the best aggregate performance for each sub-collection,
2http://www.lemurproject.orgTable 1: Number and average length of documents
for pseudo-desktop collections.
Type Jack Tom Kate
#Docs Len. #Docs Len. #Docs Len.
email 6067 555 6930 558 1669 935
html 953 3554 950 3098 957 3995
pdf 1025 8024 1008 8699 1004 10278
doc 938 6394 984 7374 940 7828
ppt 905 1808 911 1801 729 1859
Table 2: Query examples with corresponding target
collections for pseudo-desktop collections.
Query Target Collection
jose 03 kahan email
presentation 19 ppt
org address html
making the assumption that the best-performing retrieval
method is known in advance. For Uniform and Oracle col-
lection scoring, we considered that each collection has the
same chance of containing the relevant document (Uniform)
or that we have the perfect knowledge of the collection that
contains the relevant document (Oracle).
6.1 Pseudo-desktop Collections
Three pseudo-desktop collections described in Kim et al.
[10] were used for these experiments. Each collection con-
tained typical le types such as email, webpage and oce
documents related to three individuals, as shown in Table 1.
For email, the indexed elds were title,content ,date,sender
and receiver . For the other document types, title,content ,
date,URL ,summary were indexed.
To generate queries, we rst chose the target document
and took each query word based on the term frequency in
a randomly chosen eld of the document. For instance, the
query `org address' shown in Table 2 was generated by taking
the term `org' from the URL eld and `address' from the
title eld, respectively. Kim et al. reported that queries
generated using this method have the highest similarity to
a set of manual queries.
For each experiment, we generated 50 queries of average
length 2 where target documents were taken from each sub-
collection in proportion to the number of documents it con-
tains. Table 2 shows several queries we used. Lastly, all
the experiments were repeated three times since the query
generation procedure involves some randomness.
6.1.1 Prediction Accuracy
In Table 3, we compare the accuracy of type prediction in
pseudo-desktop collections for the CQL and FQL methods,
where FQL shows a clear improvement over CQL method.
Although this result should be interpreted with some reser-
vations because we are using simulated queries, the same
trend was found in the experiment using manual queries.
We also observe that both methods show reasonable perfor-
mance in the Jack andTom collections, which contain far
more email documents than other types. From this, we can
conclude that both methods are relatively robust against an
imbalance of sub-collection sizes.
6.1.2 Retrieval Performance
We now report the retrieval performance for the same
54Table 3: Accuracy of type prediction in pseudo-
desktop collections.
Jack Tom Kate
CQL 0.606 0.637 0.38
FQL 0.773 0.807 0.64
Table 5: Number and average length of documents
in a computer science (CS) collection.
Type #Docs Length
email 851 731
news article 170 352
calendar item 354 306
webpage 4727 539
oce document 1887 357
queries in Table 4. The rst noticeable trend is that both the
choice of type-specic retrieval model and type prediction
method has a big impact on the nal result. Especially, Or-
acle type prediction was much better than the FQL method,
which in turns outperformed CQL across all collections. On
the other hand, the Best retrieval method was not much
better than the PRM-D and PRM-S methods.
6.2 CS Collection
Next we report on experiments using a computer science
(CS) collection, where the documents of various types are
collected from many public sources in the Computer Science
department the authors belong to. As shown in Table 5, the
CS collection contained emails from the department mailing
list, news articles and blog postings on technology, calendar
items of department announcements, webpages and oce
documents crawled from the department and lab websites.
The documents in the CS collection are much shorter than
in the other pseudo-desktop collections.
For all document types, title and content elds were in-
dexed. Also, there were type-specic elds such as date,
sender and receiver for email, tagand author for news ar-
ticles, starttime and location for calendar items, URL for
webpages, lename for oce documents.
We used the DocTrack game for collecting queries. We
had 20 participants who were students, faculty members and
sta in the department, all familiar with the documents in
the collection. In total, 66 DocTrack games were played and
984 queries were collected using 882 target documents, some
of which are shown in Table 6.
The average length of queries was 3 :97, which is longer
than the reported length (2) in the other desktop search
studies [7]. This may be due to people paying more attention
to the task in the competitive game setting compared to
typical desktop search. The standard deviation (1 :85) of
the query length was also quite high, implying that there
is a considerable variation among the querying behavior of
individuals.
The participants were generally in favor of the game, say-
ing that playing the game was fun and felt reasonably similar
to their search experience in the desktop.
Since some of features required data for estimation, we
used 528 queries to obtain query-log feature (QQL) values
and training parameters for other features. The rest (456)
of the queries were used to evaluate the type prediction per-
formance of features and combination methods by 10-foldTable 6: Query examples with corresponding target
collections for a CS collection.
Query Target Collection
reminder jerey johns email
2010 candidate weekend calendar item
yanlei xml dissemination oce document
cs646 homework html html
Table 8: Signicance test result for type prediction
accuracy in a CS collection. Each cell shows the p-
value of paired t-test between the accuracy of two
methods.
Method CQL FQL Grid RankSVM MultiSVM
CQL 0.03 0.00 0.00 0.00
FQL 0.69 0.27 0.02
Grid 0.41 0.02
RankSVM 0.07
cross-validation. For the retrieval experiments, since many
queries did not return any documents, we used only queries
where the relevant document was ranked in the Top 50 result
set during the game.
6.2.1 Prediction Accuracy
Table 7 summarizes the prediction accuracy result, com-
paring two of the best-performing single feature runs (CQL
/ FQL) and combination methods (Grid / RankSVM / Mul-
tiSVM). The result shows that all the combination runs im-
proved performance over the best single feature runs given
by FQL, which outperformed CQL in this collection as well.
MultiSVM was shown to be the most eective among combi-
nation methods. This is understandable considering that we
had one target collection for each query, which is a natural
setting for multi-class classication. RankSVM was slightly
better than Grid but the dierence was not signicant.
The result of a signicance test is reported in Table 8,
which shows that the performance dierences between CQL
and all the other methods are signicant with a p-value of
0:05 and that MultiSVM outperforms all the other meth-
ods signicantly with a p-value of 0 :1 (using paired t-test).
Overall, this means that suggested type prediction method
(FQL) improves the performance of the CQL method and
that the combination of features improves the performance
further.
6.2.2 Retrieval Performance
Table 9 shows the retrieval performance, comparing four
retrieval methods (DLM / PRM-S / PRM-D / Best) and the
same set of type prediction methods as above in addition to
Oracle and Uniform methods.
The result mostly shows the same trends as the pseudo-
desktop collections despite the big dierence in experimental
conditions (remember that queries were algorithmically gen-
erated for the pseudo-desktop collections). FQL was better
than CQL and all the combination methods outperformed
CQL and FQL signicantly (with a p-value of 0 :05 using
paired t-test).
The only exception is that the performance of MultiSVM
was slightly worse than RankSVM. Given the superior pre-
diction accuracy of MultiSVM, it seems that the procedure
of converting the SVM output into the type prediction score
caused some problems. We can also see that Oracle type pre-
diction method and the Best retrieval method outperform
55Table 4: Retrieval performance in three pseudo-desktop collections using dierent type-specic retrieval
methods and type prediction methods.
Jack Tom KateAverageUniform CQL FQL Oracle Uniform CQL FQL Oracle Uniform CQL FQL Oracle
DLM 0.129 0.159 0.27 0.331 0.104 0.123 0.192 0.224 0.126 0.12 0.237 0.294 0.194
PRM-S 0.152 0.212 0.326 0.403 0.15 0.209 0.289 0.348 0.232 0.239 0.383 0.532 0.346
PRM-D 0.148 0.219 0.335 0.403 0.155 0.204 0.289 0.346 0.25 0.245 0.387 0.538 0.355
Best 0.154 0.225 0.336 0.414 0.157 0.217 0.302 0.361 0.241 0.245 0.388 0.542 0.354
Average 0.146 0.204 0.317 0.388 0.141 0.188 0.268 0.32 0.212 0.212 0.349 0.477
Table 7: Accuracy of type prediction for best-performing single feature runs and combination methods in a
CS collection.
Method CQL FQL Grid RankSVM MultiSVM
Accuracy 0.708 0.743 0.747 0.758 0.808
Table 11: Correlation among the prediction per-
formance of features. Each cell shows the pearson
correlation coecient between the accuracy of two
methods.
Feature FQL Dict QQL Clarity GAVG ReDDE
CQL 0.68 0.10 0.29 -0.03 -0.01 0.04
FQL 0.05 0.32 0.01 -0.01 0.01
Dict 0.28 0.02 -0.00 0.04
QQL -0.02 0.04 0.05
Clarity 0.17 0.05
GAVG 0.62
other methods, which leaves room for further improvement
in both aspects.
6.2.3 Leave-one-out Prediction Accuracy
For further analysis on the eectiveness of individual fea-
tures, we next report the result of single-feature and leave-
one-out experiments in Table 10, where we used only one
feature (single-feature) or all but one feature (leave-one-out)
to measure the performance.
All in all, features with higher single-feature performance
had bigger impacts when they were omitted, as shown in
the case of content-based features (CQL / FQL) and query
log feature (QQL). On the other hand, features based on
the initial retrieval result (GAVG / ReDDE) was shown to
be ineective. Presumably, this is because the documents of
dierent types had dierent characteristics in this collection,
which make the scores almost incomparable.
On the other hand, in some cases, low single-feature pre-
diction accuracy did not necessarily translate into small dif-
ference in the leave-one-out experiment. The dictionary-
based feature (Dict) and the performance prediction feature
(Clarity) were shown to have a high impact on combination
performance despite having the lowest single-feature results.
The result above makes more sense when considered to-
gether with the correlation among features in Table 11. Meth-
ods based on collection term statistics (CQL / FQL) and top
result set (GAVG / ReDDE) have high correlations within
the group, which explains why performance does not suer
much when one of high-performing features (e.g. CQL and
FQL) are omitted. On the other hand, that QQL does not
correlate highly with any other features explains its high
impact on the leave-one-out experiment.
6.2.4 Personalization of Feature Combination
In the experiments so far we have used the queries from all
participants together. Another benet of the feature com-
bination approach is that it can adjust the result based on
the querying behavior of each individual. For instance, ifTable 12: Feature weights trained using queries
written by each user. The last row shows the
weights trained using all queries. All the names are
anonymized.
User ID WCQLWFQLWDictWQQLWClarity
Jane 1 1 0.15 1 0.09
Yao 0.38 0.56 0.15 1 0
Rajiv 0.58 1 0 0 0
Tim 0.62 1 0 1 0.18
David 1 1 0 0 0
All Users 1 1 0 1 0.09
a user frequently includes terms that indicate a particular
collection, the learning method can improve type prediction
performance by assigning higher weight to the Dict or QQL
features.
To explore this potential value of this personalized type
prediction, we tried training feature combination methods
only with queries written by each user and looked at how
much variation it causes for resulting feature weights.
The results in Table 12 compare the weights trained for
each user. It shows that content-based features (CQL /
FQL) are not very helpful for some users (Yao), whereas
the query log does not make a dierence for others (Ra-
jiv / David). Although this is a preliminary result, the
weights show considerable variation, implying that person-
alized training data produced dierent results for each user.
We leave further analysis of this benet of personalization
for future work.
7. CONCLUSION & FUTURE WORK
In this paper, we suggest a retrieval model for desktop
search where type-specic retrieval results are merged into
a nal rank list based on type prediction scores. Using the
pseudo-desktop and CS collections, we demonstrated that
improving the type prediction method can produce signi-
cantly better nal retrieval performance.
We also introduced eld-based collection query likelihood
(FQL) { a new type prediction method that exploits type-
specic metadata { which shows superior performance to
competitive baseline methods in both collections we tested.
Although FQL is used in the context of desktop search, it
can be used as a resource selection method for aggregated or
federated search, as long as each document has eld struc-
ture.
Moreover, we developed a human computation game for
collecting queries in a more realistic setting and used this
data to train discriminative learning models that combines
type prediction methods as features. Our results show that
56Table 9: Retrieval performance in a CS collection using dierent type-specic retrieval methods and type
prediction methods.
Uniform CQL FQL Grid RankSVM MultiSVM Oracle Average
DLM 0.343 0.507 0.53 0.552 0.563 0.556 0.674 0.526
PRM-S 0.349 0.501 0.518 0.518 0.551 0.547 0.674 0.52
PRM-D 0.36 0.518 0.536 0.536 0.567 0.564 0.694 0.537
Best 0.372 0.548 0.564 0.590 0.596 0.594 0.72 0.563
Average 0.356 0.518 0.537 0.549 0.569 0.565 0.691
Table 10: Accuracy of type prediction for single-feature and leave-one-out experiment. Leave-one-out accu-
racy shows the percentage of decrease for leave-one-out experiment from the experiment using all features.
Method CQL FQL Dict QQL Clarity GAVG ReDDE
Single-feature Accuracy 0.708 0.743 0.201 0.579 0.240 0.255 0.207
Leave-one-out Accuracy -0.6% -1.7% -0.6% -3.1% -0.6% -0.0% -0.0%
the combination method can improve type prediction per-
formance compared to any of existing methods. We also
explored the benet of personalizing type prediction results,
which may potentially improve the performance further.
Our work leaves many interesting challenges. Although
this work showed the value of improving type prediction
performance, better type-specic retrieval and result merg-
ing algorithms should bring further performance gains. For
example, we used only textual features for sub-collection re-
trieval, and it should be possible to incorporate type-specic
features (e.g. PageRank score for webpage).
Also, although we focused on the known-item search task
in this paper, the retrieval model suggested here will be
equally applicable for the ad-hoc search scenario. To gather
training data for this scenario, the DocTrack game can be
modied to gather ad-hoc queries using topic descriptions
and corresponding sets of relevant documents.
8. ACKNOWLEDGEMENTS
This work was supported in part by the Center for In-
telligent Information Retrieval and in part by NSF grant
#IIS-0707801. Any opinions, ndings and conclusions or
recommendations expressed in this material are the authors'
and do not necessarily reect those of the sponsor.
9. REFERENCES
[1] J. Arguello, J. Callan, and F. Diaz.
Classication-based resource selection. In CIKM '09 ,
pages 1277{1286, New York, NY, USA, 2009. ACM.
[2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo.
Sources of evidence for vertical selection. In SIGIR
'09, pages 315{322, New York, NY, USA, 2009. ACM.
[3] L. Azzopardi, M. de Rijke, and K. Balog. Building
simulated queries for known-item topics: an analysis
using six european languages. In SIGIR '07 , pages
455{462, New York, NY, USA, 2007. ACM.
[4] J. P. Callan, Z. Lu, and W. B. Croft. Searching
distributed collections with inference networks. In
SIGIR '95 , pages 21{28, New York, NY, USA, 1995.
ACM.
[5] S. Chernov, P. Serdyukov, P.-A. Chirita,
G. Demartini, and W. Nejdl. Building a desktop
search test-bed. In ECIR' 07 , pages 686{690, 2007.
[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.
Predicting query performance. In SIGIR '02 , pages
299{306, New York, NY, USA, 2002. ACM.[7] S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. Sarin,
and D. C. Robbins. Stu i've seen: a system for
personal information retrieval and re-use. In SIGIR
'03, pages 72{79, New York, NY, USA, 2003. ACM.
[8] D. Elsweiler and I. Ruthven. Towards task-based
personal information management evaluations. In
SIGIR '07 , pages 23{30, New York, NY, USA, 2007.
ACM.
[9] T. Joachims. Optimizing search engines using
clickthrough data. In KDD '02 , pages 133{142, New
York, NY, USA, 2002. ACM.
[10] J. Kim and W. B. Croft. Retrieval experiments using
pseudo-desktop collections. In CIKM '09 , pages
1297{1306. ACM, 2009.
[11] J. Kim, X. Xue, and W. B. Croft. A Probabilistic
Retrieval Model for Semi-structured Data . In
Proceedings of ECIR '09. Springer, 2009.
[12] H. Ma, R. Chandrasekar, C. Quirk, and A. Gupta.
Improving search engines using human computation
games. In CIKM' 09 , pages 275{284, 2009.
[13] P. Ogilvie and J. Callan. Combining document
representations for known-item search. In SIGIR '03 ,
pages 143{150, New York, NY, USA, 2003. ACM.
[14] W. Press, S. Teukolsky, W. Vetterling, and
B. Flannery. Numerical Recipes in C . Cambridge
University Press, Cambridge, UK, 2nd edition, 1992.
[15] S. Robertson, H. Zaragoza, and M. Taylor. Simple
bm25 extension to multiple weighted elds. In CIKM
'04, pages 42{49, New York, NY, USA, 2004. ACM.
[16] J. Seo and W. B. Croft. Blog site search using
resource selection. In CIKM '08 , pages 1053{1062,
New York, NY, USA, 2008. ACM.
[17] L. Si and J. Callan. Relevant document distribution
estimation method for resource selection. In SIGIR
'03, pages 298{305, New York, NY, USA, 2003. ACM.
[18] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language
modeling framework for resource selection and results
merging. In CIKM '02 , pages 391{397, New York, NY,
USA, 2002. ACM.
[19] P. Thomas and D. Hawking. Server selection methods
in personal metasearch: a comparative empirical
study. Inf. Retr. , 12(5):581{604, 2009.
[20] L. von Ahn and L. Dabbish. Designing games with a
purpose. Commun. ACM , 51(8):58{67, 2008.
57