On Ranking TechniquesforDesktop Search¤
SaraCohen
FacultyofIndustrial
Engineering and Management
Technion—IsraelInstitute of
Technology
sarac@ie.technion.ac.ilCarmel Domshlak
Facultyof Industrial
Engineering and Management
Technion—IsraelInstitute of
Technology
dcarmel@ie.technion.ac.ilNaama Zwerdling
Facultyof Industrial
Engineering and Management
Technion—IsraelInstitute of
Technology
anaama@tx.technion.ac.il
ABSTRACT
This paper addresses the desktop search problem by consid-
ering various techniques for ranking results of a search query
over the ¯le system. First, basic ranking techniques, which
are based on a single ¯le feature (e.g., ¯le name, ¯le content,
access date, etc.) are considered. Next, two learning-based
ranking schemes are presented, and are shown to be sig-
ni¯cantly more e®ective than the basic ranking methods.
Finally, a novel ranking technique, based on query selective-
ness is considered, for use during the cold-start period of
the system. This method is also shown to be empirically
e®ective, even though it does not involve any learning.
Categories and Subject Descriptors: H.3.3 [Informa-
tion Storage and Retrieval]: Information Search and Re-
trieval
General Terms: Experimentation, Human Factors
Keywords: desktop search, personal information manage-
ment, ranking
1. INTRODUCTION
Due to the increasing storage capabilities of standard per-
sonal computers, users are no longer motivated to delete old
¯les. The practice of \never deleting ¯les" has distinct ad-
vantages, since it ensures that important data will never
be accidentally removed. However, as an unfortunate side
e®ect, the personal computer often becomes an unwieldy
mess. Thus, locating a speci¯c ¯le, within the ¯le system,
may become a challenging (and even daunting) task. There-
fore, numerous research and industrial projects have evolved
in the area of personal information management (PIM) [5].
These projects aim to develop technologies allowing users to
store, access, and e®ectively search for information in their
personal computer, e.g., [3, 2, 1].
The focus of this paper is on desktop search , i.e., e®ec-
tive search within a personal computer. There are currently
only limited (published) insights into the question of how
to rank desktop-search results, mostly based on very simple
techniques. In this paper we focus on the problem ¯nding
an e®ective ranking scheme for desktop search. To this end,
¤Sara Cohen and Naama Zwerdling were partially supported
by the ISF (Grant 1032/05). Carmel Domshlak was partially
supported by the BSF (Grant 2004216).
Copyrightis held by the author/owner(s).
WWW 2007, May8–12, 2007, Banff,Alberta, Canada.
ACM978-1-59593-654-7/07/0005.we implemented a simple desktop-search tool, which returns
unranked results to the users and logs the user interaction.
We develop various ranking strategies and analyze their ef-
fectiveness, \post-mortem," based on the logs.
2. RANKING TECHNIQUES
A query qis simply a sequence of words. A ¯le fis a
candidate answer forqif there is at least one word appearing
inqthat also appears in the content, ¯lename, or path of
f. We discuss various techniques for ranking a candidate
answer f.
RankingbyBasicFileFeatures. We considered both textual-
based and date-based features. For the textual-based fea-
tures we consider Content ,Path ,Name , whose values
correlate with the cosine distance between the tf.idf vec-
tors of the query and the content, path, name of f, respec-
tively. For the date-based features we allow AccessDate ,
UpdateDate , and CreateDate whose values decrease as
the distance in time grows between the date of querying and
the access, update, and create date of f, respectively.
Learning to Rank. As we already mentioned, one of the
components of our desktop search system is the logger that
stores all the data on the past search sessions of the user. In
principle, this information can be used for learning a better
ranking method. We considered two such ranking methods.
We use a Support Vector Machine (SVM) to learn a linear
function of the basic ¯le features that minimizes the classi¯-
cation error on our training logger data. (Similarly to, e.g.,
[4], we actually learned a binary relation over the candidate
answers.) Thus, the feature SVM-75 is de¯ned for each user,
by using as training data four randomly chosen subsets of
75% of its log data, with the remaining 25% used as the test
set. The results were averaged over these four samples and
over all users. Similarly, the feature SVM-90 , which has a
90=10 data partition setup, was evaluated.
Learning a ranking function using a SVM can be done in
time linear in the number of ¯le features, and polynomial
in the number of candidate answers of all queries seen thus
far. As the amount of the logged search sessions grows, the
latter complexity factor signi¯cantly slows down the learner.
Although learning can be performed o²ine, at some point
the system will unavoidably have to cut down the amount
of available training data, possibly using only some chrono-
logical su±x of the logged data. Due to this limitation, we
also considered the simple learning scheme, called LexOrd
(for \lexicographic order"), which corresponds to a lexico-
WWW 2007 / Poster Paper Topic: Search
 1183Feature Score (¿;Sall) Feature Score (¿;S2-50)
SVM-90 7.84 SVM-90 4.72
SVM-75 8.15 Selective 4.86
LexOrd 9.19 SVM-75 4.93
AccessDate 9.85 LexOrd 5.15
UpdateDate 10.18 Name 5.66
Selective 10.89 Path 6.32
CreateDate 11.51 UpdateDate 6.43
Name 11.53 AccessDate 6.54
Path 12.60 Content 6.75
Random 14.45 CreateDate 6.80
Content 15.43 Random 6.99Feature TopScorek(¿;Sall)TopScorek(¿;S2-50)
k= 1 k= 10 k= 1 k= 10
SVM-90 34.4 64.9 36.6 71.7
SVM-75 32.8 63.5 34.5 70.5
LexOrd 34.0 62.5 37.2 68.5
Selective 27.1 63.1 29.3 72.7
UpdateDate 21.6 52.5 23.4 57.8
AccessDate 19.2 52.3 19.2 52.3
Name 16.5 51.7 18.1 60.5
CreateDate 17.7 48.5 19.2 54.1
Content 13.2 45.0 14.6 55.1
Path 6.9 46.0 7.2 54.1
Random 0.0 36.3 0.0 46.7
Figure 1: Expected placement of features (left table), and e®ectiveness at k2 f1;10g(right table).
graphic aggregation of single-feature-based rankings based
on their relative e±ciency when used in isolation.
Selectiveness of Features. Learning-based ranking is rel-
evant only in presence of su±cient training data. In the
context of desktop search, this can be a real obstacle, since
users tend to issue only a few queries a day. We suggest a
the simple ranking mechanism Selective that combines the
textual-based features, based on their relative selectiveness,
that can be useful for the cold-start period. Intuitively, the
ranking mechanism Selective , combines (i) the informa-
tion carried by the textual properties of the candidate an-
swers, and (ii) the frequency of textual connection between
each such property and query, within the set of candidate
answers. Thus, Selective follows the principle underly-
ing the standard idf (inverse document frequency) measure
used in IR|the contributions of di®erent textual features to
Selective on a given candidate answer are combined via a
weighted sum in which less \common" features (in the set
of candidate answers) are given larger weights.
3. EXPERIMENTALEVALUATION
The desktop search engine was distributed to 19 volun-
teers. In total 1219 queries were issued, where 188 queries
had a single result (i.e., candidate answer), 916 queries has
2{50 results and 115 queries had over 50 results.
We analyzed the e®ectiveness of our ranking mechanisms
on the set of all queries with more than one result, with 2{
50 results and with over 50 results, denoted Sall,S2-50and
S>50, respectively. The results for S>50are similar to those
ofSalland are omitted due to space limitations.
Two measures were used to evaluate the e®ectiveness of
our ranking mechanisms: (1) Score (¿;S) (S 2 fS all;S2-50g)
is the expected placement of the user's target ¯le,1for queries
inS, according to the ranking mechanism ¿. (2) TopScorek(¿;S),
called the e®ectiveness of ¿atk, is the percentage of queries
inSin which ¿ranks the target ¯le within the top k¯les.
(We only consider queries that have at least kresults in this
measure.) Thus, a good ranking mechanism will have a low
value for Score (¿;S) and a high value for TopScorek(¿;S).
Our analysis of the ranking mechanisms considered is sum-
marized in Figure 1. When considering only the basic ¯le
features, the date-based features have signi¯cantly better
expected placement than the textual-based features for Sall,
1The target ¯le is the ¯le chosen by the user. We assume
that this ¯le is unique and is recognized by the user.while the opposite is true for S2-50.2We believe that this
can be explained since textual-based features are most useful
when they are selective (i.e., when there are few results).
Our learning-based ranking features ( SVM-75 ,SVM-90
andLexOrd ) signi¯cantly outperformed the basic ranking
features. For Score (¿;¢), we observe that: (1) SVM-75 and
SVM-90 ended up clear winners and (2) the di®erence be-
tween LexOrd and the next most e®ective method was sta-
tistically signi¯cant. For all TopScorek(¿;¢),k2 f1;10g, it
can be seen that SVM-90 ,SVM-75 , and LexOrd are more
e®ective than all basic ranking methods.
Finally, we consider e®ectiveness of Selective . For the
measure Score (Selective ;¢), we observe that Selective is
better than its most e®ective component (i.e., the textual-
based features), and this di®erence is statistically signi¯cant.
Somewhat surprisingly, on S2-50Selective even success-
fully competes with our learning-based ranking methods|
it appears to be more e®ective than LexOrd (with statis-
tical signi¯cance) and is, in fact, statistically indistinguish-
able with SVM-75 andSVM-90 . Thus, Selective appears
highly e®ective for S2-50, even though it does not involve
any learning. Observe also that Selective is always among
the four best features for TopScorek(¿;¢).
To summarize, we have shown that our learning-based
ranking methods are clearly much more e®ective than the
basic ¯le features, and that Selective is a good choice for
S2-50, during the cold-start period.
4. REFERENCES
[1] S. Dumais, E. Cutrell, JJ Cadiz, G. Jancke, R. Sarin,
and D. C. Robbins. Stu® I've seen: a system for
personal information retrieval and re-use. In SIGIR'03 .
[2] R. Fagin, R. Kumar, K. S. McCurley, J. Novak,
D. Sivakumar, J. A. Tomlin, and D. P. Williamson.
Searching the workplace web. In WWW'03 .
[3] J. Gemmell, G. Bell, R. Lueder, S. Drucker, and
C. Wong. MyLifeBits: Ful¯lling the Memex vision. In
ACM Multimedia'02 .
[4] T. Joachims. Optimizing search engines using
clickthrough data. In KDD'02 .
[5] J. Teevan, W. Jones, and B. B. Bederson, editors.
Special Issue on Personal Information Management ,
volume 49 of Communication of the ACM , 2006.
2Statistically signi¯cant di®erences in performance are de-
termined using the two-sided Wilcoxon test at the 95% con-
¯dence level.
WWW 2007 / Poster Paper Topic: Search
 1184